---
title: "NeurIPS 2016"
author: "Ed Henry"
date: "2016-12-12"
categories: [conferences, neurips, machine learning, ai]
# image: "image.jpg"
---

# Quick Summary

This year's NIPS conference had record attendance at over 6000 people! As opposed to when I attended last year in Montreal, this was an, almost, two fold increase. That said, hats off to the organizers and all of the staff for being able to double the size of a conference and still have it be a relatively smooth attendance experience.

Speaking with other attendees though, I think there was a general interest in structuring the conference a bit more differently than the way it had been. There was even mention of breaking the Deep Learning portion of the conference off into it's own conference in and of itself. That might be a bit extreme, however there was, without a doubt, a healthy amount of sessions and talks that were based on deep learning.

All said and done, though, it was an awesome experience that has left me charged to keep learning and trying the amazing ideas that were presented and exchanged throughout the conference. With that, I thought I'd post a list of all of the sessions I'd attended and try to provide a quick summary of what intuitions I'd built about the presentations. Keep in mind these notes are made both from memory and from the scribbles I have in my notebook. Come to think of it, there is actually a second thing I would have preferred, if it were possible. Sessions tended to be in the range of 20-30 minutes, and that never seemed to be enough time for individuals to present on their problems and potential progresses they had made. A lot of the problems that are being framed up can be incredible technical and may require half to three quarters of the allotted presentation time. I don't pretend to have a solution to this problem, but rather am just interested in providing feedback should anyone stumble on it.

One thing that I wish that I could figure out how to do, in a meaningful way, is to contribute to the greater research "good" that the Open Science ideology that the machine learning community follows, outside of either a large(r) ML shop that can "afford" to pay someone to be half-reseacher, and outside of direct academia. There is the new effort [AI-ON.org](http://ai-on.org/) -- so maybe that's the answer?

# Sessions Attended (and slides if I could find them)

#### Variational Inference 

[Variational Inference: Foundations and Modern Methods](https://nips.cc/Conferences/2016/Schedule?showEvent=6199)

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=6199)
* [Slides](http://www.cs.columbia.edu/~blei/talks/2016_NIPS_VI_tutorial.pdf)

#### Nuts and Bolts of Building Applications using Deep Learning

* [NIPS Session Link](Nuts and Bolts of Building Applications using Deep Learning)
* [Handout](https://www.dropbox.com/s/dyjdq1prjbs8pmc/NIPS2016%20-%20Pages%202-6%20(1).pdf?dl=0)

#### Generative Adversarial Networks

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=6202)
* [Slides](http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf)

#### Predictive Learning

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=6197)
* [Slides](https://drive.google.com/file/d/0BxKBnD5y2M8NREZod0tVdW5FLTQ/view)

#### Value Iteration Networks (award talk)

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=7437)
* [Paper](https://arxiv.org/pdf/1602.02867.pdf)

#### Intelligent Biosphere

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=6193)
* Slides coming soon...

#### InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=7140)
* [Paper](https://arxiv.org/abs/1606.03657)

#### Value Iteration Networks

* [NIPS Session Link](https://papers.nips.cc/paper/6046-value-iteration-networks)
* [Paper](https://papers.nips.cc/paper/6046-value-iteration-networks.pdf)

#### Synthesis of MCMC and Belief Propagation

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=7447)
* [Paper](http://papers.nips.cc/paper/6318-synthesis-of-mcmc-and-belief-propagation)

#### Using Fast Weights to Attend to the Recent Past

* [Nips Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=7439)
* [Paper](https://papers.nips.cc/paper/6057-using-fast-weights-to-attend-to-the-recent-past.pdf)

#### Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences

* [NIPS Session Link]()
* [Paper](Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences)

#### Machine Learning and Likelihood-Free Inference in Particle Physics

This talk was really, really cool. But the time constraints didn't allow Kyle to get into what I was interested in, and that was the embeddings work that had been done. I'm very interested in creating embeddings of tokens according to their co-occurence distribution(s).

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=6195)
* [Slides](https://figshare.com/articles/NIPS_2016_Keynote_Machine_Learning_Likelihood_Free_Inference_in_Particle_Physics/4291565/1)

#### Deep Learning without Poor Local Minima

* [NIPS Session Link](Deep Learning without Poor Local Minima)
* [Paper](https://arxiv.org/abs/1605.07110)

#### Learning to Poke by Poking: Experiential Learning of Intuitive Physics

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=7469)
* [Paper](https://arxiv.org/abs/1606.07419)

#### Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=7471)
* [Paper](https://arxiv.org/abs/1602.07868)

#### Showing versus doing: Teaching by demonstration

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=7482)
* [Paper](https://papers.nips.cc/paper/6413-showing-versus-doing-teaching-by-demonstration.pdf)

#### Relevant sparse codes with variational information bottleneck

* [NIPS Session Link](https://nips.cc/Conferences/2016/Schedule?showEvent=7483)
* [Paper](https://arxiv.org/abs/1605.07332)

## Symposia

#### Recurrent Neural Networks and Other Machines that Learn Algorithms

* [NIPS Link](https://nips.cc/Conferences/2016/Schedule?showEvent=6260)
* [Symposium Homepage](http://people.idsia.ch/~rupesh/rnnsymposium2016/)

#### Deep Learning Symposium

* [NIPS Link](https://nips.cc/Conferences/2016/Schedule?showEvent=6257)
* [Symposium Homepage](https://sites.google.com/site/nips2016deeplearnings/home)

Looking at this list after trying to recompile it from my notes, it's both exhausting and intimidating to think about trying to rehash the workshops on top of everything else listed here. So I will keep this post to just the tutorials and oral talks for the time being. I will create another post that covers the workshops material, as well.

