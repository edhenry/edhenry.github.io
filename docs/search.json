[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ed Henry",
    "section": "",
    "text": "Ed Henry currently a Staff Research Scientist and Distinguished Member of Technical Staff at Dell Technologies working at the intersection of Machine Learning and data center infrastructure technologies."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "Ed Henry",
    "section": "Research Interests",
    "text": "Research Interests\n\nCausality\nSequence Transduction\nNatural Language Processing"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Ed Henry",
    "section": "Experience",
    "text": "Experience"
  },
  {
    "objectID": "about.html#teaching",
    "href": "about.html#teaching",
    "title": "Ed Henry",
    "section": "Teaching",
    "text": "Teaching"
  },
  {
    "objectID": "about.html#conference-talks",
    "href": "about.html#conference-talks",
    "title": "Ed Henry",
    "section": "Conference Talks",
    "text": "Conference Talks"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ed Henry",
    "section": "",
    "text": "Ed Henry currently a Distinguished Scientist at Dell Technologies working on building AI products."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Software I’ve authored."
  },
  {
    "objectID": "posts/flow2vec/index.html",
    "href": "posts/flow2vec/index.html",
    "title": "NetFlow and word2vec -> flow2vec",
    "section": "",
    "text": "Important\n\n\n\nPlease note that as of 09/2022 I am in the process of converting this post into a reproducible notebook. Please check back soon!\nCode\nimport pandas as pd\nimport numpy as np\n#import pyhash\nimport gensim\nimport multiprocessing as mp\nfrom joblib import Parallel, delayed\nimport concurrent.futures\nfrom pprint import pprint\nimport random\nimport mpld3\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn import cluster\nfrom sklearn import manifold\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n%matplotlib inline\n\n# Enable mpld3 for notebook\nmpld3.enable_notebook()\n\n# Instantiate hasher object\n#hasher = pyhash.city_64()\n\n# Method to strip white test\ndef strip(text):\n    return text.strip()\n\n# Method to set dataframe entries to integers\ndef make_int(text):\n    return int(text.strip(''))    \n\n# Method to match IP against flow srcIP\ndef sort_ip_flow(ip):\n    # List to house flows when matches\n    flows_list = []\n    # Iterate over tcp_flows list\n    for flow in tcp_flows:   \n        # Comparison logic - flow[1][3] corresponds to SrcIP in flow tuple\n        if ip == flow[1][3]:        \n            # Append match to flows_list\n            flows_list.append(flow)\n    # Return dictionary of IPs and flows\n    return {ip: flows_list}\n\ndef process_flow(flow):    \n    # Create hash of protocol\n    proto_hash = hasher(flow[1][2])        \n    # Create hash of SrcIP\n    srcip_hash = hasher(flow[1][3])        \n    # Create hash of Sport\n    srcprt_hash = hasher(flow[1][4]) \n    # Create hash of DstIP\n    dstip_hash = hasher(flow[1][6])    \n    # Create hash of Dport\n    dstprt_hash = hasher(flow[1][7]) \n    # Cast flow entry as list for manipulation\n    flow_list = list(flow)       \n    # Insert hashes as entry in tuple for each flow\n    flow_list.insert(4, (str(proto_hash), str(srcip_hash), str(srcprt_hash), \n                         str(dstip_hash), str(dstprt_hash)))    \n    # Re-cast flow entry as tuple w/ added hash tuple\n    flow = tuple(flow_list)\n    return(flow)\n\ndef single_hash(flow):\n    flow_hash = hasher(flow)\n    flow_list = list(flow)\n    flow_list.insert(4, str(flow_hash))\n    flow = tuple(flow_list) \n    return(flow)\nCode\n# Import netflow capture file(s)\n\nflowdata = pd.DataFrame()\n\ncap_files = [\"/home/edhenry/Documents/datasets/ctu-13/capture20110810.binetflow\",\"/home/edhenry/Documents/datasets/ctu-13/capture20110811.binetflow\"]\n\nfor f in cap_files:\n    frame = pd.read_csv(f, sep=',', header=0)\n    flowdata = flowdata.append(frame, ignore_index=True)\n\n# Strip whitespace\nflowdata.rename(columns=lambda x: x.strip(), inplace = True)\nCode\nsubsample_cats = flowdata.loc[:,['Proto', 'SrcAddr', 'DstAddr', 'Dport']]\nsubsample_labels = flowdata.loc[:,['Label']]\n\nsubsample_cats_1 = flowdata.loc[:,['Proto', 'SrcAddr', 'DstAddr', 'Dport', 'Label']]"
  },
  {
    "objectID": "posts/flow2vec/index.html#flow2vec---exploiting-co-occurence-within-netflow-data",
    "href": "posts/flow2vec/index.html#flow2vec---exploiting-co-occurence-within-netflow-data",
    "title": "NetFlow and word2vec -> flow2vec",
    "section": "Flow2vec - exploiting co-occurence within NetFlow data",
    "text": "Flow2vec - exploiting co-occurence within NetFlow data\nAttempting to find some co-occurence patterns in the flow data according to how an algorithm like word2vec, in its skip-gram implementation specifically for this work, works. The idea is that flows, \\(V_{f}\\) for vector representation, that occur within a window \\(W_{f}\\), which can be modeled as “time” using timestamps from the capture. A visual representation of a single flow and window of flows can be seen below :\n\n\n\nFigure 1: Windows of netflows\n\n\nWhen we consider the conditional probabilities \\(P(w|f)\\) with a given set of flow captures Captures the goal is to set the parameters \\(\\theta\\) of \\(P(w\\|f;\\theta)\\) so as to maximize the capture probability :\n\\[ \\underset{\\theta}{\\operatorname{argmax}} \\underset{f \\in Captures}{\\operatorname{\\prod}} \\left[\\underset{w \\in W_{f}}{\\operatorname{\\prod}} P(w \\vert f;\\theta)\\right] \\]\nin this equation \\(W_{f}\\) is a set of surrounding flows of flow \\(f\\).\nAlternatively :\n\\[ \\underset{\\theta}{\\operatorname{argmax}} \\underset{(f, w) \\in D}{\\operatorname{\\prod}} P(w \\vert f;\\theta) \\]\nHere \\(D\\) is the set of all flow and window pairs we extract from the text.\nThe word2vec algorithm seems to capture an underlying phenomenon of written language that clusters words together according to their linguistic similarity, this can be seen in something like simple synonym analysis. The goal is to exploit this underlying “similarity” phenomenon with respect to co-occurence of flows in a given flow capture.\nEach “time step”, right now just being a subset of a given flow data set, is as a ‘sentence’ in the word2vec model. We should then be able to find flow “similarities” that exist within the context of flows. The idea is this “symilarity” will really just yield an occurence pattern over the flow data, much like word2vec does for written text.\nAnother part of the idea is much like in written text there are word / context, \\((w,c)\\), patterns that are discovered and exploited when running the algorithm over a given set of written language. There are common occurences and patterns that can be yielded from flow data, much like the occurences and patterns that are yielded from written text.\nAt the end of the embedding exercise we can use k-means to attempt to cluster flows, according to the embedding vectors that are produced through the word2vec algorithm. This should yield some sort of clustering of commonly occuring flows that have the same occurence measure in a given set of netflow captures. We can then use this data to measure against other, unseen, flows for future classification of “anamoly”. I use that word loosely as this is strictly expirimental.\nAssumptions :\n\nMaximizing the objective will result in good embeddings \\(v_{f} \\forall w \\in V\\)\n\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note with the above statment, with respect to time, is the assumption that the data I am operating from has already been ordered according to the tooling I used to acquire it.\n\n\n\nSkip-gram Negative Sampling\nOne of the other portions of the word2vec algorithm that I will be testing in this experiment will be negative sampling.\nThe objective of Skipgram with Negative Sampling is to maximize the the probability that \\((f,w)\\) came from the data \\(D\\). This can be modeled as a distribution such that \\(P(D=1\\|f,w)\\) be the probability that \\((f,w)\\) came from the data and \\(P(D=0\\|f,w) = 1 - P(D=1\\|f,w)\\) the probability that \\((f,w)\\) did not.\nThe distribution is modeled as :\n\\[P(D=1|f,w) = \\sigma(\\vec{f} \\cdot \\vec{w}) = \\frac{1}{1+e^{-\\vec{f} \\cdot \\vec{w}}}\\]\nwhere \\(\\vec{f}\\) and \\(\\vec{w}\\), each a \\(d\\)-dimensional vector, are the model parameters to be learned.\nThe negative sampling tries to maximize \\(P(D=1\\|f,w)\\) for observed \\((f,w)\\) pairs while maximizing \\(P(D=0\\|f,w)\\) for stochastically sampled “negative” examples, under the assumption that selecting a context for a given word is likely to result in an unobserved \\((f,w)\\) pair.\nSGNS’s objective for a single \\((f,w)\\) output observation is then:\n\\[ E = \\log \\sigma(\\vec{f} \\cdot \\vec{w}) + k \\cdot \\mathbb{E}_{w_{N} \\sim P_{D}} [\\log \\sigma(\\vec{-f} \\cdot \\vec{w}_N)] \\]\nwhere \\(k\\) is the number of “negative” samples and \\(w_{N}\\) is the sampled window, drawn according to the empirical unigram distribution\n\\[P_{D}(w) = \\frac{\\#w}{|D|}\\]\nLet’s disassemble this objective function into its respective terms and put it back together again :\nThe term \\(\\log \\sigma(\\vec{f} \\cdot \\vec{w})\\), from above, is used to model the\nThis object is then trained in an online fashion using stochastic gradient descent updated over the observed pairs in the corpus \\(D\\). The goal objective then sums over the observed \\((f,w)\\) pairs in the corpus :\n\\[ \\ell = \\Sigma_{f \\in V_{f}} \\Sigma_{w \\in V_{w}} \\#(f,w)(\\log \\sigma(\\vec{f} \\cdot \\vec{w}) + k \\cdot \\mathbb{E}_{w_{N} \\sim P_{D}} [\\log \\sigma(\\vec{-f} \\cdot \\vec{w}_N)]\\]\nOptimizing this objective groups flows that have similar embeddings, while scattering unobserved pairs.\n\n\n\n\n\n\nNote\n\n\n\nTODO - further exploration :\n\nRunning true tuples of SRCIP, DSTIP, DSTPORT, and PROTO\nLabel included for now, need to figure out how to persist through pipeline without skewing results - need to figure out how to match up labeling to flow after word2vec has been run\nImplement timestamp window oriented ‘sentence’ creation, current implementation created same length flow ‘sentences’ for every \\(f\\) flow\n\n\n\n\n\nCode\n# Method to slide window over dataframe of \n# flowdata and create \"sentences\"\n\ndef create_corpora(dataframe, window, corpus_count):\n    corpus = []\n    corpora = []\n    begin = 0\n    end = 0\n    for i in range(corpus_count):\n        while end &lt;= window:\n            end += 1\n        else:\n            corpus.append(dataframe[begin:(end-1)])\n        begin = begin + window\n        end = end + window\n    corpora.append(corpus)\n    return(corpora)\n\n\n\n\nCode\ncorpora = create_corpora(subsample_cats, 30, 153333)\nlabels = create_corpora(subsample_labels, 30, 153333)\ncorpora_1 = create_corpora(subsample_cats_1, 30, 153333)\n\n\n\n\nCode\n# Convert all tuples created by previous create_corpora function\n# to strings for use with tokenization which is then used in the\n# word2vec algorithm below \n\nstr_corpora = []\n\nfor corpus in corpora[0]:\n    str_corpus = []\n    for sentence in corpus.values.tolist():\n        str_corpus.append(str(sentence).encode('utf-8'))\n    str_corpora.append(str_corpus)\n\n\n\n\n\nCode\n# Here we train a model without using the negative sampling \n# hyperparameter. We will be using this for testing of \n# accuracy of model vs. using the negative sampling function\n\nflow_model = gensim.models.Word2Vec(str_corpora, \n                                    workers=8, \n                                    vector_size=200, \n                                    window=20, \n                                    min_count=1)\n\n\n\n\nCode\n# Here we train a model using the negative sampling which \n# we will then compare to the model above for the impact \n# that the negative sampling has on the clustering of flows\n\nflow_model_sgns = gensim.models.Word2Vec(str_corpora, \n                                         workers=23, \n                                         vector_size=100, \n                                         window=30, \n                                         negative=10, \n                                         sample=5)"
  },
  {
    "objectID": "posts/flow2vec/index.html#preliminary-results---very-rough-no-real-hyperparameter-tunings-exploration-etc.",
    "href": "posts/flow2vec/index.html#preliminary-results---very-rough-no-real-hyperparameter-tunings-exploration-etc.",
    "title": "NetFlow and word2vec -> flow2vec",
    "section": "Preliminary results - very rough, no real hyperparameter tunings / exploration, etc.",
    "text": "Preliminary results - very rough, no real hyperparameter tunings / exploration, etc.\nWe can see below the results may prove to be useful with respect to certain labels present in the dataset, but not others. This may have to do with the raw occurence rates of certain flow and window #\\[(f,w)\\] combinations vs. others. I use labels lightly as well as this will ultimately become an exercise of semi-supervised learning as it can sometimes be impossible for humans to interpret the results of an unsupervised learning task without any type of contextual insight, as labels can provide. In the case of written language, the “insight” that is provided is the fact that we know what the meanings of words are within the language and if they’re clustering correctly, re: synonyms and antonyms, etc.\nWe can tune for this using subsampling above in the SGNS model. Which will we do next.\n\nTODO:\n\nGridSearch for hyperparameters\n\nHere we see that there is indeed a clustering that has happened with respect to the “From-Botnet-V42-UDP-DNS”\n# Test for flow similarity, preferrably a flow that has the botnet label\n\nflow_model_1.most_similar(\"['147.32.84.165', '192.33.4.12', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\", topn=100)\n[(\"['147.32.84.165', '192.5.5.241', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9761667847633362),\n (\"['147.32.84.165', '202.12.27.33', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9741541743278503),\n (\"['147.32.84.165', '128.8.10.90', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.973616898059845),\n (\"['147.32.84.165', '78.47.76.4', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9714504480361938),\n (\"['147.32.84.165', '193.0.14.129', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9692395925521851),\n (\"['147.32.84.165', '199.7.83.42', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9687032699584961),\n (\"['147.32.84.165', '192.228.79.201', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9674479961395264),\n (\"['147.32.84.165', '192.58.128.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9664252400398254),\n (\"['147.32.84.165', '92.53.98.100', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9656703472137451),\n (\"['147.32.84.165', '192.112.36.4', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9654155969619751),\n (\"['147.32.84.165', '198.41.0.4', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9644977450370789),\n (\"['147.32.84.165', '192.203.230.10', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9633801579475403),\n (\"['147.32.84.165', '192.36.148.17', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9618400931358337),\n (\"['147.32.84.165', '128.63.2.53', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.958657443523407),\n (\"['147.32.84.165', '89.108.64.2', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9581812024116516),\n (\"['147.32.84.165', '82.103.128.82', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9558319449424744),\n (\"['147.32.84.165', '192.42.93.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9557339549064636),\n (\"['147.32.84.165', '192.26.92.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9556182026863098),\n (\"['147.32.84.165', '194.226.96.8', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9543852210044861),\n (\"['147.32.84.165', '194.85.61.20', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.953228771686554),\n (\"['147.32.84.165', '88.212.196.130', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9526883959770203),\n (\"['147.32.84.165', '195.128.49.14', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9500119090080261),\n (\"['147.32.84.165', '217.16.20.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9483109712600708),\n (\"['147.32.84.165', '85.10.210.157', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9481122493743896),\n (\"['147.32.84.165', '92.53.116.200', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9478355050086975),\n (\"['147.32.84.165', '88.212.221.11', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9470769166946411),\n (\"['147.32.84.165', '82.146.55.155', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9461535811424255),\n (\"['147.32.84.165', '192.41.162.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9459192156791687),\n (\"['147.32.84.165', '77.222.40.2', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9456772804260254),\n (\"['147.32.84.165', '199.19.57.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.945094645023346),\n (\"['147.32.84.165', '89.253.192.21', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9428556561470032),\n (\"['147.32.84.165', '199.249.120.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9426734447479248),\n (\"['147.32.84.165', '192.54.112.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9423930048942566),\n (\"['147.32.84.165', '195.2.83.38', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9414822459220886),\n (\"['147.32.84.165', '89.108.104.3', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9414548873901367),\n (\"['147.32.84.165', '78.108.89.252', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9414442181587219),\n (\"['147.32.84.165', '80.93.50.53', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9408544898033142),\n (\"['147.32.84.165', '192.31.80.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9401237368583679),\n (\"['147.32.84.165', '195.161.112.91', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.939973771572113),\n (\"['147.32.84.165', '193.169.178.59', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9395020008087158),\n (\"['147.32.84.165', '192.48.79.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9393561482429504),\n (\"['147.32.84.165', '192.33.14.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9386749267578125),\n (\"['147.32.84.165', '85.10.210.144', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9382632970809937),\n (\"['147.32.84.165', '192.12.94.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9372074007987976),\n (\"['147.32.84.165', '192.35.51.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9371063113212585),\n (\"['147.32.84.165', '213.177.97.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9366581439971924),\n (\"['147.32.84.165', '95.163.69.51', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9363342523574829),\n (\"['147.32.84.165', '79.174.72.215', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.936087965965271),\n (\"['147.32.84.165', '195.248.235.219', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9358514547348022),\n (\"['147.32.84.165', '217.16.16.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9352473020553589),\n (\"['147.32.84.165', '78.108.81.247', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9348022937774658),\n (\"['147.32.84.165', '192.5.6.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.934520423412323),\n (\"['147.32.84.165', '199.19.56.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.934291422367096),\n (\"['147.32.84.165', '217.16.22.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9341065883636475),\n (\"['147.32.84.165', '192.36.144.107', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9333975315093994),\n (\"['147.32.84.165', '81.177.24.54', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9332102537155151),\n (\"['147.32.84.165', '192.52.178.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9328247308731079),\n (\"['147.32.84.165', '83.222.0.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9324507117271423),\n (\"['147.32.84.165', '95.168.160.245', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9320420026779175),\n (\"['147.32.84.165', '95.168.174.25', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9319052696228027),\n (\"['147.32.84.165', '80.93.56.2', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9313104748725891),\n (\"['147.32.84.165', '193.227.240.37', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9309282302856445),\n (\"['147.32.84.165', '208.100.5.254', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9303311705589294),\n (\"['147.32.84.165', '77.221.130.250', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9299085140228271),\n (\"['147.32.84.165', '192.55.83.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9297108054161072),\n (\"['147.32.84.165', '84.252.138.21', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9296650886535645),\n (\"['147.32.84.165', '192.43.172.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.928945779800415),\n (\"['147.32.84.165', '89.111.177.253', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9288318753242493),\n (\"['147.32.84.165', '195.2.64.38', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9286403059959412),\n (\"['147.32.84.165', '195.128.50.221', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9278726577758789),\n (\"['147.32.84.165', '178.218.208.130', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9271195530891418),\n (\"['147.32.84.165', '192.36.125.2', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9268661141395569),\n (\"['147.32.84.165', '199.19.54.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9267032146453857),\n (\"['147.32.84.165', '79.137.226.102', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9260225296020508),\n (\"['147.32.84.165', '193.232.130.14', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9259271621704102),\n (\"['147.32.84.165', '193.232.142.17', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9246711730957031),\n (\"['147.32.84.165', '78.47.139.101', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.924452006816864),\n (\"['147.32.84.165', '217.174.106.66', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9236003756523132),\n (\"['147.32.84.165', '77.222.41.3', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9235631823539734),\n (\"['147.32.84.165', '83.222.1.30', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9203209280967712),\n (\"['147.32.84.165', '91.217.21.170', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9194321632385254),\n (\"['147.32.84.165', '89.108.122.149', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.919041633605957),\n (\"['147.32.84.165', '91.217.20.170', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9166457056999207),\n (\"['147.32.84.165', '193.227.240.38', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9165226221084595),\n (\"['147.32.84.165', '78.108.80.90', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9164752960205078),\n (\"['147.32.84.165', '78.110.50.60', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.915980875492096),\n (\"['147.32.84.165', '178.162.177.145', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9158682227134705),\n (\"['147.32.84.165', '194.85.252.62', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.915434718132019),\n (\"['147.32.84.165', '77.221.159.237', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9152796864509583),\n (\"['147.32.84.165', '193.232.146.170', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9140732884407043),\n (\"['147.32.84.165', '199.249.112.1', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9137414693832397),\n (\"['147.32.84.165', '87.224.128.4', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9121522307395935),\n (\"['147.32.84.165', '93.170.25.253', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9113649725914001),\n (\"['147.32.84.165', '195.209.63.181', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9110352993011475),\n (\"['147.32.84.165', '195.243.137.26', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9104222059249878),\n (\"['147.32.84.165', '194.0.0.53', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9094029068946838),\n (\"['147.32.84.165', '91.218.228.18', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9092046022415161),\n (\"['147.32.84.165', '194.85.105.17', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9091553092002869),\n (\"['147.32.84.165', '193.232.156.17', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9083877801895142),\n (\"['147.32.84.165', '212.176.27.2', '53', 'udp', 'flow=From-Botnet-V42-UDP-DNS']\",\n  0.9074791669845581)]\nflow_model_1.most_similar(\"['147.32.84.165', '60.190.223.75', '888', 'tcp', 'flow=From-Botnet-V42-TCP-CC6-Plain-HTTP-Encrypted-Data']\")\n[(\"['217.66.146.105', '147.32.84.229', '443', 'tcp', 'flow=Background-TCP-Established']\",\n  0.970333993434906),\n (\"['188.26.176.163', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n  0.963600218296051),\n (\"['114.75.11.242', '147.32.84.229', '80', 'tcp', 'flow=Background-TCP-Established']\",\n  0.9627201557159424),\n (\"['147.32.86.96', '147.32.87.29', '0xb612', 'icmp', 'flow=Background']\",\n  0.9622609615325928),\n (\"['195.234.241.9', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n  0.9621870517730713),\n (\"['41.130.66.62', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n  0.9606925249099731),\n (\"['131.104.149.212', '147.32.84.229', '13363', 'udp', 'flow=Background-UDP-Established']\",\n  0.9604771733283997),\n (\"['147.32.84.59', '90.146.27.130', '46356', 'udp', 'flow=Background-Attempt-cmpgw-CVUT']\",\n  0.9597481489181519),\n (\"['147.32.84.229', '78.141.179.11', '34046', 'udp', 'flow=Background-UDP-Established']\",\n  0.9597265720367432),\n (\"['147.32.84.59', '114.40.199.143', '21323', 'udp', 'flow=Background-Established-cmpgw-CVUT']\",\n  0.9592392444610596)]\n\n\nWithout label contained in the dataset\nHere we run the same hyperparameters for the word2vec algorith, this time ignoring the label and not adding it to the “word” representations.\nflow_model_2 = gensim.models.Word2Vec(str_corpora, workers=23, size=100, window=30, negative=10, sample=5)\nflow_model_2.most_similar(\"['147.32.84.165', '192.33.4.12', '53', 'udp']\")\n[(\"['147.32.84.165', '192.112.36.4', '53', 'udp']\", 0.9759483337402344),\n (\"['147.32.84.165', '193.0.14.129', '53', 'udp']\", 0.9724588394165039),\n (\"['147.32.84.165', '192.5.5.241', '53', 'udp']\", 0.9721120595932007),\n (\"['147.32.84.165', '128.8.10.90', '53', 'udp']\", 0.9712154865264893),\n (\"['147.32.84.165', '192.58.128.30', '53', 'udp']\", 0.9697802662849426),\n (\"['147.32.84.165', '192.36.148.17', '53', 'udp']\", 0.9674890041351318),\n (\"['147.32.84.165', '198.41.0.4', '53', 'udp']\", 0.9672064185142517),\n (\"['147.32.84.165', '199.7.83.42', '53', 'udp']\", 0.9657577872276306),\n (\"['147.32.84.165', '202.12.27.33', '53', 'udp']\", 0.9610617160797119),\n (\"['147.32.84.165', '192.203.230.10', '53', 'udp']\", 0.9608649015426636)]\nflowdata[flowdata['DstAddr'].str.contains(\"192.112.36.4\", na=False)].head(n=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStart Time\nDur\nProto\nSrcAddr\nSport\nDir\nDstAddr\nDport\nState\nsTos\ndTos\nTotPkts\nTotBytes\nSrcBytes\nLabel\n\n\n\n\n\n1264477\n2011/08/10 12:29:05.687373\n0.258197\nudp\n147.32.84.165\n2077\n&lt;-&gt;\n192.112.36.4\n53\nCON\n0\n0\n2\n528\n68\nflow=From-Botnet-V42-UDP-DNS\n\n\n\n1264673\n2011/08/10 12:29:06.093217\n0.258987\nudp\n147.32.84.165\n2077\n&lt;-&gt;\n192.112.36.4\n53\nCON\n0\n0\n2\n611\n77\nflow=From-Botnet-V42-UDP-DNS\n\n\n\n\nvocab_flow = []\n\nfor flow in flow_model_2.vocab.items():\n    if re.search(r\"192.112.36.4\", flow[0]):\n        vocab_flow.append(flow)\nvocab_flow\n[(\"['147.32.84.165', '192.112.36.4', '53', 'udp']\",\n  &lt;gensim.models.word2vec.Vocab at 0x7f808752a350&gt;)]\nflowdata[flowdata['DstAddr'].str.contains(\"192.5.5.241\", na=False)].head(n=2)\n\n\n\nAggregated flows, equivalent to “phrases”\nThe word2vec algorithm can also learn embeddings for phrases as well as single words for written language. The ideas I have surrounding “phrases” would be learning the embeddings for given windows of flows, if they were to present themselves in certain capacities within the captures flow data.\nThe current flow data that this notebook is based around are aggregated flows for bi-directional communication between endpoints. Exploiting something like capturing the ‘phrase’ of a flow, or thought another way, the bi-directional communication patterns that are contained within flow data might prove useful for application profiling, etc. through the use of application meta-data tracked through some sort of semi-supervised learning pipeline."
  },
  {
    "objectID": "posts/flow2vec/index.html#cluster-visualization",
    "href": "posts/flow2vec/index.html#cluster-visualization",
    "title": "NetFlow and word2vec -> flow2vec",
    "section": "Cluster visualization",
    "text": "Cluster visualization\nRaw flow vectors \\(V_{f}\\), created by word2vec, are embedded in dimensionality equivalent to the input layer of the shallow neural network that is used within the model.\n\nt-SNE Visualization\nUse t-SNE and matplotlib to visualize the clusters created using Word2Vec.\n\n\n\n\n\n\nNote\n\n\n\nTODO :\n\nBrief explanation of the tSNE algorithm and how it handles compressing higher dimensional data into 2 or 3 dimension for visualization\n\n\n\ndef perform_tsne(word_vector):\n    tsne = manifold.TSNE(n_components=2, random_state=42)\n    return tsne.fit_transform(word_vector)\n#flow_model_reduced = TruncatedSVD(n_components=100, random_state=42).fit_transform(flow_model_1.syn0)\ntest_tsne = manifold.TSNE(n_components=2, learning_rate=50).fit_transform(flow_model_1.syn0[0:4000])\nfig, ax = plt.subplots(subplot_kw=dict(axisbg='#EEEEEE'), figsize=(10, 10))\n\nx = test_tsne[:,0]\ny = test_tsne[:,1]\n\nmpld3_scatter = ax.scatter(x, y, cmap='Blues', c = y)\nax.grid(color='white', linestyle='solid')\n\nlabels = [v[0] for k,v in enumerate(flow_model_1.vocab.items()[0-4000:])]\ntooltip = mpld3.plugins.PointLabelTooltip(mpld3_scatter, labels=labels)\nmpld3.plugins.connect(fig, tooltip)\nfig, ax = plt.subplots(subplot_kw=dict(axisbg='#EEEEEE'), figsize=(10, 10))\n\n\nmpld3_scatter = ax.scatter(tsne_objs[0][:, 0], tsne_objs[0][:, 1])\nax.grid(color='white', linestyle='solid')\n\n#ax.set_title(\"Scatter Plot (with tooltips!)\", size=20)\n\n#labels = [v[0][0] for k,v in enumerate(sample)]\ntooltip = mpld3.plugins.PointLabelTooltip(mpld3_scatter)\nmpld3.plugins.connect(fig, tooltip)\nfig = plt.figure(figsize=(70, 70))\nax = plt.axes(frameon=False)\nplt.setp(ax,xticks=(), yticks=())\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.9,\n                wspace=0.0, hspace=0.0)\nplt.scatter(flow_model_embedded_1[:, 0], flow_model_embedded_1[:, 1], marker=\"x\")\n\n#for k,v in enumerate(flow_model.vocab.items()):\n#    plt.annotate(v[0], flow_model_embedded_1[k])\n\nplt.savefig('test2.eps', format='eps', dpi=600)\n\n\n\n\n\n\nImportant\n\n\n\nThings left to research / validate / test\n\nTune hyperparameters of models for all algorithms – word2vec, kmeans, tSNE\nFind fixes for limitations of larger datasets for tooling that has dependencies on numpy – kmeans, tSNE"
  },
  {
    "objectID": "posts/first-class-functions-in-python/index.html",
    "href": "posts/first-class-functions-in-python/index.html",
    "title": "First Class Functions in Python",
    "section": "",
    "text": "Typically first class functions are defined as a programming entity that can be :\n\nCreated at runtime\nAssigned to a variable or element in a data structure\nPassed as an argument\nReturned as the result of a function\n\nBy this definition, looking at how Python treats all functions, all functions are first class within Python.\nBelow we’ll see examples of exactly how this looks.\n\n\ndef factorial(n):\n    \"\"\"\n    Returns n! or n(factorial)\n    \n    e.g 5! = 5 * 4 * 3 * 2\n    \"\"\"\n    return 1 if n &lt; 2 else n * factorial(n-1)\n\nfactorial(5)\n120\n\n\n\nWe can show the first class nature of this function object using a few examples.\nWe can assign the function to a variable, which will invoke the function when calling that variable.\nfact = factorial\nfact(5)\n120\nWe can also use the map function, and pass our function as the first argument, allowing that function to be evaluated against the second argument, which is an iterable. Allowing this function to be applied in a successive fashion to all elements of this iterable.\nmap(factorial, range(10))\n[1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880]\n\n\n\nA higher order function is a bit….meta. It can take, as an argument, a function and then returns a function as a result.\nThe map() example used above is a great example of this.\nThe built-in sorted() function is another great example of this, within Python. We can pass it an iterable, along with a key that can then be applied in succession to the items in the list.\nfood = ['eggplant', 'carrots', 'celery', \n        'potatoes', 'tomatoes', 'rhubarb',\n        'strawberry', 'blueberry', 'raspberry',\n        'banana', 'cherry']\n\nprint(sorted(food, key=len))\n['celery', 'banana', 'cherry', 'carrots', 'rhubarb', 'eggplant', 'potatoes', 'tomatoes', 'blueberry', 'raspberry', 'strawberry']\nAny single argument function can be used in the key argument of the sorted() method.\nas a trivial example, we may want to use the reversed order of the characters to sort of words, as this will cause certain clustering of character strings together, such as -berry, and -toes.\ndef reverse(word):\n    '''\n    Reverse the order of the letters in a given string\n    '''\n    return word[::-1]\n\nprint(sorted(food, key=reverse))\n['banana', 'rhubarb', 'tomatoes', 'potatoes', 'carrots', 'eggplant', 'celery', 'blueberry', 'raspberry', 'strawberry', 'cherry']\n\n\n\nMap, filter, and reduce are typically offered in functional languages as higher order functions. However, the introduction of list comprehensions and generator expressions have downplayed the value of the map and filter functions, as listcomp’s and genexp’s combine the job of map and filter.\n# Build a list of factorials from 0! to 5!\nlist(map(fact, range(6)))\n[1, 1, 2, 6, 24, 120]\n# Build a list of factorials from 0! to 5!\n# but using list comprehension\n[fact(n) for n in range(6)]\n[1, 1, 2, 6, 24, 120]\n# Build a list of factorials of odd numbers up to 5!, using `map` and `filter`\nlist(map(factorial, filter(lambda n: n % 2, range(6))))\n[1, 6, 120]\nWe can see above that with the map and filter functions, we needed to use a lambda function.\nUsing a list comprehension can remove this requirement, and concatenate the operations.\n# Build a list of factorials of odd numbers up to 5!, using list comprehension\n[factorial(n) for n in range(6) if n % 2]\n[1, 6, 120]\n\n\n\nThe example above, where we’ve utilized map and filter combined with a lambda function leads us into our next example.\nThe lambda keyword created an anonymous function within a Python expression. However the syntax limits the lambda to be pure expressions. This means that the body of a lambda function can’t use other Python statements such as while or try, etc.\nThese are typically limited in their use because of the lack of the ability to use more complex control structures within the lambda functions. This can lead to unreadable or unworkable results.\nHowever, they can still prove useful in certain contexts, such as list arguments.\nfood = ['eggplant', 'carrots', 'celery', \n        'potatoes', 'tomatoes', 'rhubarb',\n        'strawberry', 'blueberry', 'raspberry',\n        'banana', 'cherry']\n\nprint(sorted(food, key=lambda word: word[::-1]))\n['banana', 'rhubarb', 'tomatoes', 'potatoes', 'carrots', 'eggplant', 'celery', 'blueberry', 'raspberry', 'strawberry', 'cherry']"
  },
  {
    "objectID": "posts/summer-of-machine-learning-2017/index.html",
    "href": "posts/summer-of-machine-learning-2017/index.html",
    "title": "Summer of Machine Learning 2017",
    "section": "",
    "text": "Disconnected\nSomething has been bothering me over the last couple of years. As I’ve progressed in my career I subsequently feel as though I’ve almost entirely disconnected with what helped me launch this part of the career to begin with. Posting what I learn about online, in an effort to help bolster the amount of useful information that is available in the vast sea of garbage that is the internet.\nChris Albon recently posted on his blog about an effort he’s going to make this summer with respect to bettering himself both professionally and personally. This post really hit home for me because it’s something that I feel like I’ve wanted to do for quite some time now. But I always feel as though the stuff I may write about or the experiments that I may run somehow won’t be “up to snuff” with others in the machine learning world because I don’t have the proper pedigree of ivy league computer science education. This summer I am hoping to run these fears down, as I know I’ve produced many good works in my professional role, even though I couldn’t publicize them.\n\n\nCommunity\nThere has also been quite the backlack on social media lately with respect to the requirement of a mathematical pedigree, see here, here, and here, etc., in order to be effective in the understanding and application of machine learning. While I do believe that having a concrete understanding of the mathematics that underly much of the machine learning ideas and processes today, I don’t see it as an ultimately impossible field to get into if you have the drive and willingness to spend time staring at equations in hopes of maybe not understanding them, but rather getting to used to them, to paraphrase von Neumann.\nI’ve also recently enrolled in classes at the local community college in an effort to bolster my understanding of the mathematical landscape that is the underpinnings of the probability, statistics, linear algebra, calculus, etc. that are all required to get used to the tools and methods that are used in machine learning, and more specifically deep learning. I wasn’t able to afford college, and still cannot afford a large university’s tuition, when I was the typical age that most attend, but I’ve been fortunate enough to find myself, now, in a position to fund my own education and I am now doing just that. I used to think of this as a personal flaw, for quite some time, but now I see that it was truly one of the things in my life that helped me develop a certain drive that I believe was required to foster the skills to build a career in technology.\nI have a deep passion for communcation in general, as most of the career has been spent in the realm of information technology infrastructure, specifically in the areas of data networks and distributed systems, and I look to ideas in papers such as Learning to Communicate with Deep Multi-Agent Reinforcement Learning as inspiration for ways that I may be able to apply this research to what I know and love, as well.\nI really just hope to continue fostering and building the communities required to make all of these amazing ideas and technologies flourish.\n\n\nSummed up\nAll of this said, I’m going to follow in Chris’ footsteps and work toward not only bettering myself, but also giving back as much as I can to the rest of the world in the same way that the people whom I’ve learned from, have. Chris set goals in his post, and I think I’ll try to do the same.\n\nGoals :\n\nWork my way through, and complete the many false starts I’ve had with Christopher Bishop’s Pattern Rognition and Machine Learning\nSame goes for the Deep Learning book from Goodfellow et. al.\nContribute as much as I can of my system’s background to the community (I’ll start with the Docker containers I’ve built for reusability and reproducability here Docker Images)\nI’ve lost 80 lbs since last year through running, but it’s time to switch it up, so I’ll start a Freeletics regimen for supplement while cutting back on running\n\n\nChris had more specific goals than I do, but I am intentionally leaving mine less concrete as I want them to be able to change and grow with me as I re-integrate back into the community as a whole, whether it be the machine learning community, or the one that I hold closest today, IT infrastructure, to be as general as possible.\nI hope this post wasn’t received as too “cheesy”, for lack of a better word, but I do feel like I want to start giving back again after a 2+ year haitus.\nI kinda ripped off the formatting of your spreadsheet Chris, I really hope you don’t mind. Thank you for being the inspiration that has finally pushed me start something more meaningful for myself and the community."
  },
  {
    "objectID": "posts/hashing-in-python/index.html",
    "href": "posts/hashing-in-python/index.html",
    "title": "Hashing in Python",
    "section": "",
    "text": "Hashing can be useful in speeding up the search process for a specific item that is part of a larger collection of items. Depending on the implementation of the hashing algorithm, this can turn the computational complexity of our search algorithm from \\(O(n)\\) to \\(O(1)\\). We do this by building a specific data structure, which we’ll dive into next.\n\n\nA hash table is a collection of items, stored in such a way as to make it easier to find them later. The table consists of slots that hold items and are named by a specific integer value, starting with 0.\nExample of a hash table (sorry for the poor formatting because markdown :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n\nEach entry in this hash table, is currently set to a value of None.\nA hash function is used when mapping values into the slots available within a Hash table. The hash function typically takes, as input, an item from a collection, and will return an integer in the range of slot names, between \\(0\\) and \\(m-1\\). There are many different hash functions, but the first we can discuss is the “remainder method” hash function.\n\n\n\nThe remainder hash function takes an item from a collection, divides it by the table size, returning the remainder of it’s hash value. Typically modulo arithmetic is present in some form for all hash functions, as the result must be in the range of the total number of slots within the table.\nAssuming we have a set of integer items \\(\\{25,54,34,67,75,21,77,31\\}\\), we can use our hash function to find slots for our values, accordingly.\nitems = [25,54,34,67,75,21,77,31]\n\ndef hash(item_list, table_size):\n    hash_table = dict([(i,None) for i,x in enumerate(range(table_size))])\n    for item in item_list:\n        i = item % table_size\n        print(\"The hash for %s is %s\" % (item, i))\n        hash_table[i] = item\n    \n    return hash_table\n\n# Execute the hash function\n# Create table with 11 entries to match example above\nhash_table = hash(items, 11)\n\n# Print the resulting hash table\nprint(hash_table)\nThe hash for 25 is 3\nThe hash for 54 is 10\nThe hash for 34 is 1\nThe hash for 67 is 1\nThe hash for 75 is 9\nThe hash for 21 is 10\nThe hash for 77 is 0\nThe hash for 31 is 9\n{0: 77, 1: 67, 2: None, 3: 25, 4: None, 5: None, 6: None, 7: None, 8: None, 9: 31, 10: 21}\nOnce the hash values have been computed, we inset each item into the hash table at the designated position(s). We can now see that there are entries with corresponding hash values stored in a python dictionary. This is obviously a very simple implementation of a hash table.\nThere is something interesting to note here, though, when working through using a simple hashing algorithm like the remainder method. We have items, in our case integers, which hash to the same value. Specifically, we can see that there are 2 items that hash to each of the 1, 9, and 10 slots. These are what are known as collisions.\nClearly these collisions can cause problems, as out of the 8 initial items that we’d started with, we only have 5 items actually stored in our hash table. This leads us into the next section we’ll discuss, and that is hash functions that can help alleviate this collision problem.\n\n\n\nHash functions that map, perfectly, every item into it’s own unique slot in a hash table is known as a perfect hash function. If we knew the collection of items and that it would never change, it’s possible to construct a perfect hash function specific to this collection, but we know that the dynamics of the real world tend to not allow something so simple.\nDynamically growing the hash table size so each possible item in the item range can be accomodated is one way to construct a perfect hash function. This guarantees each item will have it’s own slot. But this isn’t feasible, as something as simple as tracking social security numbers would require over one billion slots within the hash table. And if we’re only tracking a small subset of the full set of social security numbers, this would become horribly inefficient with respect to hardware resources available within the machine our code is running on.\nWith the goal of constructing a hash function that will minimize the number of collisions, has low computational complexity, and evenly distributes our items within the hash table, we can take a look at some common ways to extend this remainder method.\n\n\nThe folding method for hashing an item begins by diving the item into equal size pieces (though the last piece may not be of equal size to the rest). These pieces are then added together to create the resulting hash value. A good example of this is a phone number,such as 456-555-1234. We can break each pair of integers up into groups of 2, add them up, and use that resulting value as an input to our hashing function.\ndef stringify(item_list):\n    \"\"\"\n    Method to convert integer values into array of component integers\n    \"\"\"\n    string_items = []\n    while len(item_list) &gt; 0:\n        for item in item_list:\n            chars = [int(c) for c in str(item)]\n        item_list.remove(item)\n        string_items.append(chars)\n    return string_items\n\ndef folding_hash(item_list):\n    '''\n    Quick hack at a folding hash algorithm\n    '''\n    hashes = []\n    while len(item_list) &gt; 0:\n        hash_val = 0\n        for item in item_list:\n            while len(item) &gt; 1:\n                str_1 = str(item[0])\n                str_2 = str(item[1])\n                str_concat = str_1 + str_2\n                bifold = int(str_concat)\n                hash_val += bifold\n                item.pop(0)\n                item.pop(0)\n            else:\n                if len(item) &gt; 0:\n                    hash_val += item[0]\n                else:\n                    pass\n            hashes.append(hash_val)\n        return hashes\n\n# Example phone numbers\nphone_number = [4565551234, 4565557714, 9871542544, 4365554601]\n\n# String/Character-fy the phone numbers\nstr_pn = stringify(phone_number)\n\n# Hash the phone numbers\nfolded_hash = folding_hash(str_pn)\n\n# Input values into hash table\nfolding_hash_table = hash(folded_hash, 11)\n\n# Print the results\nprint(folding_hash_table)\nThe hash for 210 is 1\nThe hash for 502 is 7\nThe hash for 758 is 10\nThe hash for 969 is 1\n{0: None, 1: 969, 2: None, 3: None, 4: None, 5: None, 6: None, 7: 502, 8: None, 9: None, 10: 758}\n\n\n\nWhen dealing with strings, we can use the ordinal values of the constituent characters of a given word, to create a hash.\nIt’s important to notice, however, that anagrams can produce hash collisions, as shown below.\ndef ord_hash(string, table_size):\n    hash_val = 0\n    for position in range(len(string)):\n        hash_val = hash_val + ord(string[position])\n        \n    return hash_val % table_size\n\nprint(ord_hash(\"cat\", 11))\nprint(ord_hash(\"tac\", 11))\n4\n4\n\n\n\nIn the case above, just using ordinal values can cause hash collisions. We can actually use the positional structure of the word to as a set of weights for generating a given hash. As seen below.\nA simple multiplication by the positional value of each character will cause anagrams to evaluate to different hash values.\ndef weighted_ord_hash(string, table_size):\n    hash_val = 0\n    for position in range(len(string)):\n        hash_val = hash_val + (ord(string[position]) * position)\n    return hash_val % table_size\n\n# ord_hash\nprint(ord_hash(\"cat\", 11))\n\n# weighted_ord_hash\nprint(weighted_ord_hash(\"tac\", 11))\n4\n9"
  },
  {
    "objectID": "posts/hashing-in-python/index.html#collision-resolution",
    "href": "posts/hashing-in-python/index.html#collision-resolution",
    "title": "Hashing in Python",
    "section": "Collision Resolution",
    "text": "Collision Resolution\nWhen there are hash collisions, like we’ve seen previously, it’s important to understand ways that we can alleviate the collisions.\nOne simple way to handle the collision, should there already be an entry in our hash table with the same hash value, is to search sequentially through all slots near the original hash, for an empty slot. This may require us to circularly traverse the entire hash table to allow us to cover all possible slots. This process is known as open addressing and the technique within this process that we’re using is called linear probing.\nIn the following code examples, we’ll reuse the simple remainder method hash function that we’ve defined above. Along with the original set of integers we were hashing, as there were some collisions that occured.\nitems = [25,54,34,67,75,21,77,31]\n\n# Execute the hash function\n# Create table with 11 entries to match example above\nhash_table = hash(items, 11)\n\n# Print the resulting hash table\nprint(hash_table)\nThe hash for 25 is 3\nThe hash for 54 is 10\nThe hash for 34 is 1\nThe hash for 67 is 1\nThe hash for 75 is 9\nThe hash for 21 is 10\nThe hash for 77 is 0\nThe hash for 31 is 9\n{0: 77, 1: 67, 2: None, 3: 25, 4: None, 5: None, 6: None, 7: None, 8: None, 9: 31, 10: 21}\nWe can see there were multiple collisions within this dataset. Specifically hashes of 1, 9, and 10. And we can see in the resulting table that only the last computed hashes are stored in the respective table slots.\nBelow we’ll implement an lp_hash function that will perform linear probing over the slots available within the table for any collisions that occur.\nitems = [25,54,34,67,75,21,77,31]\n\ndef rehash(oldhash, table_size):\n    return (oldhash+1) % table_size\n\ndef lp_hash(item_list, table_size):\n    \n    lp_hash_table = dict([(i,None) for i,x in enumerate(range(table_size))])\n\n    for item in item_list:\n        i = item % table_size\n        print(\"%s hashed == %s \\n\" %(item, i))\n        if lp_hash_table[i] == None:\n            lp_hash_table[i] = item\n        elif lp_hash_table[i] != None:\n            print(\"Collision, attempting linear probe \\n\")\n            next_slot = rehash(i, table_size)\n            print(\"Setting next slot to %s \\n\" % next_slot)\n            while lp_hash_table[next_slot] != None:\n                next_slot = rehash(next_slot, len(lp_hash_table.keys()))\n                print(\"Next slot was not empty, trying next slot %s \\n\" % next_slot)\n            if lp_hash_table[next_slot] == None:\n                lp_hash_table[next_slot] = item\n    return lp_hash_table\n\nprint(lp_hash(items, 11))\n25 hashed == 3 \n\n54 hashed == 10 \n\n34 hashed == 1 \n\n67 hashed == 1 \n\nCollision, attempting linear probe \n\nSetting next slot to 2 \n\n75 hashed == 9 \n\n21 hashed == 10 \n\nCollision, attempting linear probe \n\nSetting next slot to 0 \n\n77 hashed == 0 \n\nCollision, attempting linear probe \n\nSetting next slot to 1 \n\nNext slot was not empty, trying next slot 2 \n\nNext slot was not empty, trying next slot 3 \n\nNext slot was not empty, trying next slot 4 \n\n31 hashed == 9 \n\nCollision, attempting linear probe \n\nSetting next slot to 10 \n\nNext slot was not empty, trying next slot 0 \n\nNext slot was not empty, trying next slot 1 \n\nNext slot was not empty, trying next slot 2 \n\nNext slot was not empty, trying next slot 3 \n\nNext slot was not empty, trying next slot 4 \n\nNext slot was not empty, trying next slot 5 \n\n{0: 21, 1: 34, 2: 67, 3: 25, 4: 77, 5: 31, 6: None, 7: None, 8: None, 9: 75, 10: 54}\nUsed a little more interestingly, we can use the weighted ordinal hash function that we’ve defined above, combined with the lp_hash function that we’ve just defined, to store string(s) for later lookup.\nanimal_items = [\"cat\", \"dog\", \"goat\", \n         \"chicken\", \"pig\", \"horse\",\n        \"ostrich\", \"lion\", \"puma\"]\n\ndef rehash(oldhash, table_size):\n    return (oldhash+1) % table_size\n\ndef weighted_ord_hash(string, table_size):\n    hash_val = 0\n    for position in range(len(string)):\n        hash_val = hash_val + (ord(string[position]) * position)\n    return hash_val % table_size\n\n\ndef lp_hash(item_list, table_size):\n    \n    lp_hash_table = dict([(i,None) for i,x in enumerate(range(table_size))])\n    \n    for item in item_list:\n        i = weighted_ord_hash(item, table_size)\n        print(\"%s hashed == %s \\n\" %(item, i))\n        if lp_hash_table[i] == None:\n            lp_hash_table[i] = item\n        elif lp_hash_table[i] != None:\n            print(\"Collision, attempting linear probe \\n\")\n            next_slot = rehash(i, table_size)\n            print(\"Setting next slot to %s \\n\" % next_slot)\n            while lp_hash_table[next_slot] != None:\n                next_slot = rehash(next_slot, len(lp_hash_table.keys()))\n                print(\"Next slot was not empty, trying next slot %s \\n\" % next_slot)\n            if lp_hash_table[next_slot] == None:\n                lp_hash_table[next_slot] = item\n    return lp_hash_table\n\nprint(lp_hash(animal_items, 11))\ncat hashed == 10 \n\ndog hashed == 9 \n\ngoat hashed == 4 \n\nchicken hashed == 4 \n\nCollision, attempting linear probe \n\nSetting next slot to 5 \n\npig hashed == 3 \n\nhorse hashed == 10 \n\nCollision, attempting linear probe \n\nSetting next slot to 0 \n\nostrich hashed == 6 \n\nlion hashed == 8 \n\npuma hashed == 10 \n\nCollision, attempting linear probe \n\nSetting next slot to 0 \n\nNext slot was not empty, trying next slot 1 \n\n{0: 'horse', 1: 'puma', 2: None, 3: 'pig', 4: 'goat', 5: 'chicken', 6: 'ostrich', 7: None, 8: 'lion', 9: 'dog', 10: 'cat'}"
  },
  {
    "objectID": "posts/sequential-and-binary-search-in-python/index.html",
    "href": "posts/sequential-and-binary-search-in-python/index.html",
    "title": "Sequential and Binary Search in Python",
    "section": "",
    "text": "This notebook will include examples of searching and sorting algorithms implemented in python. It is both for my own learning, and for anyone else who would like to use this notebook for anything they’d like."
  },
  {
    "objectID": "posts/sequential-and-binary-search-in-python/index.html#searching",
    "href": "posts/sequential-and-binary-search-in-python/index.html#searching",
    "title": "Sequential and Binary Search in Python",
    "section": "Searching",
    "text": "Searching\nFinding an item in a collection of items is a pretty typical search problem. Depending on the implementation, a search will tend to return a True or False boolean answer to the question of “is this item contained within this collection of items?”.\nAn example of this can be seen below, using Pythons in operator.\n# Finding a single integer in an array of integers using Python's `in` \n# operator\n\n15 in [3,5,6,9,12,11]\nFalse\nWe can see this returns a boolean answer of False, indicating that the integer isn’t present in the array.\nBelow is another example where the answer is True.\n# Finding a single integer in an array of integers using Python's `in` \n# operator\n\n11 in [3,5,6,9,12,11]\nTrue\nPython provides useful abstractions like this for a lot of search and sort functionality, but it’s important to understand what’s going on ‘under the hood’ of these functions."
  },
  {
    "objectID": "posts/sequential-and-binary-search-in-python/index.html#sequential-search",
    "href": "posts/sequential-and-binary-search-in-python/index.html#sequential-search",
    "title": "Sequential and Binary Search in Python",
    "section": "Sequential Search",
    "text": "Sequential Search\n\nUnordered array\nDatum, in arrays such as the ones used in the examples above, are typically stores in a collection such as a list. These datum within these lists have linear, or sequential relationship. They are each stores in a position within the array, relative to the other datum.\nWhen searching for a specific datum within the array, we are able to seqeuntially evaluate each item in the list, or array, to see if it matches the item we’re looking for.\nUsing sequential_search, we simply move from item to item in the list, evaluating whether our search expression is True, or False.\n# Search sequentially through a list, incrementing the position counter\n# if is_present is not True, otherwise set is_present to True and return\n\ndef sequential_search(li, item):\n    position = 0\n    is_present = False\n    \n    while position &lt; len(li) and not is_present:\n        if li[position] == item:\n            is_present = True\n        else:\n            position = position + 1\n    \n    return is_present\n\ntest_array = [1,31,5,18,7,10,25]\nprint(sequential_search(test_array, 2))\nprint(sequential_search(test_array, 25))\nFalse\nTrue\nThe example above uses an example of uses an unordered list. Because this list is unordered, we will need to evaluate every item in the list to understand if it is the item that we’re searching for. Because this is the case, the computational complexity of our sequential_search function is \\(O(n)\\).\nHere is a table summarizing the cases :\n\n\n\nCase\nBest Case\nWorst Case\nAverage Case\n\n\n\n\nitem is present\n1\n\\(n\\)\n\\(\\frac{n}{2}\\)\n\n\nitem isn’t present\n\\(n\\)\n\\(n\\)\n\\(n\\)\n\n\n\nThis can be seen as such :\nFor every \\(n\\) and every input size of \\(n\\), the following is true:\n\nThe while loop is executed at most \\(n\\) times\nposition is incremented on each iteration, so position &gt; \\(n\\) after \\(n\\) iterations.\nEach iteration takes \\(c\\) steps for some constant \\(c\\)\n\\(d\\) steps are taken outside of the loop, for some constant \\(d\\)\n\nTherefore for all inputs of size \\(n\\), the time needed for the entire search is at most \\((cn+d) = O(n)\\).\nAt worst, the item \\(x\\) we’re searching for is the last item in the entire list of items. This can be seen as\n\\(A[n] = x\\) and \\(A[i] \\ne x\\) for all \\(i\\) s.t. \\(1 \\le i \\lt n\\)\n\n\nOrdered array\nIf we assume that the list, or array, that we’re searching over is ordered, say from low to high, the chance of the item we’re looking for being in any one of the \\(n\\) positions is still the same. However, if the item is not present we have a slight advantage in that the item that we’re looking for may never be present past another item of greater value.\nFor example, if we’re looking for the number 25, and through the process of searching through the array, we happen upon the number 27, we know that no other integers past number 27 will have the value that we’re looking for.\ndef ordered_sequential_search(li, item):\n    position = 0\n    found = False\n    stop = False\n    \n    while position &lt; len(li) and not found and not stop:\n        if li[position] == item:\n            found == True\n        else:\n            if li[position] &gt; item:\n                stop = True\n            else:\n                position = (position + 1)\n    \n    return found\n\ntest_li = [0,2,3,4,5,6,7,12,15,18,23,27,45]\nprint(ordered_sequential_search(test_li, 25))\nFalse\nWe can see that we are able to terminate the execution of the search because we’ve found a number greater than the number we’re searching for with the assumption that the list being passed into the function is ordered, we know we can terminate the computation.\nModifying the table above, we can see that with the item not present in our array, we save some computational cycles in the negative case.\n\n\n\nCase\nBest Case\nWorst Case\nAverage Case\n\n\n\n\nitem is present\n1\n\\(n\\)\n\\(\\frac{n}{2}\\)\n\n\nitem isn’t present\n\\(n\\)\n\\(n\\)\n\\(\\frac{n}{2}\\)\n\n\n\nThis can prove really useful if we can somehow, somewhere else in our data structure definitions, that we can guarantee ordering of our arrays. This example is left for future work as it’s more abstract to just the search examples we’re displaying here."
  },
  {
    "objectID": "posts/sequential-and-binary-search-in-python/index.html#binary-search",
    "href": "posts/sequential-and-binary-search-in-python/index.html#binary-search",
    "title": "Sequential and Binary Search in Python",
    "section": "Binary Search",
    "text": "Binary Search\nWith sequential search we start by evaluating the first entry of array for whether or not it matches the the item that we’re looking for, and if it does not we proceed through the entire collection, trying to find a match. There are at most, at any time, \\(n-1\\) more items to look at if the item we’re currently evaluating is not the one we’re looking for.\nBinary search takes a bit of a different approach to the problem. Instead of searching through the collection, sequentially, starting with the first item in the list or array, the process starts at the middle. If the middle item of the list is not the item that we’re looking for, and is larger than the middle value, we can drop the entire bottom half of the list and save ourselves that much computation time.\n# Binary search example\ndef binary_search(li, item):\n    first = 0\n    last = (len(li) - 1)\n    found = False\n    \n    while first &lt;= last and not found:\n        midpoint = ((first + last)//2)\n        if li[midpoint] == item:\n            found = True\n        else:\n            if item &lt; li[midpoint]:\n                last = (midpoint - 1)\n            else:\n                first = (midpoint + 1)\n    return found\n\ntest_li = [0,2,3,4,5,8,10,15,17,21,25,32,42,45]\nprint(binary_search(test_li, 45))\nTrue\nUsing our handy table again, we can analyze the complexity of the binary search algorithm.\n\n\n\nComparisons\nApproximate Number of Items Left\n\n\n\n\n1\n\\(\\frac{n}{2}\\)\n\n\n2\n\\(\\frac{n}{4}\\)\n\n\n3\n\\(\\frac{n}{8}\\)\n\n\n…\n\n\n\n\\(i\\)\n\\(\\frac{n}{2^{i}}\\)\n\n\n\nThe number of comparisons necessary to get to this point is \\(i\\) where \\(\\frac{n}{2^{i}} = 1\\). Solving for \\(i\\) is \\(i = log n\\). Therefore, binary search has a computational complexity of \\(O(log n)\\)."
  },
  {
    "objectID": "posts/machine-learning-in-the-wild/index.html",
    "href": "posts/machine-learning-in-the-wild/index.html",
    "title": "Machine Learning in the Wild",
    "section": "",
    "text": "I won’t be covering anything purely technical in this post, but I wanted to get some thoughts out there on what it takes to bring Machine Learning into production,\n“Any sufficiently advanced technology is indistinguishable from magic.” - Arthur C. Clarke\nThis quote rings true today as much as the day that it was written. Especially when it comes to Machine Learning. I work with customers every day who are curious what this new magic called Machine Learning(ML) is and how they can apply it to a problem they’re facing. There’s no denying that ML is something that we should all be paying attention to but as we start to wade into the waters of ML, and discussing the process of framing the problem, testing our hypotheses, and bringing a system through to production we need to understand that this requires a bit of a shift in our traditional engineering process. In this post we will outline the general process that I’ve followed while helping customers move from idea/MVP to production."
  },
  {
    "objectID": "posts/machine-learning-in-the-wild/index.html#dont-start-with-machine-learning",
    "href": "posts/machine-learning-in-the-wild/index.html#dont-start-with-machine-learning",
    "title": "Machine Learning in the Wild",
    "section": "Don’t start with Machine Learning",
    "text": "Don’t start with Machine Learning\nML in undoubtedly changing the way we approach solving problems in the world today. More importantly, however, is knowing what problem it is that you’re trying to solve. Like many sciences and engineering discplines practitioners are looking to understand a problem domain in an effort to guide decision making processes. It’s this decision making process that is what we think about when approaching a problem. I want to highlight a few key questions one should ask about the problem at hand before rummaging through their toolboxes for an overpowered approach. Some of these key questions are as follows.\n\nWhat decision are we trying to make?\nIs there an existing decision making mechanism in place?\nAre we trying to replace this decision making mechanism?\nWho is this decision impacting?\nWhat will the potential outcomes of this decision be?\nWhat properties of the system am I trying to capture?\nDo we have domain experts involved?\nDo we have the right domain experts invoved?\n\nNone of the questions I’ve provided have anything to do with Machine Learning directly. Rather, they’re asked to help guide the process of applying ML and measuring it’s utility in a given problem domain. These types of questions will help you scope the problem you’re working on and provide a certain acceptance criteria."
  },
  {
    "objectID": "posts/machine-learning-in-the-wild/index.html#data-and-the-system-generating-it",
    "href": "posts/machine-learning-in-the-wild/index.html#data-and-the-system-generating-it",
    "title": "Machine Learning in the Wild",
    "section": "Data and the system generating it",
    "text": "Data and the system generating it\nOnce we have a thorough understanding of the problem that we’re trying to apply ML to, and we understand what decision it is that we’re trying to make / replace, we can start to investigate what data that system generates. Many of the customers I’ve worked with think about this process as just the ML oriented questions around the types of features, what types of random variables they are, what classes we may want to define for a supervised learning problem, etc. We also need to understand questions not directly related to the data generating system, itself. Questions around data collection, storage, access, governance, provenance, etc. Questions like :\n\nHow is the data being collected?\nWhat steps are being taken to validate the data collection mechanisms?\nHow reliable are the measurements being collected?\nWhere is the data being persisted?\nWhat controls are in place to manage access to this data?\nHow is this data being versioned?\n\nThese questions will influence the success of your ML project, overall. Without data, we’re left without the ability to use this powerful new technology."
  },
  {
    "objectID": "posts/machine-learning-in-the-wild/index.html#deliverying-production-grade-software",
    "href": "posts/machine-learning-in-the-wild/index.html#deliverying-production-grade-software",
    "title": "Machine Learning in the Wild",
    "section": "Deliverying Production Grade Software",
    "text": "Deliverying Production Grade Software\nThe next step in the adoption of ML is understanding not just what software is, how to write it, and how to use an SCM system to manage releases, but one also needs to understand what running software in production entails. Anyone who has been part of the industry for any length of time in the last 5 to 7 years has heard the term DevOps before. This term is just a blend of the terms Development and Operations and the concept of blending the two words captures the idea of minimizing the distance between the two general concepts. What it means to be a developer on a production level system requires a thorough understanding of not only the code base, but the properties and characteristics of our production grade systems and what the upper and lower bounds are on the performance of our systems.\nYou may have also heard the term machine learning engineer used within industry, as well. This new title and role is a result of the differences, yet seemingly similar, approaches needed to move from experimentation to production in ML systems. Some of the things to think about involve both the data acquisition, and machine learning (training and serving) pipelines.\n\nWhat does testing look like for our pipelines, end to end?\nHow do we perform validation of performance of our system (not just the model)?\nHow can we provide tooling to measure for concept drift?\nHow can we qualify acceptable throughput of our system, end to end?"
  },
  {
    "objectID": "posts/machine-learning-in-the-wild/index.html#now-for-machine-learning",
    "href": "posts/machine-learning-in-the-wild/index.html#now-for-machine-learning",
    "title": "Machine Learning in the Wild",
    "section": "Now for Machine Learning",
    "text": "Now for Machine Learning\nNow that we have a thorough understanding of the problem domain, use case, data generating system, and all of the software required to stand a system up in production we can start to loop back to the opening thoughts of this post. We need to understand what methods we might be able to use to describe the phenomenon that we’re trying to capture. Typically in Machine Learning by describe, we mean capturing the variance of the overall system in a way that we can use to ultimately make decisions.\nIn order to understand how we can apply ML, in production, we need to understand everything outlined above because it will impact the choices we make from a modeling perspective. Anecdotally we don’t want to make the decision of using a non-parametric model in a given problem domain where the datasets might be growing rapidly. A concrete example of a use case that I’ve run into in this effect has been modeling data networking traffic characteristics. The rate at which data is generated and collected depends on the number of devices sending information over a network. With the types of connectivity we see today this can quickly become a show stopper from moving a model into production, especially if we have to layer feature engineering into the pipeline, on the fly, as well. A non-parametric model will quickly run out of resources in terms of computational and/or time, depending on the problem domain.\nUnderstanding the use case, the decisions we’re trying to make, what mechanisms are in place to collect and persist the data that our system is generating, and the software tooling and ecosystems required to put pipelines into production are imperative to providing a seamless experience of applying Machine Learning in the real world."
  },
  {
    "objectID": "posts/machine-learning-in-the-wild/index.html#end-to-end-throughput",
    "href": "posts/machine-learning-in-the-wild/index.html#end-to-end-throughput",
    "title": "Machine Learning in the Wild",
    "section": "End-to-end Throughput",
    "text": "End-to-end Throughput\nLast but not least after all of the steps outlined above are thoroughly understood we can start to provide the engineering guarantees we’re used to in the technology world. Anyone who has heard the term speeds and feeds understand that we’re talking about the end-to-end throughput of an entire tech stack, from start to finish. Once we are able to effectively measure all of the components of our ML system we can then identify bottle necks and work to alleviate them in a measureable and defensible way.\nOverall, deploying Machine Learning in production may start out as feeling orthogonal to the traditional software engineering methodologies, but once an organization works through a few proof of concept or minimum viable product efforts, they will then understand what it takes to provide a meaningful end-to-end experience for using this new magic called Machine Learning."
  },
  {
    "objectID": "posts/bfs-in-python/index.html",
    "href": "posts/bfs-in-python/index.html",
    "title": "Breadth First Search in Python",
    "section": "",
    "text": "In this notebook / blog post we will explore breadth first search, which is an algorithm for searching a given graph for the lowest cost path to a goal state \\(G\\).\nThe cost is intentionally abstract as it can be defined as whatever you’d like it to be, whether it be the least amount of vertices traversed to get to \\(G\\) or whether it be the lowest sum of the weights of edges between a given state and the goal state, \\(G\\).\nSome quick notational and fundamental review of the definition of a graph is below :\n\nVertex\n\nEnd state, also called a node, of a given path through a graph \\(G\\)\nCan also house additional information known as a payload\n\nEdge\n\nAlso called an arc, the element that connects two vertices within a graph\nCan be either one way or two way; one way = directed graph or digraph\n\nWeight\n\nA value assigned to an edge to denote “cost” of traversing that edge between two vertices\n\n\nWith these definitions we can formally define as a graph, \\(G\\) where \\(G = (V,E)\\).\n\\(V\\) is a set of vertices and \\(E\\) is a set of edges, respectively.\nEach edge is a tuple \\((v,w)\\) where \\(w,v \\in V\\), adding \\(w\\) as a third component to represent the weight of that vertex.\n\nPath\n\nA sequence of edges that connect two vertices.\nFormally defined as \\(\\{w_{1},w_{2},...,w_{n}\\}\\) such that \\((w_{i},w_{i+1}) \\in E \\ \\ \\ \\forall 1 \\le i \\le n-1\\)\n\n\nThere are great libraries that provide Graph ADT’s, but in this example we’ll implement a Graph class ourselves. It will be useful in understanding a graph and how we can use it.\nWe’ll define two classes to support this effort, a Vertex class, which will represent a given vertex being added to the graph, and a Graph class which holds the master list of vertices.\nclass Vertex:\n    def __init__(self, key):\n        # unique ID for vertex\n        self.id = key\n        # dict of connected nodes\n        self.connected_to = {}\n    \n    def add_neighbor(self, neighbor, weight=0):\n        # Add an entry to the connected_to dict with a given\n        # weight \n        self.connected_to[neighbor] = weight\n        \n    def __str__(self):\n        # override __str__ for printing\n        return(str(self.id) + ' connected to: ' + str([x.id for x in self.connected_to]))\n    \n    def get_connections(self):\n        # return keys from connected_to dict\n        return self.connected_to.keys()\n    \n    def get_id(self):\n        # return vertex id's\n        return self.id\n    \n    def get_weight(self):\n        # return weights of edges connected to vertex\n        return self.connected_to[neighbor]\nclass Graph:\n    def __init__(self):\n        # dictionary of vertices\n        self.vertices_list = {}\n        # vertex count\n        self.num_vertices = 0\n        \n    def add_vertex(self, key):\n        # increment counter when adding vertex\n        self.num_vertices = self.num_vertices + 1\n        new_vertex = Vertex(key)\n        self.vertices_list[key] = new_vertex\n        return new_vertex\n    \n    def get_vertex(self, n):\n        # check if vertex exists, return if True\n        if n in self.vertices_list:\n            return self.vertices_list[n]\n        else:\n            return None\n        \n    def __contains__(self, n):\n        # override __contains__ to list all vertices in Graph object\n        return n in self.vertices_list\n    \n    def add_edge(self, s, f, cost=0):\n        # add edge to graph; s = start node; e = end node\n        if s not in self.vertices_list:\n            nv = self.add_vertex(s)\n        if f not in self.vertices_list:\n            nv = self.add_vertex(f)\n        self.vertices_list[s].add_neighbor(self.vertices_list[f], cost)\n        \n    def get_vertices(self):\n        # return keys of vertices in Graph\n        return self.vertices_list.keys()\n    \n    def __iter__(self):\n        # override __iter__ to return iterable of vertices\n        return iter(self.vertices_list.values())\n    \nnode_names = [\"A\", \"B\", \"C\",\n              \"D\", \"E\", \"F\",\n              \"G\"]\n# Instantiate graph object and add vertices\ng = Graph()\nfor i in node_names:\n    g.add_vertex(i)\n# add a bunch of edges between vertices\ng.add_edge('A','B')\ng.add_edge('B','C')\ng.add_edge('C','E')\ng.add_edge('E','D')\ng.add_edge('D','B')\ng.add_edge('E','F')\ng.add_edge('B','E')\nfor v in g:\n    for w in v.get_connections():\n        print(\"(%s, %s)\" % (v.get_id(), w.get_id()))\n(A, B)\n(C, E)\n(B, E)\n(B, C)\n(E, F)\n(E, D)\n(D, B)\n# list our vertices\nfor i in node_names:\n    print(g.get_vertex(i))\nA connected to: ['B']\nB connected to: ['E', 'C']\nC connected to: ['E']\nD connected to: ['B']\nE connected to: ['F', 'D']\nF connected to: []\nG connected to: []\nfrom collections import deque\n\ndef breadth_first_search(starting_node, goal_node):\n    visited_nodes = set()\n    queue = deque([starting_node])\n    \n    while len(queue) &gt; 0:\n        node = queue.pop()\n        if node in visited_nodes:\n            continue\n        \n        visited_nodes.add(node)\n        if node.get_id == goal_node.get_id:\n            return True\n        \n        for n in node.connected_to:\n            if n not in visited_nodes:\n                queue.appendleft(n)\n    return False        \nUsing the breadth_first_search implementation that we’ve written, above, we can then ask the graph is there exists a path between multiple nodes. Our function will return a True or a False accordingly.\nbreadth_first_search(g.get_vertex('A'), g.get_vertex('G'))\nFalse\nPast creating our own Vertex and Graph objects that we can use to assemble our own graphs, we can use libraries like NetworkX to create graphs and implement algorithms, like breadth first search, over them.\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nedges = [('A','B'),('B','C'),('C','E'),\n         ('E','D'),('D','B'),('E','F'),\n         ('B','E')]\n\nnetworkx_graph = nx.Graph()\n\nfor node in node_names:\n    networkx_graph.add_node(node)\n\nnetworkx_graph.add_edges_from(edges)\n\nnx.draw_networkx(networkx_graph)\n\n\n\nFigure 1: networkx_graph visualization\n\n\nBut the library also has the added ability to generate random graphs for us. In this case, the dense_gnm_random_graph() will generate a random graph of \\(G_{n,m}\\) where \\(n\\) is the node count and \\(m\\) are the number of edges randomly distributed throughout the graph.\nnetworkx_graph_1 = nx.dense_gnm_random_graph(10,10)\n\nnx.draw_networkx(networkx_graph_1)\n\n\n\nFigure 2: network_graph_1 visualization\n\n\nThe networkx library tends to return iterators for each object within the graph context, such as the graph iteself, or the nodes within a graph or the neighbors of a particular node within the graph. This is useful because traversal algorithms such as breadth first search tend to operator in an iterative manner.\nnodes returns an iterable for the nodes in a graph\nall_neighbors returns an interable for all neighbors of a passed in graph and specific node\n# quick hack to traverse the iterables returned\nfor node in nx.nodes(networkx_graph_1):\n    neighbors = []\n    for neighbor in nx.all_neighbors(networkx_graph_1, node):\n        neighbors.append(neighbor)\n    print(\"Node %s has neighbors : %s\" % (node, neighbors))   \nNode 0 has neighbors : [3]\nNode 1 has neighbors : []\nNode 2 has neighbors : [4, 6]\nNode 3 has neighbors : [0, 9, 5]\nNode 4 has neighbors : [9, 2]\nNode 5 has neighbors : [3]\nNode 6 has neighbors : [8, 2]\nNode 7 has neighbors : [8, 9]\nNode 8 has neighbors : [9, 6, 7]\nNode 9 has neighbors : [8, 3, 4, 7]\nOr just because, here’s a list comprehension that can do the same thing, that actually shows off a bit of Python’s nested list comprehension functionality. It is possible to also push the print function into the list comprehension below, but it only works in Python 3+ and but is not considered pythonic – so I’m only leaving it to return the nested arrays that a list comprehension normally would.\n[[neighbor for neighbor in nx.all_neighbors(networkx_graph_1, node)] for node in nx.nodes(networkx_graph_1)]\n[[3],\n [],\n [4, 6],\n [0, 9, 5],\n [9, 2],\n [3],\n [8, 2],\n [8, 9],\n [9, 6, 7],\n [8, 3, 4, 7]]\nThe networkx library also includes many, many algorithm implementations already so we can utilize their built-in breadth_first_search algorithm, as we see below. We’re able to print a traversal of the graph starting at node 0 and print the entire path taken through the graph.\nprint(list(nx.bfs_edges(networkx_graph_1, 0)))\n[(0, 3), (3, 9), (3, 5), (9, 8), (9, 4), (9, 7), (8, 6), (4, 2)]\nMuch like we see above, the networkx library also has a built-in depth first search algorithm that will traverse the graph and return an unordered list of tuples of edges that are traversed. I will save a depth first search implementation over our custom Graph object for future posts.\nprint(list(nx.dfs_edges(networkx_graph_1, 0)))\n[(0, 3), (3, 9), (9, 8), (8, 6), (6, 2), (2, 4), (8, 7), (3, 5)]\n\nReferences\nProblem Solving with Algorithms and Data Structures\nNetworkX Documentation"
  },
  {
    "objectID": "posts/algorithmic-toolbox-week-2/index.html",
    "href": "posts/algorithmic-toolbox-week-2/index.html",
    "title": "Algorithmic Toolbox - Week 2",
    "section": "",
    "text": "As a refresher, I’ve started working through the Algorithmic Toolbox course offered on Coursera. It’s been a while since I’ve reviewed a lot of the basic algorithms and data structures fundamentals, so I figured I would work through the course to grease the bearings again, so to speak.\nThat said, this is a notebook that covers some of the concepts and programming assignments in Week 2 of the course. I will try to post most of the stuff I review and examples I work through for anyone who may find it interesting and useful.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport timeit\n\n%matplotlib inline\n\nBigO Notation\nWhen working with algorithms, it’s typical to measure their “time” as, not a function of how long it takes an algorithm to run according to a wall clock, but rather as a function of the size of the input of the algorithm. This is called the rate of growth of of the running time.\nWhen utilizing BigO notation we can distill the “most important” parts and cast out the less important parts. We can see this by looking at the ex_run_time function we’ve defined below. This imaginary algorithm runtime, \\(6n^{2}+100n+300\\), takes as many machine instruction to execute. Again, this is an example.\nIn the example below, we’ve defined two functions that calculate this imaginary runtime according to a user defined input size. We’re going to use this illustration to show that the upper bound on this execution time for this algorithm is defined by the \\(n^2\\) portion of the imaginary runtime of \\(6n_2 + 100n + 300\\).\ndef ex_run_time(coef, n):\n    rt = []\n    for i in range(n):\n         rt.append((i, (coef*(i**2))))\n    return rt\n\ndef decomp_run_time(coef_a, coef_b, n):\n    rt = []\n    for i in range(n):\n        rt.append((i, ((coef_a*i) + coef_b)))\n    return rt\nex_rt = ex_run_time(6, 100)\ndecomp_rt = decomp_run_time(100, 300, 100)\nline1_plt = plt.plot(ex_rt, label='Line 1', linewidth=2)\nline2_plt = plt.plot(decomp_rt, 'b', label='Line 2', linewidth=2)\nplt.legend(['100n + 300','6n^2'], loc=2)\nplt.show()\n\n\n\n\n\npng\n\n\nFigure 1: ?(caption)\n\n\nLooking at the graph above, we can see that the runtime of the \\(6n^2\\) portion of the runtime dominates the total runtime of the algorithm, overall. Using this assumption, when working with BigO notation, we can drop the \\(100n+300\\) portion of the runtime complexity, as we’re working against the squared element of the overall runtime. Looking at the graph, we also see that the runtime complexity for the \\(n^2\\) term of our algorithm intersects the line for the other terms, as well. But the safe assumption here is that this algorithm’s complexity will be overall dominated by the squared term, in any reasonable size input.\nWe can even scale the coefficients of the imaginary complexity to prove that this intersection won’t shift much and we’ll still be bounded by the squared term.\nscaled_ex_rt = ex_run_time(0.6, 2500)\nscaled_decomp_rt = decomp_run_time(1000, 3000, 2500)\n_scaled_line1_plt = plt.plot(scaled_ex_rt, label='Line 1', linewidth=2)\n_scaled_line2_plt = plt.plot(scaled_decomp_rt, 'b', label='Line 2', linewidth=2)\nplt.legend(['1000n + 3000','0.6n^2'], loc=2)\nplt.show()\n\n\n\nFigure 2: ?(caption)\n\n\nChecking out the graph above, we see that the size of the input was able to be scaled pretty considerably. But we can also see that around the input size of 1650, we still end up losing our to the squared term in the runtime of out algorithm. Using this logic, for general purposes, for any reasonable input we can use define the runtime complexity of this algorithm as \\(n^2\\).\n\n\nGenerator methods\ndef generate_seq(start, stop, size):\n    '''\n    Generate a sequence of integers useful in testing the functions below\n    '''\n    \n    return np.random.random_integers(low=start, high=stop, size=size)\n        \n\n\nProgramming Assignments\ndef calc_fib(n):\n    '''\n    Task : Given n, find the last digit of the nth Fibonacci number F_n\n    \n    Input : Single integer n\n    \n    Constraints : 0 \\ge n \\ge 10**7\n    \n    Output : Last digit of F_n\n    \n    '''\n    int_a = 0\n    int_b = 1\n    \n    if n &lt;= 1:\n        return n\n    \n    elif n &gt;= 0 and n &lt;= 45:\n        fib_int = calc_fib(n-1) + calc_fib(n-2)\n        return fib_int\n    \n    else:\n        print(\"%s is out of range. Please try an integer between 0 and 45.\" % n)\n%%time \nfor i in generate_seq(1,2000,1):\n    calc_fib(10)\nCPU times: user 10 ms, sys: 0 ns, total: 10 ms\nWall time: 1.52 ms\ndef get_fibonacci_last_digit(n):\n    '''\n    Task : Given n, find the last digit of the nth Fibonacci number F_n\n    \n    Input : Single integer n\n    \n    Constraints : 0 \\ge n \\ge 10**7\n    \n    Output : Last digit of F_n\n    \n    '''\n    fib_array = np.zeros(shape=n, dtype=int)\n    fib_array[0] = int(0)\n    fib_array[1] = int(1)\n    \n    if n &gt;= 0 and n &lt;= 10**7:\n        counter = 2\n        for i in fib_array[2:n]:\n            fib_array[counter] = ((fib_array[counter-1] % 10) + (fib_array[counter-2] % 10))\n            counter += 1\n        return (fib_array[counter-1] % 10) + (fib_array[counter-2] % 10)\n    else:\n        print(\"%s is out of range. Please try an integer between 0 and 10,000,000.\" % n)\n%time\nprint(get_fibonacci_last_digit(3))\nprint(get_fibonacci_last_digit(331))\nprint(get_fibonacci_last_digit(327305))\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 18.8 µs\n2\n9\n5\n\n\nGreatest common divisor\ndef euclidean_gcd(a,b):\n    if b == 0:\n        return a\n    a_prime = a % b\n    return(euclidean_gcd(b,a_prime))\n%%time\nprint(euclidean_gcd(18,35))\nprint(euclidean_gcd(28851538, 1183019))\n1\n17657\nCPU times: user 10 ms, sys: 0 ns, total: 10 ms\nWall time: 981 µs\n\n\nLeast common multiple\ndef lcm(a, b):\n    if a &gt;= 1 and a &lt;= (2*(10**9)) and b &gt;= 1 and b &lt;= (2*(10**9)):\n        return (a*b) // euclidean_gcd(a,b)\n    else:\n        print(\"something is wrong\")\n%time\nprint(lcm(6,8))\nprint(lcm(28851538, 1183019))\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 15 µs\n24\n1933053046"
  },
  {
    "objectID": "posts/machine-learning-and-ai/index.html",
    "href": "posts/machine-learning-and-ai/index.html",
    "title": "Machine Learning and Artificial Intelligence",
    "section": "",
    "text": "There is a seemingly common thread right now in the tech industry around the confluence of terminology that can be used when addressing the broader practice of studying and applying machine learning to interesting problems. For some value of interesting.\nFirst I want to start with a history of the term artificial intelligence. The term was coined by John McCarthy for the famous conference at Dartmouth that was held in 1956, hosted by that of Marvin Minksy, and Claude Shannon, as well. The conference’s aim was to study the idea that every aspect of learning or other feature of intelligence can, in principle, be so precisely described that a machine can be made to simulate it. Said differently the goal of the conference was to investigate the plausibility of a machine being able to emulate, at least at first perceptual, problems that were previously reserved for the domain of humans.\n\n\nI want to start this post with some definitions from some of the major literature in the field to normalize what is meant when using different words and phrases to describe the application of science to quantify and/or qualify some dataset.\nThe first term I will address is Machine Learning and I will decompose the phrase into its constituent components. Both machine, and learning.\n\n\nDepending on your choice of common English dictionaries the definition of the word machine can vary. We will utilize the Oxford Dictionary which defines the term machine as follows:\n\nAn apparatus using mechanical power and having several parts, each with a definite function and together performing a particular task.\n\nThe definition fits that of a modern computer. Most modern consumer computers follow the Von Neumann architecture in that there are disparate components working together to perform a particular task. Mainly a Central Processing Unit, and a Memory Unit that work in orchestra to take as input some value, and produce another value as output. This is the definition that I will apply to the word machine.\n\n\n\nFigure 1: Von Neumann Architecture Source\n\n\n\n\n\nWe will also appeal to the Oxford Dictionary for the definition of the term learning.\n\nThe acquisition of knowledge or skills through study, experience, or being taught.\n\nThis definition is in line with something that is intuitive but can sometimes become lost in the noise of the every day higher level interactions we’re having with the world around us. We are, in real time, learning on many levels of abstraction that are presented to us through out cognitive faculties. Consciously learning the fundamental theory of algebra is a seemingly active exercise in which we study material to learn the theorem and how to apply it. However separate from that conscious learning, we are also subconsciously learning from many inputs we are receiving. Suppose we decide to stop studying our material on algebra and instead decide that we want to go for a jog. As we begin our journey things are going smoothly and when we aren’t paying attention we stumble on an obstruction in the middle of the pathway. As we stumble our brain is subconsciously using many inputs from our sensors contained within our system called the human body to right our trajectory to ensure we don’t fall. Using a seemingly more concrete example would be a gymnast performing a routine that they have repeatedly practiced until they are satisfied. A similar analogy could be made for the increase and decrease in requirement of oxygen within the body as we are burning more energy throughout the process of exercise. As our heart rate increases our respiration rate increases to compensate for the increase in oxygen required for metabolism.\n\n\n\n\nI want to formalize the definition of machine learning in the way that the research and industry tend to apply them.\nI think about it in terms of Tom Mitchell’s definition, in his book Machine Learning, which offers a formal definition in the bounds by which science and engineering can work.\n\nA computer program is said to learn from experience E with respect to some class of task T and performance measure P, if its performance measure P at task T as measured by P, improves with experience.\n\nThis definition also captures the fundamental areas of research and industry application that exist within machine learning.\nAbove, when referencing emulation of tasks that are normally performed by other systems that exist in the world, I mean that these systems can be a product of biological or human engineering efforts. There are many tasks that biological systems, such as humans and animals, perform which can be emulated using this process called machine learning. Actions that we perform such as recognizing objects around us, to understanding the approximate trajectory of an object, such as a ball, and being able to intercept that ball to catch it. Along with other processes such as understanding what someone is saying through the combination of speech and gesture recognition while communicating.\nThere are also other, external to the human psyche, tasks that we can use machine learning for as well. We may lack the biological sensors to measure many of the systems around us but we can build sensors for these systems and then use machine learning to have computers “learn” about these systems. Systems like the weather, astronomical objects, search engines, etc.\n\n\n\nNow I want to address the term artificial intelligence in the same way that I had addressed machine learning. By decomposing it into its constituent words, defining them, and attempting to define what the combination of the two terms, means.\n\n\nThe term artificial is defined by the Oxford Dictionary as follows:\n\nMade or produced by human beings rather than occurring naturally, especially as a copy of something natural.\n\nThis term is a bit more broad in its definition in that many of the objects we see around us are an artificial representation of something naturally occurring. Things like artificial flowers, or artificial limbs that enable individuals perform tasks that might otherwise not be possible. The key interpretation here is the fact that it is the creation of something that seemingly isn’t naturally occurring. Notwithstanding the logical argument that could be made that there is some hierarchical interpretation to the idea of something being created by something that was created naturally, humans, ergo whatever was created could be interpreted as something naturally occurring.\n\n\n\nNow we come to a seemingly ill-defined term that exist in the world of science and engineering. That of intelligence. When we look at the Oxford definition of the word we can see just how general the interpretation is:\n\nThe ability to acquire and apply knowledge and skills.\n\nThe ability to apply knowledge and skills is interestingly broad in its definition. As with the logical argument that could be made above about what really is artificial and what isn’t, a similar logical argument could be made in that if we create a machine that embodies some sort of skills or abilities that we as humans have acquired, are we creating an intelligent system? Or is the system itself required to acquire the referenced knowledge and skills. We might have to go one rabbit hole deeper in order to make this definition a bit more concrete.\nKnowledge is defined as:\n\nFacts, information, and skills acquired through experience or education; the theoretical or practical understanding of a subject.\n\nYet again we’re up against the wall with some of the lingual abstractions present in this definition. Facts, information, skills, experience, education, etc. all contain some sort of implicit definitions in which we tend to take for granted. Rather than looking into the definitions and etymologies of words used to describe a phenomenon that we are observing, we can appeal to a more rigorous definition of what intelligence may be. Legg and Hutter (2007) provide a working definition of machine intelligence.\nLegg and Hutter start with an analysis of 70 or so definitions of intelligence from different areas of academia including researchers in artificial intelligence and psychologists. There are a few salient definitions from both backgrounds that I would like to reference.\n\n“Intelligence is a general factor that runs through all types of performance.” A. Jensen\n\nI quite like this definition because it affords us a general interpretation in that many of the systems that we build, deploy, and label as intelligent can logically satisfy this definition. All systems have performance measures used to justify whether or not that system is able to perform better than previously, due to some mechanism contributing to the idea. There are also others that are seemingly more philosophically intriguing as well:\n\n“The capacity for knowledge, and knowledge possessed.” V. A. C. Henmon\n\nThis eludes to the idea that there is some form of consciousness that needs to exist, a self awareness of ones own capacity for knowledge. This is less concrete in the way of mathematical definition, but I do enjoy at least entertaining the idea if for nothing more than thought exercise.\nLegg and Hutter (2007) distill these definitions down to something more general as their definition seems to capture many of the special case interpretations of the 70 odd quotes:\n\n“Intelligence measures an agent’s ability to achieve goals in a wide range of environments.” S. Legg and M. Hutter\n\nAs they note in their paper this definition has implicit in it many of the abilities an agent should have to define it as intelligent, abilities such as learning, adapting, and understanding.\n\n\n\n\nNow that we’ve defined artificial and intelligence, we can define what the two words mean together. It is an agent that doesn’t occur naturally that can some how achieve goals in a wide range of environments.\nThere is a more formal definition as is defined in Legg and Hutter (2007). That is left to the reader for investigation. For now we will leverage just the general linguistic definition of the term intelligence.\nThere are what seem to be direct lines that can be drawn between artificial intelligence and reinforcement learning in that both definitions and the latter’s frameworks encompass the process of training an agent on a given environment to improve its performance over time to achieve whatever goals have been defined, and depending on the area of research there is also the research into the transferability of these agents between many different environments. Whether it be an already trained agent being exposed to a new environment or whether a particular methodology is applicable to more than one environment.\n\n\n\nThis leads me back to the reason for this writing. It is an attempt to normalize the nomenclature that we as an industry use when addressing the application of these technologies to problem spaces. There are many ways that these terms can become muddied and conflated and I want to ensure we’re all speaking the same language as we make the efforts to apply these technologies in new and interesting ways.\n\n\nI also want to provide a concrete example of where these definitions can be used in our specific problem space of technology systems. Depending on the scale at which we are analyzing a given system, one could be analyzing a single computational device that is part of a larger cluster of computational devices that are meant to distribute computational operations or storing state in a persistent manner. In respective parlance, distributed systems and databases.\nWhen reasoning about the application of machine learning to systems such as these, there are many aspects of the system that we can attempt to model using methodologies that fall firmly in the machine learning definition. A relatively simple example would be the application of some form of novelty detection with respect to the operation of the system. When collecting sensor data at times when the system is considered in steady-state operation, or nothing is currently wrong with the system, we can use novelty detection techniques to model either the data that has been collected itself, or the data generating distribution that we assume our data set has been produced from. Commonly referred to methodologies used to perform this are the application of autoencoders which can learn to reconstruct an input given some compressed representation of that input, or something like a variational autoencoder which attempts to model the parameters of the data generating distribution that produced our dataset that we’re analyzing.\nWhere we can cross the line into the area of artificial intelligence is when we start to use models to affect change on the system that we are reasoning about. When we think about this from a particular perspective of infrastructure operations, it would be the assumption that a system that is artificially intelligent would be able to modify the configuration of a given distributed system to improve the operation of that system. This definition is more in line with that of reinforcement learning, that I haven’t covered in this post. This will be covered in later posts.\nThis may become part of a multi-post series in an effort to combat the “buzzwordyness”, for lack of a better term, of the industry side of the applications of these methodologies, and I will update posts accordingly.\nMore to come…"
  },
  {
    "objectID": "posts/neurips-2019/index.html",
    "href": "posts/neurips-2019/index.html",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "DISCLAIMER : THESE ARE RUNNING NOTES FROM MY CONFERENCE ATTENDANCE, I MAY OR MAY NOT COME BACK TO UPDATE THESE NOTES PLEASE USE AT YOUR OWN RISK\n\n\n\n\nThe presenter covers the analysis of different Q-learning approaches in the context of their presentation.\nThey also cover the importance of evaluating a learned policy and how that can be done.\n\nWe need to understand how well out optimcal policy is able to learn an optimal set of decisions\nWe need to propose a set of actions that is realistic for the organization to implement\nWe need to interpret out how our optimcal policy differs from the data generating behavior policy.\n\nThese questions that we need to ask, outlined above, help us to evaluate the effectiveness of our approaches in the real world.\n\n\nDefn : Evaluating a learned policy through the simulation of the environment\nConsiderations :\n\n\n\nAre we even able to simluate the dynamics of the environment?\n\n\n\n\n\n\nMore easily applied to historical data (we don’t have access to a simluation environment)\nImportant sampling can be used to estimate the expected reward under a new policy\nRequires estimates of the behavior policy which is usually estimated by separate supervised models\n\nThis might be the only way forward as we don’t have the capability to simulate the dynamics of the envoironment.\n\n\n\nGoal of PS : estimate the value of a new policy\nTake the ratio of the optimal bahavoior policy at a given time point and * reward at the time step / number of data points – this generates an estimator for the expected value of importance sampling\nSlide\n\n\n\nHas high unbounded variance\nweighted importance sampling has lower variance, but is a biased estimator (because of the weighting)\nVariance is much higher when our policy drastically differs from true behavior\nVariance increases with the length of the episode\nRequires accurate estimate of the behavior policy\nNewer evaluation methods (MAGIC) use both important sampling and model-based approaches depending on the sparsity if your episode\n\nIn summary, as time progresses the variance of the policy will increase, this seems intuitively correct\nPaper : Evaluating Reinforcement Learning Algorithms in an Oberservational Health Setting\n\n\n\n\n\nCertain domains may Require offine evaluation due to ethical or data issues\nOPE may seem relevant in scenarios where a simulator isn’t available, but high variance makes estimating the value of the optimal policy extremely difficult\nInvestimg time to build a model of the environment is important\n\n\n\n\n\n\nPrivacy Principles at Apple for ML\n\nData Minimization\n\nCollect only what we need\n\nOn Device Intelligence\n\nProcess data local to devices - this prevents the uneeded transport that can allow for eyes that shouldn’t see it\n\nTransparency and Control\n\nAllow for ‘opt-in’\n\nSecurity\n\nThe foundation for all of this\n\nThreat Model\n\nMalicious Adversarys\n\nPerform arbitrary inferences on the learned model to extract data points\n\nCurious onlooker\n\nPassively looking at updates\n\n\nCentral Differential Privacy with small privacy parameters (Epsilon &lt;= 2)\n\nmoment accounting\n\nLocal Differential Privacy\n\nlocal pertubation on the device modifies the vector into a new vector \\[ z = M(w) \\]\n\nControl - allows users to opt into this feature within device settings\n\nExpose the information that is being sent to apple\n\nActual parameters, and many other data points\n\n\n\nRetention * Keep as little data as possible for as short a time as possible * If user deletes a data source, immediately remove the corresponding training data, as well.\n\n\n\nThe question being asked “Can data be encypted and still be used to train models”\n\n\n\nHomomorphic encryption\n\nData encrypted localled on some device and transmitted back to some central repo\nPerform some function wrt to data and the function itself can or cannot be encrypted as well\n\nSecure Multiparty Computation\n\nFederal Ownership of data\n\nMultiple parties involved in the encryption scheme\nWe can then evalute functions\n\n\ntrusted execution environments\n\nMuch like Enclaves from Intel – thought these were proven to be insecure with the meltdown and spectre attacks\nattestation - we can attest to the fact that only the desired function was executred\n\nDifferential Privacy\n\nExecute some function on some data that has had noise added to it\n\nWe have to formally prove that the noise we’ve added still provides some guarantees through the distributional definition?\n\nNeed to understand this more.\n\n\n\nFederated Learning\n\nAdditive noise that has interesting properties described in Secure Aggregation (see Secure Aggregation Protocol Paper)\n\nHow and why does this matter to PyTorch\n\nEnd state of this would be to have a flag in an API where (privacy=True)\nFar from that.\nBuilt a framework called CrypTen URL\n\nCrypTen Design Principles\n\nEager Execution\n\nEasier troubleshooting and learning curve\n\nRealistic\n\nsome other OSS projects assumed 2 parties, they wanted to head toward N parties\n\n\nEncryption by sharing\n\nMultiparty computation\n\nMultiple parties process “part” of the data\n\nimages divded between parties would be done pixel by pixel and this might be uninteresting to any single participant\nNo parties can reconstruct the actual data without collaboration from all parties\n\n\nAssume we have an integer representing some information : \\[x \\in \\{0,..,N-1\\}\\]\nChoose some random mask in the same interval : r ~\nEncrypt by subtracting the mask that we’ve samples above\n(x-r) becomes independent btween participant clients\nDecryption in this domain is easy, just add all of the shares\n\njust need agreement from all of the participant parties\nWe can design this “agreement” in many different ways\n\n\nEncryption is homomorphic\n\nadding a public value : \\[ [x] + y = (x^(0) + y) \\]\nMultiplication needs triples of encrypted random numbers with the property that \\[ [a][b]=[c] \\]\n\nonce we have these tiples we can then generated a share for \\[a\\], \\[b\\], \\[c\\]\n\nthese tiples are sent the participant parties who then calculate epsilon and deltas\ncontain no information because they’re all encrypted\n“open epsilon and delta” - leak no information because they’re substracttions from a random number\n\n\n\nOperations for Neural Nets\n\nConbvolutions are additions and multiplies\ndivisions are approximated\nnon-linearities that involve exponentation are\nrelu break down into sharing ‘bits’\n\nHigh level architecture slide\nFeature List slide\nHas ONNX integrations\n\n\n\n\n\nURL Recording\n\nGradient Descent with Bayes\n\nClaim is that we can derive this by choosing Gaussian with fixed covariance\nGlobal to local approximation\n\nNewton’s Method from Bayes\n\nIn this case we choose a multivariate Gaussian SlidePaper\nWe can express in terms of gradient of Hessian of the loss\n\nWe can use this expectation parameter and ask questions about higher order information about the loss surface\nIf we’re not sure about a second order method, then principles say that we shouldn’t maybe use these methods\n\n2 parts\n\nchoose the approximation\nsecond is choosing which order of parameters we might want to use\n\n\nRMS/Adam from Bayes Slide Paper\n\nThis has taken the community a long time to figure out but we can see that we can draw lines between the bayeslian learning rule for multivariate Gaussian and RMSprop\n\nSummary\n\nIf we add momentum to the bayesian objective a lot of these things can be explained using similar prinsiples\n\nBayes as optimization Slide Papers\n\nWhat we can do to derive the bayes rule in this special case\n\nWe define the loss to be the negative log joint (estimate)\nWe can plug this into an objective function over all distributions\nWith no restriction we should arrive at the posterior distribution\nEntropy is the negative expected value\nThe expectation of the log ratio of q over e^-l becomes 0 (as log(1) = 0)\n\n\nBayes with Approximate Posterior\n\nTrying to make a point that using this learning rule, how do we optimize it?\n\nOptimizing it in the right way allows us to do much more than variational inference, including exact bayesian inference\n\nBayesian Principle, rather than variational principle\n\nConjugate Bayesian Inference from Bayesian Principles Slide Paper\n\nComputing these messages in forward backward algorithm, we’re finding the expectation of the gradient\nWe can write this loss as two parts\n\nloss of the joint\ndepends on the data (conjugate really means depends on the data)\n\nChoose a q to match the sufficient statistics\nWe can write this as a combination of a learning term and a quadratic term\nCompute the expectation fo the loss, we can see that it’s linear in the expectation parameter\nThe expectation of the loss is linear in the expectation parameter\n\nNeed this to compute the squares (see slide)\n\nThis is a generalization that applies to things like forward backward, SVI, Variational message passing, etc.\n\nThis is all proved in the paper link above\n\n\nLaplace Appriximation Slide\n\nRun the newton method and eventually it will converge to the laplacian\n\n\n\n\n\n\n\nUncertainty Estimation for Image Segmentation Slide\n\nWe can see missing pieces of sidewalk, etc. this shows wher\n\nSome Bayesian Deep Learning Methods Slide\n\nOne line of work proved popular and just keep running standard DL, and then use some ideas to dropout some weights, and doing this it somehow corresponds to solving this bayesian problem (see paper)\n\nPros : Scales well to large problems\nCons : Not flexible\n\nPoint is to get the average with the goal of “how do we choose this average”\n\nGet this model, and perturb and this allows to add some noise and allow us to explore a bit\n\nThe principle of SGD corresponds to a Gaussian with parameters that we cannot really control\nWe can use any parameterization we want\n\nScaling up VI to ImageNet Slide\n\nTaking a sample of the gradients and this helps us to scale it to ImageNet\n\nVariation Online Gauss-Newton Slide\n\nImprove RMSprop with Bayesian Touch\n\nRemove the “local” approximation of the Expectation\nAdd some second order approximation\nNo square root of the scale\n\nTakes some more computation but it’s worth it in that we’re estimation varaince of the diagonal gaussian then we might want to make that tradeoff\nEstimating a diagonal gaussian with some variance around it, and the variance scaled\nWe can borrow a lot of tricks from the DL side of the world through the framing of the problem we covered previously\n\nBDL Methods do not really know that they are performing badly under dataset shift\n\nThis is telling us about uncertainty and about performance\n\nWe are shifting the data slowly and we can see that accuracy goes down\nIf we’re estimating uncertainty it should be reflected in our calibration\n\n\nResources for Uncertainty in DL Slide\nChallenges in Uncertainty Estimation\n\nWe can’t just take bayesian principles and apply them to non-convex problems\ndifferent local minima correcpond to various solutions\n\nlocal approximations only capture “local uncertainty” – in the same way that DL only captures a local solution to the functional defn\nThese methods miss a whole lot of the data space\nThis is a very hard problem\n\nMore flexible approximations really tells us that we need to go beyond second order optimization\n\nFundamentally there are tools that are missing for us to do this\n\n\n\n\n\n\n\nWhich examples are more important for a classifier given Slide\n\nDoes our model really “know’ this?\nDoes the model understand why it is the way that it is?\n\nModel View vs Data View Slide\n\nBayes automatically defines data-importance\nPoints closer to the boundary are more “important”\nThe data view tells us what makes the model certain\n\nDNN to Gaussian Processes Slide\n\nTrained with just a deterministic method (Adam, etc)\nCan we warp that line to get a distribution?\n\nGet a gaussian approximation of this red line and it turns out that the GP are posteriors of linear models\n\nposterior is equal to the posterior approximation\n\nFind a basis function where this linear approximation and we can convert it to a Gaussian Process\n\nThese things seem to a dual of eachother\n\n“Global” to “Local” Slide\n\nThis posterior approximations connect “global” parameters (model weights) to “local” parameters (data examples)\nWhen we use gaussian approximation, we approximate this loss function\nLocal parameters can be seen as “dual” variables that define the “importance” of the data.\n\nContinual Learning Slide\n\nWe’re not seeing part of the data and when we do this with NN’s we show that if we do this in the naive way then we start forgetting the past\nThere is no mechanism to remember the past, this global thing that I want to remember what I did classify and what mistakes I’d made in the past\n\nContinrual Learning with Bayes Slide\n\nRemembers almost everything that happened\nComputing this posterior is challenging, so we can use posterior approximations\n\nSome regularization-based continual learning methods Slide\nFunctional Regularization of Memorable Past (FROMP)\n\nIdentify, memorize, and regularize the past using Laplace approximation (similar to EWC)\n\n\nChallenges in Continual Learning Slide\nTowards Life Long Learning Slide\n\n\n\n\n\n\n\nDivergence Measures\n\nAre P and Q the same?\n\nWe can measure the difference or the ratio of probabilities P - Q or P/Q\nDivergence measure measuing difference will\n\nIntegral Probability Mertrics\n\nIPM are looking for a function that is well behaved , meaning smooth\n\ndiffernce in distributional expectations\n\n\nThe MMD: integral probability trick Slide\n\nMaximimze the mean discrepancy of the distributions\n\nsmooth function for P vs Q\n\nAssuming we can axpress our algorothm using dot products, we can take advantage of the analytical form for solving\nInfinitely many features that allow us to tell what is different when we use MMD\n\nFeature dictionary allows me to distinguish P and Q, no matter the difference between them\n\nExpectations of functions are linear combinations of eprected features\n\nTurns out the expectation of F is a fot product of F and X\n\n\n\n\n\n\nHow does the Wassterstein 1 behave?\n\nUsing a wasserstein-1 function that is lipschitz defined\n\nPhi divergences Slides\n\nTaking the ratio of the expectations of the densities\nTaking the reverse KL\n\n\n\n\nSlides\n\nCIFAR 10\n\nWeird conclusions in the reults of this paper\nIn the testing of CIFAR 10 - given these distributions how can we measure if they’re the same?\nRemember that MMD(P,Q) =\n\nEstimating the MMD Slide 1 Slide 2\n\nDiffernce between the mean embeddings (difference between these latent representations)\nExpected features of the same size\nDifferences in the mean of the distributions and across the distributions\nTake an empirical average and we can measure this\nWith this discrepancy, is this MMD true? Small numner “0.09” is small, but not 0.\n\nBehavior of the MMD Slide\n\nP,Q laplace with difference variances in y\nsamples frawn iid from P and Q\nIf we keep drawing on P and Q, we can see that this looks a lot like a normal distribution\nAsymptotics of the MMD are a normal distribution\nCentral limit theorem results hold\nAsymptotically normal with a mean at tthe TRUE MMD, and variance sigma^2 of the MMD\n\nvariance decays asymptotically\n\nWhat about when P and Q are the same? Slides\n\nturns out it’s an infinite sum of chi^2 distributions\n\nthis distribution depends on choice of kernel and what thr distribution of the data is\nWe do know that it converges to distribution of something\n\n\nA summary of asymptotics\n\n\nDistributions are close, they’re normal\n\n\nThe same and its this weird mixture of chi^2\n\n\nClassical statistical tests\n\nDistance is big, then we can say they’re not the same\nIf the estimate is less than a threshold then maybe they’re the same or we didn’t have enough data to capture the variance\nWe can take the MMD estimator and ask whether our estimator is bigger than a threshold CL Slide\n\nunder the known process, when P=Q, we want to reject the null at the rate most 0.05\nProbability of doing that, to be less than L\n\nWe can shuffle all examples together, which is a random mixture of dogs and fish Slide\n\nwe can estimate the distance between these new tilde’s and we can estimate what this actually means when P=Q\nWhat is the 1 - quantile, and that should be a good estimator\n\nGiven a kernel, we can now run a test\n\nChoosing a kernel, we can start with exponentiated quadratic\n\nKernel is characteristic no matter what bandwidth we pick\nAs we see infinitely many examples, all of the maxx escapes to the right\n\nProblem is, we never have infinite samples\nIn our example, bandwidth choice mastters A LOT\n\nIf we choose too smooth of a kernel then we get a witness function that can barely distinguish between the two distributions\n\nPower will be really low because of rejection bandwidth will be really high\n\nIn high dimensions, it doesn’t matter what bandwidth we pick because the bandwidth is based on pixel distance between images which breaks down in the curse of dimensionality\n\nOften helpful to use a relevant representation, by creating a new representation\n\nTake some hidden layer near the end of a classifier (reneralizes a little bit better)\n\nMeasure MMD between 2000 hidden dimensional representation from a classifer\nTurns out KID and FID use MMD and give way better properties\n\nInteresting that they use the semgentation mask as the pixel count (linear kernel of counts of pixels)\n\nThis seems super informative\n\n\n\nWhat about tests for other distances\n\nSometimes nice closed forms are useful\n\nChoosing the best test\n\nPicking a kernel that’s good for a particular model\nPower depends on the distributions P and Q (and n)\nCan maybe pick a good kernel manually for a given problem\n\n\nOptmizing MMD for test power Slide\n\nAs we see more and more data this will converge to Gaussian\n\nfor large n, the second term is negligible\n\nOur estimator is differentiable in kernel parameters\n\nData Splitting\n\nImportant we don’t trick ourselves and keep everything statistically bound\nWe need to be looking at the test error here\nWe split part of the data to learn the kernel, and the other part to test that kernel\n\nThis second part is exactly the standard testing framework covered above\nThis is a methodology notion\n\nLearning a kernel is very helpful Slide\n\nAlternative Approach\n\nWe can train a classifier to do something like this above\nWe split the data, train a classifier to distinguish X from Y and evaluate it on the other half of the data\n\nAccuracy 50% = can’t tell, accuracy = 100%, clearly different\n\n60%, the fact that we can classify at all tells us the distributions are different\n\n\n\nClassifier as two-sample test Slide\n\nalmost exactly equivalent\n0-1 kernel inflates invariance, decreases the test powr\n\nInterpretability Slides\n\nCan we distinguish two distributions\nBreak up each image into its component pixels and learn a kernel for each pixel\n\nUsing an ARD kernel\n\nWe can look at where the witness function “cares about the most”\n\nhistogram of witness function might overlap, as the means are close to eachother\nthe points that have the witness function interprets that it looks the most like a dataset\n\n\nMain references and further learning Slide\n\n\n\n\n\n\nSlides\n\nThis tutorial will outline how representation learning can be used to address fairness problems\nA Framework for Fair Representation Learning\n\nRepresentation as a fairness problem Slide\n\nCreating a data regulator Slide\n\nDetermines what the fairness criteria are\ndetermines data sources\naudits results\nWhen Training\n\nInteracting with all of the stakeholders to understand the fairness criteria\n\noutput is the fairness criteria\n\nDeterminining fairness critera\n\nAlgorithmic fairness\nDataset fairness\n\n\nExamples for how to do this\n\nPartition the dataset into space of disjoint cells such that similar individuals are in the same cell.\nIndividuals in the same cell should be treated similarly\n\nLipschitz continuity implies individual fairness\n\nGood news : One can achieve fairness through Lipshitz regularization.\nBad news : Data is non-Euclidean (eg. images, graphs, etc).\n\nStandard Euclidean distance metrics aren’t a good measure for this\n\nChallenge : Can we learn representations of the data such that the l_2 norm is a good metric to compare instances?\n\nGroup Fairness : Similar Classifier Statistics across groups Slides\n(im-)possibility result for group-fair classification\n\nClassifier statistics are not artbitrarily flexible\neg. Binary classification statistics have two degrees of freesom thus can match two independent statistics across groups\nBeyond binary classification, the degrees of freedom grows quadratically with number of classes\n\nGroup Fairness : Advantages and Challenges\n\nAdvantages\n\nFairly easy to compute and roughly scales with the number of samples\nOften easier to explain to policy-makeers (as in terms of population behavior)\nMore existing work, strategies already exist for representation learning Challenges:\ndata regulator must determine which classifier statistics to equalize.\nfairness of the representation depends on the quality of the fairness metric chosen by the regulator\nGroup fairness can lead to (more) violated individual fairness, e.g intersectionality can lead to fairness gerrymandering\n\n\n\n\n\nMetric Elicitation Slide\n\nDetermine the ideal evaluation metric by interacting with users, and experts\nQuery an oracle for this fairness metrics\n\n\nWhich statistics should be equalized across groups?\nCommonly used measures are straightforward functions of classifier performance statistics.\n\nData Producer\n\nComputes the fair representation given the data regulator criteria\n\nData User\n\nComputes ML model given sanitized data\n\n\n\n\n\n\n\nSlides\n\nHow can two people living in the same world come to two different conclusions?\n5 Things Everyone in ML Should know about\n\nHumans continuously form beliefs\n\nWe don’t set them and we’re done\nWe continuously update our beliefs\nEvery time we encounter an instance of a bowl, we update our beliefs about bowls\n\nCertainty diminishes interest\n\nWhat you think you know is what determines your curiosity\nPeople do not have an accurate model of their own uncertainty\nIf you think you know the answer, you won’t check, and if we present the right answer to the person they still reject it\n\nMight be why there is confirmation bias\n\n\nCertainty is feedback driven\n\nhigh level beliefs about concepts\n\nmost useful for the decision making points in our lives\n\nwe are sometimes certain when we shouldn’t be\n\n\nPeople learned a novel rule based concept\n\nboolean logic to determine daxxy-ness\n\nIn the beginning there is no concept, it could be a property of the system, or not\nEntropy of an idealized model has little to do with interest and learning\n\ninstead this certainty comes from feedback\n\nReasoning about the world\n\nFlat earthers – if they watch online videos that confirm this it might increase the chance of early adoption of this idea as truth\n\n\nLess feedback may encourage overconfidence\nHumans form beliefs quickly\n\nEarly evidence counts more than later evidence\n\nLeads to becoming certain and plays down our ability to update our beliefs\n\n\n\nThere is no such thing as a neutral tech platform\n\nThe order in which information is presented makes a huge difference in our understanding of the world\nThis reinforces some of the studies done around the 2016 elections\nChildren are consuming more online data and this is affecting them\n\n\n\n\n\nSlides\n\nVeridical - coinciding with reality\nPCS Framework for Data Science Paper\n\nPredictability (P) (From ML)\nComputability (C) (From ML)\nStability (S) (from statistics)\nBridges two of Breimann’s cultures\n\nPCS connects science and engineering\n\nPredictability and stability embed two scientific principles\n\nStability unifies and extends myriad works on perturbation analysis\n\nIt’s a minimum requirement for reproducibility, interpretability, etc.\nTests DSLC by shaking every part (I describe this as wiggling all parts of the system to see how it changes the output)\nThere is always something to follow up when building models\n\nNew users\nNew patients\nNew collaborators\n\n\nData Perturbations\n\nData modality choices\nSynthetic data\nData under different environments (invariance)\nDifferential privacy (DP)\nAdversarial attacks to deep learning algorithms\nData cleaning also falls into this data perturbation bucket\n\nModel/algorithm perturbations\n\nRobust statistics\nSemi-parametric\nLasso and Ridge\nModes of non-convex empirical minimization\n\nHuman decision is prevalent in DSCL\n\nWhich problem to work on\nWhich data sets to use\nHow to clean\nWhats plots\nWhat data perturbations, etc.\nWRITE THIS ALL DOWN (MODEL CARDS FOR MODEL REPORTING)\n\nReality correspondences &lt;- great description of what we do when we “model” something\nHow do we choose these perturbations?\n\nOne can never consider all perturbations\nA pledge to the stability principle in PCS would lead to null results if too many perturbations were considered\nPCS requires documentations on the appropriateness of all perturbations\n\nExpanding statistical inference under PCS\n\nModern goal of statistics is to provide one source of truth\n\nCritical examination of probabilistic statements in statistical inference\n\nViewing data as a realization of a random process is an ASSUMPTION unless randomization is explicit\n\nTHIS DATA COULD HAVE BEEN GENERATED NON-RANDOMLY\n\nWhen not, using an r.v. actually implicitly assumes “stability”\nUse “approximate” and “postulated” models\n\nInference beyond probabilistic models\n\nWe need to have a way to bring PDE models in with things like synthetic data\nProposed PCS framework\n\nProblem formulation - translate the domain question to be answered by a model/algorithm (or multiple of them and seek stability)\nPrediction Screening for reality check : filter models/algorithms based on prediction accuracy on held out test data\nTarget value perturbation distribution - Evaluate the target of interest across “appropriate” data and model pertubations\n\n\n\nMaking Random Forests more interpretable using stability\n\n\n\n\n\nSlides Poster\n\nTightest uniform convergence bound that eventually shows it is vacuous\nGiven a training set \\(S\\), algorithm \\(h\\) in \\(\\mathbb{H}\\), then [Sl]\nIn what setting do we show this bounds failure/\n\nSeparating an nested hyperspheres, with no hidden noise, and completely separable\nObserve that as we increase number of training data point, the loss follows as expected\nAs we change the label of the datapoints between the hyperspheres, we take the set of all data points and show that s’ is completely mis-classified even though it is a completely valid member of the training dataset.\n\nintuitively this can happen only if the boundary we have learned has “Skews” at each training point\n\nWhat this means is the learn decision boundary is quite complex\nThis complexity that even the most refined hypotehsis class is quite complex\nThis proves the bounds are vacuous\n\nThis overparameterzed deep network can\n\nLooking aheead it’s important to understand the complexities contained within the decision boundaries and derive new tools as a test case\n\n\n\n\n\n\nNeural Tangent Kernel’s (NTK’s)\nTheoretical contribution\n\nWhen width is sufficiently large (polynomial in number of data, depth and inverse of target accuracy) the predictor learned by applying gradient descent\n\nEmpirical contribution\n\nDynamic programming techniques for calculating NTK’s for CNN’s + efficient GPU implementations\nThere is still a gap between the performance of CNN’s and that of the NTK’s\n\nThis means that the success of deep learning cannot be fully explained by NTK’s\n\nFuture directions\n\nUnderstand neural net architectures and common techniquest from the lends of NTK’s\nCombine NTK with other techniques in kernel methods\n\n\n\n\n\n\nSlides Poster\n\nLearning large overparameterized DNN’s, the empirical observation don extremely wide networks shows that generalization error tends to vary\nDeep RELU networks are almost linear in terms of their parameters on small neighborhoods around random initialization\nApplicable to general loss functions\nGeneralization bounds for wide and DNN’s that do not increase in network width\nRandom feature model (NTRF) that naturally connects over-parameterized DNNs with NTK\n\n\n\n\nSlides Poster\n\nLipschitz constant means with 2 points X and Y, they’ll be close before and after being passed through the neural network\n\ngeneralization bounds and robust classification lean on this\nThis problem of computing Lipschitz constants is NP hard so we try to find tight bounds around this\nSay we have an accurate upper bound of a model\n\nWe can take a point f(x), and we can measure of mis-classification and input that back into the network\n\nWe can certify that if we perturb X in a small ball drawn around this delta, it odesn’t change the classification\nIf we can find this small lipschitz constanc we might be able to prove that this network has a form of robustness\n\nHOw do we do this?\n\nProduct of the norm of the matrices\n\nSimple methods like this give upper nounds to the lipschitz constant that are conservative. Can we do anything more accurate?\n\nWe cna frame up finding this Lipschitz constant as a non-convex optimization problem\n\nOver approximate the hidden layers via incremental qudratic constaints\n\nGive rise to a semi-definite program giving us this tight upper bound that we’re looking for\n\nWe can trade off scalability with accuracy of the upper bound\n\n\nHow does this bound we get compare to others?\n\nWe show that in general our bound is much tighter than other bounds\n\n\nAdversarial robustness\n\nHypothesis trained using adversarial optimizers\n\nEmperitically when we evaluate this lipschitz constant these networks have much lower\n\n\n\nAccurate and scalable way of calculating Lipschitz constants in Neural Networks\n\n\n\n\nSlides Poster\n\nLarge initial learning rates are crucial for generalization\nScaling back by a certain factor at certain epochs\n\nsmall learning rates early on lead to better train and test performance?\n\nLearning rate schedule changes the order of patterns in the data whcih influence the network\n\nclass signitures in the data that admit what the class is, but it will ignore other patterns in the data\n\nLarge learning rates initially learn easy patterns but hard-to-fit patterns after annealing\nNon-convexity is crucial because different learning rate schedules will find different solutions\nArtificially modify CIFAR 10 to exhibit specific pattern types\n\n20% are hard to generalize - because of variations in the image\nEasier to fit in the second set 20%, easy to generalize but hard to fit – this is by construction\nPath that imitates what the class is\n60% of examples overlay a patch on the image and the memorization of the patch early on shows this method fits early and doesn’t generalize well\n\n\n\n\n\nSlides Poster\n\nHow do we design principle regularizers for DNN’s\n\nCurrent technqiues are designed ad-hoc\n\nBatch-norm and dropout - we know they work, but noy why\n\nCan we prove a theoretically upper bound on the generalization and hope it improves performance\n\nBottle-neck prior\n\nmost priors only ocnsider the norms of weight matrices and because of this they get pessimistic bounds that are exponential in depth\n\nBounds that depends more on data dependent properties\n\nupperbounded by the weights and training data\ninformal theory is that this can be upper bounded by (see slides)\nJacobian norm isthe max norm of the jacobian of the model on the hidden layers\nmargin is the largest logit of the output, minus the second largest\n\nINterpretation of this bound is it measures the ’Lipschitzness” of the network around training examples\nNoise stability is small in practice with looser bounds (see slides)\nRegularize the bound\n\npenalize the square jacobian norm in the loss\nNormalzation layers such as batch norm and layer norm\n\nHelps in a lot of settings\n\n\nCheck the bounds correlate with the test error and we found that our bound correlates well with test error\nCOnclusions\n\nTighter data dependent properties\nBound will avoid the exponential dependency on th depth of the network and optimizing this bound helps to improve performance\nFollow up work : tigher bounds and empirical improvement over strong baselines\n\n\n\n\n\nSlides\n\nAddress something asked by DaVinci - How are we made?\n\nWe’re created from a single cell and it eentually creates every cell in our body.\nHow does this process happen?\n\nCells are like tiny computers, taking input and output through things like proliferation, differentiation, and activation\nALl cells have the same genetic code – 3 billion letters\n\nHow our genome is the instruction set for assembling the different cells\nTelling eachother how to behave\nWe have 100’s of cells\n\nSingle cell RNA-sequencing in droplet micro-fluids\n\nMeasures for every single gene and cell for what gene it is and what cell it came from\n\nThe data matric for one sample ~ 250 million\n\nGene by cell matrix that is rife with errors and artifacts from measurement\nWe only capture about 5% of the transcripts, or what the humans are expressing\n\nIn the field, zero-inflation has taken root\n\nIt’s wrong\n“Drop out” – this is uniform sampling and it sometimes leaves us to capture no gene or transcription gene\n\nevery sample is affected by this\nNo value is at it’s actual value\nThis should be modeled properly and not with 0 inflation\n\nHow do we handle all of this data\n\nWe like to visualize into 2 and 3 dimensions\nPCA failed this data type and we couldn’t visualize it very well\n\nFollowing a Keynote at NeurIPS, someone presented T-SNE and it seems to fit the data well and we get good cell level separation\nWhile we might have a matrix of 20000 genes x 100000 cells – T-SNE and UMAP seem to capture this non-linearity well\nWe have this manifold because cell phenotypes are highly regulated\n\nWe can see in 3D the nicely shaped non-convex shape\nSimilar shapes in families because few regulators drive cell shape\nLots of feedback loops and interactions between these genes, which limits and constricts the phenotypes a cell can be in\nStill have challenges in visualization\n\nBuild better vis for this data as it’s non-uniform and 5 orders of different density\n\nChallenge is to handle this data with such different densities, it trips up many of the approaches we have today\n\n\nCommon way to viz beyond 2 or 3 dimensions using nearest neighbor graphs\n\nWe connect a cell to cells nearest that cell\n\nThis is dependent on probability distributions which help to define this similarity metric\n\n\nThe idea is one we have this graph we can really retain the manifold and use things like geodesic differences\n\nEach tiny differnce in this graph can represent small differences between cells\n\nWe can do distances and walks in these grpahs that allow us to measure the distance between cells\n\n\nData is extreme structures and there are communities\n\nSocial media community detection approaches find cell states and cell densities that are captures as cliques in the graph\n\nNice thing is that these graphs are connected\n\nThey share connectivity which allows us to cpature cell type transitions\n\nThese transitions are very sparse relative to the cell type\n\n\nUsing 100 ro 200 examples over 10000 entities\n\nThis isn’t regular science and this works because biology has lots of structure and isn’t adversarial in that respect\n100000 or 1000000 cellshave awesome things – treating each cell as a computer we can assume that the mollecrular influences create statistical dependencies in the data\n\nOut of the box algorithm gets us a correct reconstruction with no prior information of TCell networks\nThisallows us to do disease regulatory networks and helps us to understand what is wrong in this specific cancer patient\n\nAsynchronous nature of the data\n\nAll of our immune cells are in our bone marrow and these cells are able to generate all varities of immune cells within our bodies\nAsynchrony enables the inference of temporal pgoression\nFrom a single time point we can capture all of the dynamics of the process\n\nPesudo-time\n\nReconstructing developement which allows us to reconstruct order grom a single time point\nTHis process is highly non-linear we can order cells by chronology and the assumption is cell phenotypes change gradually ` * Cannot be treated as absolute values as we know this data is incredinly noisey\n\nWanderlust\n\nWe are able to reconstruct accurate progressions and discover order and timing of key events along differentiation\nThis checkpoint of DNS recombinations inside of a cell, we wanted to understand if it were OK\n\npediatric cancer is caused by understanding this checkpoint\nThis wasn’t known until we could find this tiny new cell population and it’s novel regulation\n\n\nData is structured\n\nbifurcations through use of walks and waypooints\n\nthe direct route between two cells along the same path should be more direct than non-immediate connections\n\nThese waypoints help to resolve structure\n\nWe find these using spectral clustering\n\n\n\nMapping development\n\nWe want to order these cells on their manifold and understand how they bifurcate\nWhat decision making is going on and what is their possible future cell types and propensity to turn into these cell types\nPalantir : Building a Markiv Chain out of this graph allows us to find time ordering in our neighbor grpahs\n\nstrong assumption that development goes forward and not back\nbroken in processes such as cancer\n\nbuild a directed graph from this\n\nwe can look at the extrema states and we find ourselves with an absorbing markov chain\nThis allows us to compute the end states of all of our cells and we can roll out the fate for each cell\n\nentropy of these probabiities for all cells\n\nThe proof is in the pudding – applied to early mouse developemnt (endoderm (all internal organs are made of this))\nData organized nicely along these dimensions\n\ncells aligned along temporal orders\napproximal distral organizations\nThese organizations happen head to tail\nA smooth gut tube, even though we can’t see anything that accounts for this organization, we can see the primodal organs jutting out from this tube\n\nWe can transcriptionally see where cells are headed a full day before they progress in that way\n\nWe go into the early days of the first decisions of the cells\n\ncell can become one of many classes\n\n\nWe can take spatio-temporal maps of the mamaallian endoderm\n\nWe see when FGR1 and FGR2 are both high, they’ll be primitive endoderm\n\nVery high entropy Right before this deicsion is made and entropy drops immedaitely after\n\nanalysis shows that biologist saw that these cells are plastiq – they can change by jumping out of that area and into the emryonic layer and assume the nature of the other cells\n\nPlasticity was predicted computationally, and we were then able to verify empirically\n\n\nThe Human Cell Atlas\n\nCells in our body, relationships between then, and transitions that happen within the human cellular system\nMost of the data is still single cell genomics\n\nThis atlas will have single cell genomics and spacial information of these cells\n\nThis will require tons of computation\n\nglobal and open community that anyone can join\npublic data of 10 billion cell playground\n\nHuman cell atlas will serve as a healthy reference and ground truth for disease\n\nThe methods we have now don’t scale\nData harmonization\n\ndata from multiple samples that might be diseased\n\nour methods mistake disease for biological differnce\n\n\nFactor analysis for good gene programs\n\nHow this data factors betwen cells and genes\nSimply comparing disease to normal\n\n\n\nLatent sapces : Deep Learning in scRNA-seq\n\ncount basis projected into latent space\n\nData denoising and integration\nlow dimensional embeddings\n\nInterpretation of latent factors is still lacking\n\nOur goal is not to predict, but to understand\n\nOften the outlier is the most important\nmachine learning is all about the common mechanism and not the outlier, whereas biology wants to know those outliers\nKeep our eye on the goal in biology and understanding that something rate\nDendritic cells are rare\n\nThese cells split into different\n\nCell types aren’t necessarily clusters\n\nThough clusters still have their own version of structure to them\n\nThe more we zoom in, the more we find structure in this data and we see that meta-cells have real peaks in their density\n\nThese meta-cells are defined by different programs and different covariances\n\n\nAcute myioloid lukemia is accute cancer gone awry\n\nNormal immune cells seem to overlap\nThese are early projectors of cancer cells – before they go awry and crazy\nWe want to know what happens that normal &lt;&gt; breaks, and cancer forms\nWhen we look at classicial methods, these diseases don’t connect\n\nWe believe in covariation and find a manifold that is driven by covariation and not just normal distributions\nCovariation in much lower dimensional space is much more computationally efficient\nThe regulatory systems that go awry\n\nWhen we run these methods, we can see exactly where the cancer breaks off and becomes cancer\n\n\n\nResponse to therapy\n\nBone marrow transplant patients who relapse and understanding how that relapse happens\nUnderstand the immune populations that differ between them, and using these dynamics we can see cell populations that really follow and raise up as the tumor burden rises and falls\nThese are very tiny populations, so one has to be very careful in computation\n\nEpigenetic data\n\nWhat potential regulators that can be regulating these systems\nWe can build generative processes, and using these latent variables we can understand different properties of these biological systems\n\nWhat is the covariate nature\nWe can understand inter-variable influence\nWhat factors combine to what targets through their regulators\n\n\nMost cells in a tumor in solid tumors are not cancer\n\nimmune cells and supportive tissue make up 90%\nusing factor analysis we can see cancer highjacks early development of these processes\n\nusing a program that the embryo knows to metasticize a new organ in another part of the body\n\nunderstanding how they survive in the brain\nIdentify that all cancers, both breast and lung, have the same gene that created their ability to survive there\n\n\nCancer uses regenerative mechanisms for it’s evil deeds\n\n1 change in 6 billion base pairs can make it go different under injury\nThe reason that this is is because there is enormous cross talk and remodeling between the eputhelial systems and the cancer\n\nSpacial techniques are critically important\n\nRapid autopsy programs allow us to collect samples\n\n\n\n\n\n\nSlides\n\nStochastic optimization and empirical risk minimization\nWe want to minimize the empirical risk of a very large sample\n\nConvergence theory\n\nDepending on loss function’s convexity or strong convexity\n\nOnline Convex optimization\n\nHere we consider an online game where each player predicts t+1\n\nsuffers a loss \\(f_t(w_t)\\)\nloss measures total loss of a fixed \\(w\\) from hindsight\nvery similar to SGD\n\n\nCompressed sensing / sparse optimization\n\nLASSO\n\nminimize quadratic function with a constraint in the \\(l_1\\) norm\n\n\nProximal gradient methods\n\nAdding up of convex\n\n\n\n\n\n\nSlides\n\nImitation learning is a powerful method for learning complex behaviors\n\ndriving cars, flying drones, and grasp and pitch\nBehavioral cloning\nSupervised learning through observations of experts\nNot perfect\n\nExpert state and we roll out these states we get errors of the imitator acculator that show up in other parts of the state space\ndistributional shift that arises due to this causality\n\nDoes more information lead to better performance?\n\nWhat happens under distribution shift?\nTurns out that a given model learns to pay attention to the road and brakes when someone is in the road\n\nThese fail because the model cannot infer causality\nCan we predict the expert’s next action\n\nThis is the only cause\nThe expert ignore it and its nuisance variable\nEnd state variables\n\nIf we learn 1 imitator that watches the road\nWe can learn another imitator that wrongly treats both variables as a cause\n\nIN general if we have 2^N possible causal graphs\n\nExistence of causal confusion\n\nConsider 2 examples\n\nlearns actions through history and one that doesn’t\nvalidation score on held out data history plays a role in working well with history but in test it causes confusion\n\nHow do we demonstrate this\n\nWe add to the original state and use this information to create confounded states\nThis corresponds to having just the causal\nUse a VAE and treat the dimensions of the latent variables as potential causes\n\nUsing behavioral cloning on the original state we get expert like rewards\nOn confouded states, we do much worse indicating causal confusion\n\nWhat we need is to have a causal graph that indicates the random variables that the expertt pays attention to\nIn the first phease we learn from all possible causal graphs\n\nBinary vectors 1 - cause, 2 - nuisance\nrandomly sample a causal graph and mask out the nuisance part of the state\nwe concatenate the graph and feed it to into a NN and it predicts an action\n\nbehavioral cloning loss\n\n\nIn the second phase we infer the correct causal graph\n\nintervention changes th distribution of the state\nwe score all possible graphs on additional information\n\nMode 1 : query reward\nMode 2 :\n\n\nCollect trjectories as policies\nQuery expert on states\nPick graph most in agreement with experts\n\nDAGGer baseline performs significantly worse\n\nLearned graph visualization\n\nlearned causal graph can ignore nuisance variables\nMore information can hurt performance without this effort\nHow to scale this up to more complicated tasks\n\n\n\n\n\n\nSlides\n\nMDP formulation\nDivergence minimization perspective on inverse learning\n\nGAIL or AIRL\n\nKL and JS divergences could be used\nAdversarial training for divergence minimization\n\n\n\n\n\n\nSlides\n\n\n\nSlides Paper\n\n\n\n\nStandard RL Setting\n\nDiscount factor in play here\n\nThis can be modeled as an MDP\nGoal : find an optimal policy to maximize reward which is some long term objective\n\n\n\n\nSlides Poster\n\nInnate abilities in animals\n\nwe’re beginning to understand that ML architectures seem to have innateness\nCNN’s are so well suited to image processing that they can do many tasks in that area\n\nHow far can we push this innateness idea\n\nTo what extent can NN architectures along encode solutions to tasks?\nDifferent kind of NAS\n\nWE’re looking for NAS’ that perform without any training at all\nJudged on 0-shot performance\nBecause of the large weight space\n\nUse a single value for our weight space\nJudge how well the network works by doing several roll out with different values\n\nCreate a population of minimal networks\n\nThese have inputs with no hidden nodes, connected to some outputs\n\nPerformance of the network is averaged over rollouts and then ranked\n\nvary the networks to create new populations and continue the process\n\nWe can vary in one of 3 ways\n\nInset node\nAdd hidden connection\nChange the activation function\n\nGauss\nReLU\nSigmoid\n\n\n\nTested on 3 RL tasks\n\ncart-pole\nbi-ped\ncar racing\n\nCompare these WAN found topologies\n\nwhen trained can reach SoTA\n\nRandomize the weights they don’t perform well\nShared weights produce pretty good behaviors\nIf we tune the weights, they perform the same kind of SoTA performance as general purpose networks\n\nAble to do this with much smaller networks, sometimes orders of magnitudes\n\nFor fun we tested this on MNIST\n\nHere we use a random weight – we get an expected accuracy of 82%, best single weight is 92% – little better than linear regression\n\nSearching for building blocks toward a differnt kind of neural architecture search\n\narchitectures that have innate biases and priors\n\n\n\n\n\n\nSlides\n\nWhat has changed since Rosenblat started playing with NN’s?\n\nCompute power\nData\nThis gave Dean the idea to found Google Brain\n\nEarly 2010’s\n\nEdge TPU - 2 watts and 4 TOPS\n\nMobile-net-v2 @ 400FPS\n\nCoral.ai prototyping boards\nRunning workloads locally is important, BUT NOT SUFFICIENT, tool for implementing and reasoning about privacy\nHave to be “smart” about what data is thrown off of a device using ML\nSanity wrt energy consumption and other natural resources\n\nmoving data is what costs energy\nonce the data is in a register and we want to operate over it, it’s relatively free\n\nmoving it is what costs energy\n\n\n\nHow do we make these giant distributed frameworks run efficiently, effectively, and privately?\n\nFederated learning allows centralized inference but localized training\n\nScale?\n\n100’s of millions of android phones have machine learning running on them\nScale story is more complex\n\ndata are both abundant and rare / precious\n\nwe don’t always have access to the data\n\ncompute is both massive and limited/precious\n\ndon’t effect UX\n\nPremium on quick covergence, ie/ &lt; 1 pass over the data\nFL both enabled ML fairness and can be in tension with it\n\nlong tail and rare event learning can endanger some of the privacy aspects\n\n\nGenerative models are really important in the federalted setting – and it’s not just about making pretty pictures…\n\nCapturing this underlying data generating distribution is key\n\n\nOpen problems\n\ntightening bounds, extending domains, handling infrastructure\n\nWhere is all of this AI stuff going anyway?\n\nML / Data Science\n\nConflicting narratives\ngenerally discussing regression problems\n\nAI\n\nPassing a test or winning a game\nSuper human performance given\n\nA well defined problem\nA loss function\nenough data\n\nJust how remarkable territory this actually covers\nWhat’s the loss function for more profound things like\n\ncriminal justice\ncouples therapy\nArt\noptimal hiring\n\n\nThis is not just a human issue\n\nNeurophilospher\n\nSuccess of ANN’s, notwithstanding, is a far cry from what intelligent bodies on this planet can do\n\nMotivational tribes are messy - paper in 2016 Paper\n\n\nEcoli\n\nbacteria have a 1 bit output\n\ncan go forward / back / turn like an RC car\ntrjectory os ecoli looks (see slides)\n\nEnergy is a function of their consumption and subsequent output\nIs this consumption methodology optimal?\n\nWhat actually has been optimized by evolution in this process?\nInverse Reinforcement Learning\n\nwell studied but ill-conditioned\n\nModeling a signalling and sensoring function of the bacteria\n\nGenome is the reward map here\n\nthrough this evolutionary approach we can see chemotaxis(SP?)\n\nThe error bars on this example are relaly big\n\ncolonies have lots of reward maps\nhuge variety of reward maps that do work (see slides)\n\nWhat persists, exists\nevolution decides on what is good or bad\nthis is not exactly optimization**..\nSimple GAN Paper\n\nAll points are stable in wasserstein gan’s\nThe combined GAN is not doing gradient descent, locally each actor here is going gradient descent of its own well-defined cost function.\n\nPut together, the combined system\n\n\nLoss functions and gradient wrt special and general relativity\n\nEvery actor is curving the space and this leads to general relativity\n\nMany “solutions” in this bacteria case\n\nsignally begets collectivity\n\nOptimization is not really how life works\n\nit’s also not how brains work\n\n\nBackprop\n\nLooking at real neurons are a hell of alot more complicated\nBrains don’t just evaluate a function\n\nThey develop\nimprint\npre-programmed tasks\nself-modifying\n\nLooking at learning in ML, we’re trying to minimize a loss by picking a particular set of weights\n\nChain rule for all of this stuff\n\nbackprop through a linear layer, we can see that these backprop equations look very similar to forward prop\n\nthere’s a symmetry here\nAlso this weight update equation looks kind of Hebbian\n\nANN’s generally only implement the top part of the equation\nIf we didn’t do the bottom parts of the equation, it wouldn’t be doing much\n\n\nThe learning part of ML is a lot more complex than the feedforward linear layers in RELU\n\nThere’s always feedback\nThere’s always temporal dynamics\nAlso\n\nmomentum\nmini-batch\nadam\nstructured ranom init\n\n\nNeurons have all the building blocks\n\nPer-cell state\nPer-synapse state\nTemporal averaging\nRandom number sources\nmultiple timescales\n\nCan we learn to learn with these building blocks?\nA more general, biological symapse update rule that doesn’t require gradient descent\n\nLSTM at every synapse\nShared weights, but individual state\nNoise gate g\n(Anti-)Hebbian\nNeurons can behave the same way\n\nPer cell state and learned behaviour for how to propagate\n\nEquations are factorial and not scalary\n\nChemical activity\nallows multiple timescales\nmu parameters allow mixing at different time scales\nSlow timescales needed for learning, but also useful for time- qquestions\n\nWeights (or connectum)\n\nlearning\nDevelopment\n\nLSTM parameters\n\nIn supervised learning paradigm:\n\nshort brain lifetimes\n\n\nSelf-organizing neural cellular automata\n\nSelf-training NN’s that are training each cell\n\nreproduces a pattern\nlearns how to do this via purely local interactions\n\n\nThese kinds of fundamentally social concepts and ensembles of “things” come together and create the systems we have today\n\nhow we find this dance\n\nGrand Challenges\n\nBrains with fully evolved architectures\nUnderstanding and characterizing evolved systems\n\nrealm of anthropology and sociology\n\nProblem solving by artificial societies\nLarge-scale meta learning in the Federated setting\nPurposive “artificial ecology” engineering\nDynamical systems theory for neural ensembles\nCan we define quantitative “SOTAs” for sociality?\nCan we think about what it would mean to approach this kind of “curved space” AI ethics\n\nArtificial Life approaches?\n\n\n\n\nSlides Poster\n\n\n\n\nThese things are linked together in really interesting ways and he’s going to convince us of this\nConnected to the notion of agency\nSome people think that it might be enough to take what we have and grow our datasets and computer speed and all of a sudden we have intelligence\n\nNarrow AI - machines need much more data to learn a new task\nSample efficiency\nHuman provided labels\n\nThese dont catch changes in distribution, etc.\n\nNext step completely different from deep learning?\n\nDo we need to take a step back to classical eras?\n\n\nThinking fast and slow\n\n2 tasks\n\nkinds of things we do inuitively and consciously and we can’t explain verbally\n\nThis is currently what DL is good at\n\nSlow and logical, sequential, conscious, linguistic, planning, reasoning\n\nFuture DL\n\n\nWe’re generalizing in a more powerful and conscious way in a way that we can explain\n\nThe kinds of things we do with system programming\n\n\nMissing to extend DL to reach human level AI\n\nout of distribution generalization and transfer\nHigher level cognitive system\nHigh level semantic representations\n\ncorresponding to the kinds of concepts we link back to language\n\nCausality\n\nMany of these things tend ot be causal in effect\n\nAgent perspective\n\nBetter world models\nCausality\nKnowledge-seeking\n\nThere are questions between all of these 3 things listed above\n\nIf we make progress in one, we can make progress in another\n\n\nConsciousness\n\nRoadmap for priors for empowering system 2\nML Goals : handle changes in dsistribution\nSystem 2 basics : attention & consciousness\n\nCognitive neurscience to understand the human side of the consciousness\n\nConsciousness Prioer : sparse factor graphs\n\nWe can think of these things as assumptions of the world\n\nThe joint distribution betwen these high level concepts can be thought of as a factor graph\n\n\nTheoretical framework\n\nmeta-learning\nlocalized changes hypothesis -&gt; causal discovery\n\nLocalized in some abstract space\n\n\nCompositional DL architectures\n\nArchitectures we should explore to introduce the compositionality that we’ll need to explore\n\nNN’s that operate on sets of objects, and not just vectors\nDynamical recombination\n\n\n\nChanges in distribution (from IID to OOD)\n\nArtifically shuffle the data the achieve that?\n\nNatures does not shuffle the data, we shouldn’t\n\nIRM paper from Bottou\n\n\nOut of distribution generalization and transfer\n\nNo free lunch : need new assumptions to change this IID assumption\nIf we discard this IID assumption, we need to replace it by something else\nBengio posits priors might be the way to do this.\n\n\nOOD Generalization\n\nThe phenomenon of learner being able to genearlaize in some way to a different distribution\n\nIf we are a learning agent (agent = actions)\n\nwe almost always face non-stationarities\n\nDue to actions of self (agent)\nactions of other agents\nmovement through time and space\n\n\nOnce we start looking at multi-agent systems, it gets even more complicated\nTHERE IS NO STATIONARITY IN OUR ABILITY TO SAMPLE REALITY IN THE WAY THAT WE DO\n\n\nCompositionality helps IID and OOD to generalize\n\nIntroducting more forms of compositionality allows us to learn from some finite set of combinations about a much larger set of combinations that are NOT in the set of the data that we have today\nDistributed representations\n\nHelps us see why we get an exponential advantage\n\nIf we make the right assumptions, these things can be explained by variables and factors, and once we train a bunch of eatures we can generalize to new combinations of these features\n\nEach layer is composed for the next one, and this gives us another exponential advantage\n\nThe one we know best we find in language\n\nWe call this systemasticity\n\n\n\nThis opens the door to better powers of analogies and abstract reasoning\n\nSystematic generalization\n\nDynamically recombining existing concepts into new concepts\nEven when new combinations have 0 probability under training distributions\n\neg. science fiction scenarios\neg. Driving in an unknown city\n\nNot very successful with the use of DL systems\n\nCurrent methods when asking models to answer questions not in the distribution they do not know how to answer them\n\n\nConstrast with Symbolic AI Programs\n\nAvoiding the pitfalls of classical AI rule-based symbol-manipulation\nNeed efficient large scale learning\nneed semantic grounding\nneed distributed representations for generalization\nefficient = trained search (also system 1)\nNeed uncertainty handling\nBut want\n\nsystematic generalization\nfactorizing knowledge into small exchangable pieces\nmanipulating variables, instances, references & indirection\n\n\nSystem 2\n\nConsciousness and attention\n\nFocus on one or a few elements at a time\n\nwhen translating we focus on a specific word to do the translation\n\nContent-based soft attnetion is convenient, can backprop to learn where to attend\n\nSoft-max that conditions on each of the elements and we can see how well we match on context\nAttention is parallel in that we compute a score for each and decide which ones where we want to put attnetion\n\nAttention should be thought of as the internal action\n\nneeds a learned attention policy\n\n\nSoTA language models all rely on attention\n\nHow attention, connected to memory, can also unlock the problem of vanishing gradients\nOperating on unbounded sets of key value pairs\n\nWe can think of attention as creating a dynamic connection between layers\n\nas opposed to being hard coded today\nThis is great, but from the point of view of the receiving model, it receives a value but it has no idea of where it’s coming from\n\nWe condition to the value, we have the concept of a key, an identifier for where this value came from\n\nWe use this as a routing mechanism\n\n\nDownstream computation can know what the value it’s receiving and where it’s coming from\n\nCreating a name for these objects through a form of indirection\nwe have systems of operating on sets\n\n\n\nFrom attention to consciouness\n\nC-word is a bit taboo – but maybe not anymore\nnumber of theories are related to the global workspace theory\n\nThis theory says that what is going on with consciousness, there is a bottleneck of information in that some elements of what is computed in your brain is selected and then broadcast to the rest of the brain and influencing it\nConditions heavily on perception and action\n\nAlso gives rise naturally to the system 2 abilities above\n\n\n\nRelation to ML?\n\nML can be used to help brain scientist understand consciouness\nWork in neuroscience is based on fairly qualitative defns\n\nML can help us to quantify what this actually means\n\nFeedback loops help provide specific tests that we can use to measure these concepts\nOne of these motivations it to get rid of fuzziness and magic that surrounds consciousness\nProvide advantages to these particular form of agents\n\nThoughts, Consciousness, Language\n\nThere is as trong ling between thoughts and language, in that one can be translated between mediums farily easily though a lot of information si dropped on the floor during decoding\nWe want to explore things like Grounded Language Learning, by learning through environment interaction and perception\n\nAllows a learning to get to patterns through to it’s understanding of how the world works\n\n\nThe consciousness prior : sparse factor graphs\n\nWe can use these systems to encourage our learning systems to do a good job at out of distribution reasoning\nSparse factor graph\n\nattention : to form conscious state, thought\nA thought is low dimensional object\n\nWe sample these from a larger higher dimensional conscious state\nThe conscious states that we sample\nThe thoughts we consciously have are pushed through this consciousness bottleneck\n\n\nWhat do these computations mean\n\nSome kind of inference is required\nWhat kind of joint distribution of high level concepts are we reasoning about\n\nThink about the kind of statements we make with natural language\n\n“If I drop the ball, it will fall on the ground”\n\ntrue but involved very few variables in that the statement only contains a finite number of words\n\nThe relationship that I need to descibe can tightly capture the elements of this joint probability through very few variables\n\nDisentangled factors != marginally independent, eg. hand & ball\n\nWe think of these as having this very structured joint distribution\nThey come with very strong and powerful relationships\nInstead of imposing a very strong prior of complete margin independence we can find some prior that finds a joint distribution between these high level variables\n\n\nMeta-Learning : End to end\n\nMeta-learning is learning to learn\n\nBackprop through inner loop\nHaving multiple timescales of learning\n\niterative optimization like computation\nout of loop evolution\n\nIn this way we can talk about evolution algorithms, etc. and when we talk about this in the life of an individual\n\nlifetime learning is the outer loop and local interaction through time is the inner loop\n\n\nWe can train it’s slow timescale meta parameters to generalize to new environments\nWhat kind of hypothesis can we make?\n\nbecaue these actions are localized in space and time, because these things are locallly temporal then we can try to understand\nIndependent of cause and mechainsim – from an information perspective\n\nlearning from one mechanism tells you nothing of the others\nif something like this changes due to an intervention then ew only need to adapt the portion of the model that has to deal with that part of the distribution\nIt can be explained by a tiny change\n\n\nGood representations of variables and mechanisms + localized change hypothesis\n\nfew bits needed to adapt to what has happened\n\n\nHow to factorize a joint distribution in this way?\n\nLearning whether A causes B\n\nLearner doesn’t know but we might observe just X and Y\nTurns out, if we have the right composition then we can use this to learn about how to map X to Y, such as things like pixels that don’t have causal structure in that image itself\n\nThe assumption that these high level variables are causal doesn’t work on pixels\n\nWe cna’t find a pixel that causes another pixel\n\nLearning neural causal models\n\nwe can avoid the exponential number of grpahs that need to be considered through this\nONe of the things found was that in order ot facilitate the causal structure the learner should try to infer the intervention on which variable it was performed\nmost of the time our brain is trying to figure out “What caused the changes that I am seeing?”\n\nAble to find these commonly used causal induction methods\n\nAttacking this problem in a deep learning friendly way\n\ndefined obejective with some regularization\n\n\n\nOperating on sets of objects\n\nUsing dynamically recombinations of objects\nRecurrent Independent Mechanisms\n\noperating on sets of these objects\napplied to recurrentness\n\nstate is broken into pieces\nconstaining the way these networks are talking to eachother in that they done in sparse and dynamic ways\nvectors aren’t the standard vectors but rather sets of pairs\n\nnetworks are exchanging variables along with their type (key, value) pairs\n\nleads to better out of distribution generalization than those that don’t use these structures\n\n\n\n\nTested in reinforcement learning\n\nfound it helped in atari games\n\n\nRecap\n\nConscious processing by agents, systematic generalization\n\nSparse factor graphs in space of high-level semantic variables\nSemantic variables are causal : agents, intentios, controllable objects\n\nShares “rules” or modules that are reusable across tuples\n\nA particular subnetwork recieves input that is different dependent on context\n\nThis can be applied to different instances in that it’s much more like a bayes net but the same parameters can be used in many spaces\n\n\nanother really important hypothesis is that the changes in dsitribution are mostly localized if we’re presented information\nThings preserved across changes in distribution have to be grounded in that they’re stable and robust to stationarity\n\n\n\n\n\nSlides\n\nBrain is an organize that integrated bio-information with electricity\n\nworking on integrating across discplines to get better models of brain functions\n\nCausality???\n\nIf we understand how the brain does what it does, we can reverse engineer it and use that to understand it better\nCan we turn on and off other areas of the brain\nWe come to this conclusion around causality because human beings have observed the earht other and over again in many contexts and test these assumptions using models and test these assumptions and map them back to our models\n\nThis is important for neuroscience\n\nMany of the manipulations of the brain cause it to not function how it does naturally\n\nIt might make more sense to observe the system over and over again\nWe can test these models\n\nIf that’s the case, the human body is 98 degrees farenheit\n\n\n\n\nDepression (DSM)\n\ndepressed mood\ndiminished interest\nincrease or decrease in appetite\nhypersomnia or insomnia\n\nIs MDD prevention a viable therapeutic strategy?\n\nImagine a disease case where someone has a heart attack of failure\n\nmake that heart pump more normally\nIdea is to make a diseased heart and make it function more normally\nOne of the things that has the greatest impact on the system of the heart was aging\n\nmeasured variables in lifestyle that might help predict heart attack later on in life\n\n\n\nEmotions at latent networks\n\nThe idea is that we use fMRI to look at changes in bloodflow in the brain\n\nUsing these changes as a proxy for brain activation\nTaking healthy controls, or students\nPut them in a scanner and let them watch movies while inducing emotions\n\nUse ML to see what emotional patterns were induced\n\nAfter about 20 mins they tap the students and ask how they were feeling\nThis suggests we’re able to liberate the emotion from the patients’ brain without self-reporting\n\n\n\nAssumptions\n\nAssumption 1: Emotional encoding at the second timescale\n\nThis is important because people have classically thought about controlling variables and repeatedly observe a system\nUseful for studying vision, motor function, sensory systems, etc.\n\nThis system might be built to do something very different than emotional systems\n\nWhen we have a system to process emotions, we want something that has information resonance at a slower timescale than moving arms, and legs, etc.\nEmotions are encoded at the timescale of seconds\n\nAssmption 2 : Emergent Properties\n\nWe have a cell property and it traverses a neuron and checmical information is sent down the axxon\n\nhard to think about emergent properties of these systems\na seizure is an emergent property of the brain\n\nwouldn’t generate this phenomenon without more than 4 cells\n\n\nThe system is working together in an integration fashion to create these properties\nTHINK SLEEP\nLOcal field potentials can be used to measure certain properties of the system\nLFP coherence (functional connectivity)\nsycnhrony or coherence\n\nwe can infer directionality in a circuit\n\nCoupling between cell firing and LFP activity\n\nAssumption 3 : Local Field Potentials reflects the activity of populations of neurons (emergent features)\n\nTrying to find things that generalize across brains, species, and inviduals\n\nLatent Network Model\n\nEach layer is useful for one of things we wanted to do\n\nEach of us had to believe one layer of this model\n\nInformation across frequency and information that is leading or lagging\n6000 things that we could measure and quantify in an animals brains\nPhase offset\n\nAcquiring brain network behavior\n\n\n\n\n\nSlides\n\nHype and sensationalism drive some of the interest but there is a substory there around automation\n\nFei Fei Li - write in NYT that enthusiasm for AI is preventing us from reckoning our immersion into it…\nMichael Jordan\n\nNeed well thought out interactions with humans and computers\n\n\nThese are not new\nWe have a 60 year old design challenge to find an optmiality between divisions of labor and automation\n\nAutomation and user control\n\nWaht si the appropriate balance here?\nChallenges of automation\n\nAutomated methods may be biased or innacurate\nThese concequences can be quite damanging in the real world\nLoss of critical engagement and domain expertise\n\nWe lack a global view as humans and over-weight local information\n\nBalancing automation and control can be done through building models of capabilites, actions, and controls around the tasks that we perform\n\n3 Examples\n\nExploratory Data Visualization\n\nincorporate tasks that the user is trying to achieve into the design of the visualization of the data\nSee slides for multi-verse analysis of the topics that might be present in these documents\nWhat makes a visualization good?\n\nTask specific and subjective references\nFoundational issues in perception that we can build upon\n\nShows the long standing results in psychophysics in how our perceptual system can quickly decode visual types of information\nCommon exploration pitfalls\n\nOverlooking data quality issues\nFixating on specific relationships\nMany other biases… *Data Voyager\nexamples in slides\nWe want to suppoer systematic considerations of the data\nModel user’s search frontier, optimize for related chart specification, seeded by the user’s current docus\nCandidate charts pruned and ranked using a formal model of design constraints\n\n\n\n\n\n\n\n\n\nSlides\n\nPaper has been around for almost 2 years now\nLots of forward work – along with an extended paper on arxiv today along with code\nData efficiency is a serious problem for deep RL\nPrior and weights are typically very difficult to interpret\n\nWhy do we expect good performance?\nPossible what we are doing inference with a terrible prior\n\nSeminal results of the paper\n\n1994 - showed that a single MLP with K hidden units, with a carefully scaled prior\n\nscaling outgoing weights by 1/K – as you take the limit as k -&gt; inf\n\nthe vector converged to distribution multivariate with mean 0 and unit variance\nsome form of gaussian quadrature\nproves this with standard multivariate central limit theorem\n\n\n\nCentral Limit Theorem\n\n1 dimenstional CLT there are some interesting things\nRandom variables converges to CDF at all points where the CDF is continuous\nCLT says that if we consider IID rv with mean 0 and unit variance\nSome sublties\n\nConsider an IID sequence of 2 possibilities [-1, 1] with P(0.5) has mean 0 variance 1\nWe can define a set A\n\nThen for all n where A has probability zero under N(0,1)\nThis set has 0 probability under this distribution\n\nbe careful with what convergence of distribution actually means\n\n\n\n\nWhat does this mean for a stochiastic process to converge in distribution\n\ncarefully scaling the prior\nweights coming out of these layers will have 1/k_1 and 1/k_2 respectively\nc_2 is a normal quadrature is defined in terms of the covariance of the previous layer making it a recursive kernel definition\n\nDeep Neural Networks as Gaussian Process\n\nReleased same day and accepted as same confernece\nCheck this paper out as well Paper\n\nRigorous proof provided\nWhy would we expect a CLT here at all with multiple hidden layers\n\nRadford Neil\n\nFeeding a single data point through and we can look at the f_1 units - each will converge independent of other variables\n\nMultiple input data points\n\nthere is a correlated normal vector at each f^(1)\n\nat some point, increasingly independent vectors converge to a correlated normal vector\n\n\nProblem with the argument\n\nPreliminaries\n\nNeed a convergent non-linearity\n\nDraw a bounding envelope on any point around the non-linearity\n\nwe might get something that might not be defined if we don’t, it effectively stabilizes everything\n\n\nThink about the network as an infinite sequence of network\n\nThe hidden layers may grow at different rates as long as they all tend toward infinity\n\nFormal statement of the theorem\n\nProof sketch\n\nproceed through the network and by induction starting to closest data\nat each layer, reduce the problem to the convergence of any finite linear project of data and units\n\nExchangability\n\nAn infinite sequence is exchangable if any finite permutation leaves its distribution invariant\nde Finetti’s theorem\n\nan infinite sequence of random variables is exchangable iff ti’s IID conditional on some random variable\n\n\nExchangable CLT slide\n\napplies to triangular arrays\n\nExperiments in the paper are relatively small data with low dimensionality Slide\n\nIn the majority of cases considered, the agreement is very close\none can’t tell the difference between the GP and 3 layer NN\nSlide shows, empirically, there seems to be little difference between a standard GP and a DNN with 3 hidden layers\n\nLimitations of Kernel Methods Slide\n\nThis property might not be a good thing\nKernel methods are affine transformations of the training outputs\nThis limits the rperesentation that we can learn\n\nDeep GP’s\n\nNot marginal GP’s because they have finite restrictions in the norm\nThis prevents the onset of the CLT\n\nSubsequent work Slide\n\nCNN’s also converge to GP’s\nNeural Tangent Kernel considers not what just happens for the initial distribution of the NN, but also what happens when we apply gradient descent\n\n\n\n\n\nSlides Paper\n\nBackground\n\nVectrorize the output of the NN’s into an n x k vector\nwe know that the application that we have done will handle 1D output\nAll theoretical results apply to multi-outputs\nWe know that NN outputs are a function of the parameters which in turn are a function of time (think evolution of gradient descent)\n\nNatural GD\n\nappealing because it has convergence, covariance, and invariance under reparameterization\nFisher Information Matrix allows GD to take the curvature of the distribution space into account\n\nSmall changes in parameters can effect the training dynamics\nInverse Fisher allows us to take into account this space’s information geometry\n\n\nConcatenated Fisher Information\n\nwe can condition the FIM on a single data point x\n\nTraining dynamics under natural gradient descent\n\nNatural Nerual Tangent Kernel includes the fisher information matrix which includes the distribition geometry into account\n\nAssumptions\n\nNetwork overparameterization\nPositive definiteness\n\n\nImplications\n\nComputing the NTK yields an interesting result\nBound on prediction discrepancy Slide\nEmpirical results\n\nSymthetic data Slide\nTheoreitcal bound is meant to be tight\nThe values increase further away from data – see tails of the plot\nComparing the predictive distribution – see slides\n\nFuture Direction\n\nApproximate inference\nscaling to larger datasets\nClassification tasks\nGeneralization analysis\n\n\n\n\n\n\nThere is huge interest in the intersection between Neural Networks and Bayesian Believers\nepistemic estimation is REALLY important for areas with high risk\n\nbayesian or not there is a really great potential to healthcare and autonomous driving\n\nType of Uncertainty Slides\n\nEpistemic uncertainty “how much do I believe this coin is fair?”\n\nmodels’ belief after seeing the population\nreduces when we have more data\n\nAleatoric Uncertainty - “What’s the nest coin flip outcome?”\n\nIndividual experiment outcome\nnon-reducible\n\nDistribution Shift - “Am I still flipping the same coin?”\n\nIndicating a change of the underlying quantity of interesting\n\n\nQuick intro to BNN’s Slide\n\ninstead of learning point updates, let’s put a distribution in place here over the parameters\nin practice \\(p(w|D)\\) is intractable\n\nFind an approximation \\(q(W) \\approx P(W|D)\\)\n\n\nWeight space uncertainty is less interesting\n\nin many cases NN’s weights are NOT scientific parameters we’re interested in\nsymmetries/invariance in parameterization\n\nexmaples like swapping nodes and scaling of weights, we’re still approximating the same function\n\n\nThis introduces vagueness Slide\n\nsample weights from the Q distribution\nfolklore belief for function-space (or output-space) uncertainty:\n“Epistemic uncertainty should be high when new input is less similar to observed inputs”\nWhat do “high uncertainty” and “less similar” mean qualitatively?\n\nThis is typically “eye-balled”, leaving it to be subjective by definition\nThere is really no agreeable diescription of where and by how much it should be higher\n\n\nEvaluating by comparing to references Slide\n\nBNN’s performance relies on a approximate posterior\nEvaluating inference:\n\ncomputes some distance metric between q(W) and p(W|D)\n\nFunction space “reference posterior” for BNN regression:\n\nsome hope in function space\nwide BNN has GP limit (under certain conditions)\nfor regression problems \\(p_{GP}(f|D)\\) is tractable\n\n\n\n\n\n\nSlides\n\nHow do we accomplish learning from scratch or from very small amount of data\n\nModeling image formation\n\ngeometry of the image\nSIFT features, HOG features, etc.\nFine tuning from ImageNet features\nDomain adaptation from other painters\n\nFewer human priors as we move down the list above\n\nCan we explicitly learn priors from previous experience?\nBrief Overview\n\nGiven 1 example of 5 classes :\n\nclassify new examples\n\n\nHow does meta-learning work?\n\nOne approach : parameterize learner by a neural network\nAnother approach : embed optimization into the learning process\nThe Bayesian perspective : learn priors of a Bayesian model that we can use for posterior inference\n\nThe problem\n\nHow we construct tasks\nWhat if label order is consistent?\n\nA single functional can solve all the tasks\nThe network can simply learn to classify inputs, irrespective of the data distribution\n\n\nMeta-training to “close the box”\n\nIf you tell the robot the task goal, the robot can ignore the trials\nanother example : pose estimation and object position\n\nmemorize the post and orientation of the meta-training set\nat meta-test time, without knowing the canonical orientation, we don’t be able to accurate predict the orientation\n\n\nWhat can we do about this?\n\nIf we had a proper bayesiaan meta-learning algorithm that was learning a proper posterior, we might not have this problem\nHowever, I’m not sure if we have the tools to create a proper meta-learning algorithm\nIf the tasks are mutually excluse, a single function cannot solve all the tasks (due to label shufflinf, etc.)\nIf tasks are non-mutually exclusive, a single function can solve all tasks\n\nmultiple solutions to the meta-learning problem\n\n\nMeta-regularization\n\nControl the information flow such that we can do zero-shot learning from the data\n\nminimize the meta-training loss and the information contained within the parameters of the model\nregularizing the weights forces the model to use information from the data as opposed\n\nCan combine this with a favorite meta-learning algorithm\n\nDoes meta-regularization lead to better generalization?\n\narbitrary distribution over \\(\\theta\\) that doesn’t depend on the meta-training data\n\nMeta-world benchmark\n\n\n\n\nSlides\n\nMotivation\n\nExperimental design problems that can be cast a a global optimization over some parameter space\noptimizing on non-linear projections\n\n\n\n\n\n\nWhy do we care about function space priors?\nLots of testing methods for bayesian approaches\n\nsee slides\n\nBut these all have non-Bayesian approaches that are competitive\nThree X’s\n\nExploration\nExplanation\nExtrapolation\nThese cases all depend crucially on having good priors that reflect thr structure of the underlying distribution\n\nCompositional GP Kernels\n\nGP’s are distributions over functions parameterized by kernels.\nPrimitive Kernels\nComposite kernels\n\ntaking products of kernels\n\nThis can express things like periodic structure that gradually changes over time\n\nNo need to specify structure in advanced and can be inferred online during training\n\n\nStructured Priors and Deep Learning\n\nDemonstrates the power and flexibility of function space priors\nProblems\n\nRequires a discrete search over the space of kernels for each candidate structure\nNeed to re-fit the kernel hyperparameters\n\n\nDifferentiable Compositional Kernel Learning for Gaussian Processes\n\nNeural Kernel Network\n\nrepresents a kernel\ninputs are 2 input locations\noutput is the value between them\n\n\n\n\n\n\nSlides\n\nChallenge the assumption\n\nASsumptions that the approximate posterior that we use to model our BNN, ought to have correlations between the weights\nMean-field assumption that our weight distributions are independent of eachother because we’re avoiding intractability\nThis is less true as our neural network gets much deeper\n\nWhy might our approximate posterior need to have correlation between weights?\n\nMaybe the true posterior does?\nA lot of intuitions we have come from this small interpretable single layer 4 neuron model\n\nWhat we think is that alot of these effects disappear as we get deeper and deeper networks\n\n\nWith depth, we can induce rich correlation over our output distribution with mean-field weights\n\none way to do this is to have covariance between \\(\\theta_{1}\\) and \\(\\theta_{2}\\)\nAs we get a deep network, we can get richer covariance structures\n2 inputs and 2 outputs with a simple weight layer w\n\nassuming linearity\nmean-field approximation\n\nLesson from the linear case\n\n3+ mean-field layers can approximate one full-covariance layer\nMore layers allow a richer approximation\n\nMeasuring the price of this mean-field approximation in NN’s that do have non-linearities\n\nHMC true posterior\nfit a full-covariance gaussian\nfit a diagonal covariance gaussian\nmeasure the difference between them, and it should give us an understanding of how costly the extra assumption of diagonality is\n\nMeasuring the ‘price’ of the mean-field approximation Slide\n\nhold parameters model throughout this testing\n\n\nWhat are the implications here?\n\nRely less on UCI evaluation with a single hidden layer\nMore research into other problems with Mean-Field Variational Inference\n\nE.g. sampling properties of high-dimensional gaussian (“Radial BNN’s”)\n\nLess research into structured covariance variational inference\n\n\n\n\n\nSlides\n\nIn bayesian statistics, priors are meant to represent our knowledge about the domain\nMapping domain knowledge to neural networks is hard\nControlled Directed Effect\n\nMeasure sensitivity of an outcome vaiarble to changes in a set of variables while all other factors are held fixed\n\nTypes of CDE Priors : Monotonicity and Invariance\n\nNeed to translate domainknowledge into expectations about CDEs for transition x -&gt; x’\n\nGuiding Functions\n\nOne way to think about input transoformations at every point x R, pick a direction R^{D} to push x\nthink of this guiding functions\n\nTranslating the CE into a prior\n\nCDE is expensive to compute, but can apprixmate it using gradients, and ignore scale\nDefine an error function for local invariance and monotonicity\nIpose a gaussian prior over this Error function above for a BNN\n\ncan be used for mini-batch variational inference\n\n\nContribution is proposing a framework for applying these priors above\n\ntoy examples\n\nin 1D - sampling from a NN - assuming independent gaussian priors for each weight – the functions depend a lot on\nimportant to understand this is a local constraint\n\nWe can impose a prior that it increases with\n\nConsider invariance case instead of monotonic\n\n2D manifold where the values of f and g are equal\n\nnot clear if we should be following f or g\n\npredictions are independent of changes in g\n\nsignificantly reduces error\n\n\n\n\nInvariance priors on COMPAS\n\nFirst trained model g(x) to predict defendant’s race, then trained a second model f(x) to predic recidivism w/ local invariance to g(x)\n\nwithout loss of accuracy, we can close the gap between false negative and false positive rates\n\nThresholding schemes\n\nThis si a building block for translating domain knowledge into a prior\n\n\n\n\nSlides\n\nPredicting the effect of human genetic variation from sequences alone\nProblems we saw in a number of areas dealing with the language of biological data\ngenotype to phenotype\n\nif we want to change the phenotype, we want to understand the interaction of the environment\nwe also want to design biological sequences\n\n10 billion people\n\n\n1000 billion genomes\n\nImportant to understand models and analysis we have are based in the 80’s and 90’s\nPotential sequences that are functional\n\npotential sequences that are functional are much much larger than that\n\nwe might want to predict this in expectation\n\nWhy do we need better prediction and design in biology\n\nuncertainty for medical decision making\nmolecular biology that impacts human health\npredicting how pathogens will mutate\nsynthetic biology for designing theripeutic impact\n\nWhat does this sequence look like under constraint?\n\ndon’t even have benchmark datasets ready for people to play with these environments\nmain thing to get across that the estimation of uncertainty really matters\n\nCan’t we just measure the effects of all genetic variation\n\nmutation effect prediction is hard\n\nmutation effect prediction lacks\n\nsparsity sampling\nnoisy\nchanging 1 position in the DNA, it’s not just thinking about that position but all of it’s impacts and confounders\n\npropr art:\n\ncompute what’s conserved across evolution\n\nsequence alignment and preservation\nthe way way we regard these sequences are the result of billions of experiments run on the human species\nnot accurate to look at one column because of dependency on positions\ncapturing the dependence between sequences\n\nusing pariwise factors are powerful\n\npsuedo likelihood because we can’t calculate the partition function\n\n\n\ncapture these complex dependencies\n\nwe can’t keep adding terms to likelihood models\nmutation prediction with a variational autoencoder\n\ninfer a generative model of the family\n\nestimate how probably sequences are\n\nA doubly variational autoencoder on decoder weights prevents overfitting\n\nbiological constraints included in the model\n\nlatent variables are generated for each sequence in alignment\nshowing that the latent space seems to be capturing some structure slide\n\nwe want to predict genetic variation\n\nhow are we going to know if we’re right?\nDeepSequence captures mutation effects better than state of the art\n\n\nBut P(X) has been dependent on alignments…\n\n“alignments” used by every branch of biology, genetics, clinical decisions – these are all methods from ~ 20 years ago\nall heuristics\nchallenge is to build models that don’t depend on these alignments\n\nalignment uncertainty\ninsertions and deletions\n\nReinterpret our methods in terms of structured noise distributions\nNew Seq2Seq regression model : intuition\n\nconditioned on the initial sequence X, predict sequence Y\n\ncan change letters in X and have the capacity to delete them\ndeveloped and explored simple seq2seq model\n\nused categorical distribution over nucleotides, etc.\n\nsample W from the prior over variables-size\n\n\n\nThis generalizes past algorithms and models\n\nHierachical latent alignment HMM\n\nSample latent x from a population\n\nproperties that make it really easy to use in BNN methods\n\nThis is an unsolved problem for HMM’s which have been used in biological sequencing\n\nmarginal is a smooth function of x and which allows for automatic differentiation\ninference method is SGD\n\nInference methods\n\nalignment HMM on the encoder side\n\nResults Slide\n\nSummary Slide\n\nhave done this on several families of protiens\nALignment uncertainty Example\nLatent representation from PCA models reflect the underlying biolog of VDJ recombination\nFlu Virus evolution Slide\n\nWe can see the evolution across the latent space\nwe might be able to predict sequences\n\n\n\n\n\n\nSlides Poster\n\nInteraction with liquids happens every day\n\nSpecific containers and specialised tools to manipulate these liquids\nWe can approximate the way these things will behave\nThe shape of the continaer has causal influence over the way that liquids interact with it\nViscosity of the liquid has intersting causal properties\n\nThinking about this from a robotics perspective\n\nSome of the things very natural to us are hard for robots\n\nthe complex properties of liquids makes this hard for robots\n\n\nWhat is it that we, as humans, do to help this manipulation\n\nCogSci theories\n\nWe have some approx simulation in our heads that enable these predictions\nPeople have shown that we can invert this simulator in our head and make predictions about properties in our heads\nDifferent types of interactions can give us different cues about viscosity\n\nWe need some sort fo fast approxiate like thsi for embedding in robots\nWe don’t need exteme accuracy but rather representing these objects in a more approximate and efficient way\n\nIntuition as approcimate simluation\n\nNVIDIA Flex\n\nPosition based dynamics\nAs with any simulation we use, we have a reality gap\n\nThis is discrepancy between observation in the real world and simulation environments\n\nSources of error\n\nmodel approximation – not much we can do here\nparameterization – we have to set the parameters of the model and without correct settings we’ll get variance in our predictions\n\nSim2real discrepancy is what we’re trying to track in this\n\n\nTwo stage process\n\nEstimate parameters & learn to pour\n\nwe want generative models to enable adaptatoin to dynamics of the environment\neven though it’s the same experiment, we want to minimize spillage, but at the same time we want it to spill a bit because it’s informative\n\n\nLearning how to pour\n\nHow to use the sim here\n\naction and observation spaces are as follows\ncount the number of particles that fall outside of the container\nmeasure the spillage with a scale, and noramlize such that we have a % spillage to compare between the two domains\nFind the relative distance between source and target container while measuring how fast it’s being filled\n\nThe way we do this is to model this as a Gaussian Process\n\npour N times (37 in paper though 15 should be enough)\nlearning combinations of velocity and relative spillage\nafter learning this we transfer it right to the robot\n\nApproximate fluid simulation is useful\n\ngeometry of the container causes high spillage!\ninitialization of policy with simulation works best\n\n\nTo stir or not to stir\n\ncalibration to properties of the liquid through perception\nThe way we calibrate is to perform, in a synchronous way\nCohesion models the best the change in viscosity in the real world\n\nthe condition is the thing that is being modeled\nsimulator cannot model adhesion\n\nthese characteristics cannot be simulated\nis there a way we can model this friction coefficient that might be present in specific liquids?\n\n\n\n\n\n\n\nSlides\n\nGoal of this research is to study generative models of physics from a cognitive science perpsective\n\nHemholtzian idea of perception as inverse optics\nSome uderlying true state of the world\n\nbut we don’t have access ot that\nwe only have access to retinal images\nthere is some lawful set there\n\ncan we invert this image, knowing what we know from optics, to derive information about the world\n\n\nThe world changes over time and gives rise to a sequence of images that we see\n\nthese changes happen in lawful ways such as dynamics\n\nWe don’t want to treat these observations as IID\nWe can use this to constrain our inferences\nPerception is constrained by dynamics\n\nPeople’s judgements about the slant of a ramp given the visual state of the ramp\n\nas our perception of the ramp changes, if affects how we perceive the world\n\n\n\nWhat are these dynamics in the world and how do we capture that?\n\nIntuitive Physics ENgine\n\nThe generative models we have in our heads are based on object baed representation\n\nSome probability distribution presents a range of world state\nThis gives us a range of possible ways the world might unfold\n\nWe run out model forward and count up the number of blocks\n\n\nImportant features\n\nobject based\n\nshows object based importance in early human development\n\nprobabilistic model\n\nNot just one possible future, but range that we can make predictions over\n\nWe don’t need a veridical model of physics but one good enough to action plan\nThis physics engine should favor speed and efficiency over precision\nMOdel is generalizable in that we don’t need to learn separate physics models for all situations\n\nHow do we do this?\n\nPredict - have a generative model of the world, ask what happens next, run out the model and observe\nProbabilistic framework unlocks a lot of additional capabilities for perception\nPerceive causality – remove A from simulation\nMake plans and choose actions based on these model run outs\n\n\n\nPhysics in the loop of perception\n\nPerceiving what is in the world\n\nseeing occluded objects Papers\nseeing surprising events Papers\n\nUnderstanding actions in the world\n\nSeeing Occluded objects with generative models in the loop\n\nIf we have a set of objects that have cloth draped over them, we can infer what object might be under that cover\nWe need some sort of generative model that allows us to internally ask “what would this look like with a cloth over it?”\nSee slides for how to model this occlusion phenomenon\n\nuse dynamics and physics of cloth to find a draped cloth geometry\nInference with Bayesian Optimization is key here\nUnderstanding How that cloth might drape is important for understanding what something occluded with cloth might look like\n\n\nSeeing Surpriving Events like an infant\n\nDetecting violations of expected dynamics\nPermenance\n\nobjects can’t teleport\n\nSolidity\nCOntinuity\n\nwhen objects violate these properties of how objects work, then they can update their model of the world\n\n\nWhat do we need to build into an agent such that it can percieve the world but then update their understanding of the world according to some surprise factor\n\nPerceive violations of these principle drives learning\n\nADEPT Model Slide\n\nGiven an image - we first extract object information\n\napproximate de-renderer\nthis object has attributes has understanding of position, velocity, etc.\n\nshape information is thrown away\n\n\npropose object masks\n\nfeed through renderer gets object properties\n\nInternal scene representation -&gt; physics observations\n\nobjects are moving at certain velocity and objects interact, they don’t move through eachother\n\nWe want to match the above observations against a “ground truth”\n\nthis isn’t matching in pixel space, but rather matching wrt objects\nwe also have to gracefully deal with unexpected events\n\ndisappears and we want to handle it by saying this is something weird that happened, but this is my new normal and I no longer need to track it\n\n\nMeasuring Surprise\n\nviolation of expectations (from psychology)\ncreation of a bunch of physics based violation types\n\nthese match to infant understanding principles\n\n\n\nInfants don’t see non-physical events\n\nthis allows us to constrain our space of potential evaluation\nobjects in shapenet\n\nAlternate Theory\n\nBootstrap these princples above\nCan we learn this from enough data?\n\nRapid trial and error (repuposing of objects)\n\nexample of a stake and a tent\n\nrule out branch, pinecone\npick rock\n\nfinding representations of the properties of these objects is inherent to planning in these situations\nthis seems to be a core feature for people\nPHYRE benchmark\n\nFocused on model-free RL from balanced datasets\nlearn generative model of the dynamics of the envornment\n\nVisual foresight for learning to push objects with tools\n\nfrom vision required many samples + demonstrations\n\n\nSSUP Framework\n\nsample, simulate, update\n\nPrior\nInternal simulator\nLearning mechanism\n\n\nConclusions\n\nCausal models of dynamics are important for perception and action\nTypes of representations & dynamics are crucial\n\nobject-based, approximate world models\n\nJust generative models is not enough – requires additional information\n\n\n\n\n\n\nNeural Netwoks and CONVNETS are super dense\n\nwe’re grabbing much of background context, etc that don’t necessarily matter\n\nHierarchical compositionality\n\nWay fewer parameters\n\nthe whole model was quite interpretable ane debuggable\n\neach unit was a node in a graph – allowing representations of images in graphs\ninference was done in a very hacky way\n\nAI Today\n\nAI for simulation\nSimulation needs a lot more learning involved\nOpen Problems\n\n3D Envornments / Scenes\n3D Objects\nActivities\nBehavior\n\nScalability, realism, diversity : Learn how to simulate!\nScene composition\n\nmaking this a little more scalable\nIn gaming, worlds are build using sort of probabilistic\n\n\nMeta-SIM\n\n\n\n\n\nCross-bite challenge\n\nBuilding Machine That Learn and Think Like People\n\nUnsupervised Object Tracking\n\nTraining (no annotations!)\n\nfind donstruction of videos in terms of moving objects\n\nTesting\n\nnew set of images from the same distribution and eval performance same as supervised learning\n\nSequential Attend, Infer, Repeat (SQAIR) Paper\n\nUnsupervised object tracking\n\nVariational autoencoder\ntrained by maximizing the ELBO\n\nhopes to learn the dynamics of the objects\n\n\n\nSpatially Invariant, Label-Free Object Tracking (SILOT)\n\nnew architecture\nincludes features to help it scale up\nallows objects to condition and coordinate on eachother\n\nwe can sidestep the require sequential structure\n\n\n\n\n\n\n\n\n\n\n\nLet’s let machine learning figure out the catastrphic forgetting problem\n\nFraming the problem up as a meta-learning problem\nThis is called “meta-training”\nOnce this training is done, we take the meta-vector and evaluate on all T tasks that have been learned\n\nOnline aware meta-learning Paper\n\ndoesn’t suffer from catastrphic forgetting\nlearns to induce a sparsity in it’s representation\n\nactivates fewer neurons – ie. most of them\n\nGets a lot right, but it’s still ultimately subjective to SGD\n\nhard problem of finding representation once SGD gets applied to it\n\nWe allow control over SGD “neuromodulation”\n\ndirectly modulated activations\n\n\nANML (Neuralmodulated Meta-Learning Algorithm)\n\nneuromodulatory network can gate the DNN that will also gate the backward pass\n\nselective activation\nselective plasticity\n\n\nOmniglot, following OML\n\neach character type is a class/task\ndifferentiate through 600 tasks\n\nevaluate on all 600 tasks – this is WAY too unstable for today’s SGD methods (like 9000 steps)\n\nLearn sequentially on one class in the inner loop\n\nContinual Learning is Hard\n\nNormal deep learning\n\nIID sampling (no catastrophic forgetting)\nmultiple passes through data\n\nSequential Learning *ANML might be leading to an overall solution to catastrophic forgetting\n\n\n\n\n\n\nRepeated Decisions with Imperfectly Known Consequences\n\nBrain science – they frame this as a multi-bandit problem\n\nExperimental Setup\n\nfour arm bandit tasks\n4 conditions\n\n\n\n\n\n\n\n\n\nDeep Understanding vs Shallow Understanding\n\nResponding (frequently) in behaviorally appropriate ways without really getting the overal picture\n\nlook up Eliza\n\nreveals the gulliability gap\n\n\nKeyword matches are used all the way through to 2014\n\nGoostman (won some version of the Turing test)\nDoesn’t represent real progress\n\nGPT-2 seems fluid\n\nlong way from early generations\nnot actually coherent\nOften plausible for a few sentences of text of surrealist fiction, where there are no facts of the matter\n\nPrediction at the world level != prediction at the world level\n“Local coherency; global gibberish” - Dan Brickley\nAdversarial NLI : A new benchmark for Natural Language Understanding\n\nState of the art models learn to exploit spurious correlations in the data – we see this in the visual perception field as well\n\n\nWhat is deep understanding?\n\nDeep Understanding is being able to\n\nconstruct an internal model of what is said/depicted in a story/article/movie/etc\nperform every day inferences about what is left unsaid\n\n“What do I think of Western civilization? I think it would be a very good idea.”\n\nArguably the closes to deep understanding of the oft-misaligned CYC\n\nCan make nuanced inferences about character motivations\nBut : system doesn’t have a natural langauge front end (you cna’t just feed Romeo & Juliet in)\nRelies on human experts to encode each problem\nthere are also serious issues of coverage, sealing with uncertainty, etc.\nMOve this out of it’s domain, it would fail completely\n\nin some interesting ways it’s the closest thing we have\n\n\nShallow prediction vs deep parser\n\nthere are lots of tools out there that are useful\n\nlots of coverage issues\nparse sentence into units and do symbolic computation\n\n\nHow might we get to deeper understanding : Two ways of thinking about that moving forward\n\n\nBenchmarks don’t encourage out-of-the-box thinking\nBenchmarks can and are often easily gamed\nThe Kaggle Effect\n\noptimizing for a single metric leads to tradeoffs and shortcuts which make you over specialized\n\nBenchmarks take a lot of time to develop\nBenchmarks are prepackaged; humans experience rarely is\n\nkids don’t get to download datasets and test\n\nWe shouldn’t, in princple, expect any single benchmark to suffice\n\nno one thing should be measured because the mind is not one thing\nintelligence is clearly multidimensional\ninvolves manY vectors\n\n\n\nAdvice to young scholars\n\ndon’t just look to what the ML community is publishing\nlots of extent data in other fields involving the brain that might be useful\nplenty of work suggesting other challenges as well\n\nChildren’s over-regularization errors\n\n1992\nwidely modeled throughout the 90s\nlots of people modeled this data, didn’t need to be on kaggle, but people tried to figure it out\nnot packaged nearly and nicely – we need to go find this data for our use\nAll models out there cheated relative to what a child does\n\nlist of stems in past tense forms as data but kids don’t have this available to them directly\nThey’re able to map grammar structures without the spoonfeeding of the field\n\n\nMarcus et al 1990\n\ncouldn’t use transitional probabilities because of the way they structured the grammar\ndata is still there, not in benchmark form\nkids only had 2 minutes of data\n\nInfant Learning Rule\n\nmany models proposed in 1999\n\nAdult generalization of inflection to foreign phenomenon\n\nHebrew speakers could generalize to sounds in English though they’d never seen it before\n\nUniversally quantified 1:1 mapping\n\nAll these are examples of free generaliation of universally\n\nEven today there are challenges in learning UQOTMS in systems that lack operations ovre variables\nOnly now is the importance of this issue started to become recognized\n\nOOD generalization\n\nComprehension challenge\n\nToward a benchmark for Dynamic Understanding\n\ndevelop internal models about what is happening\nDistinguished from static understanding\n\nconventional knowledge about what happens in general/generic/ordinary undertanding\ndynamic understanding is keeping track over time in some situation\n\n\nCaveats\n\nNot claiming sufficiency capturing all aspects of NLU\nNot claiming this is the only way to improve NLU benchmarks\n\nWe do think that too few existing tasks look directly at dynamic understanding\nTwo for static understanding\nFour for dynamic understanding\nThe dix tasks are illustrative not exhaustive\nStatic task 1 : Conventional knowledge\n\ntests understanding of every dat factual knowledge\n\neg. the part of a fish that gives it’s body rigidity is…\n\n\nStatic task 2 : transformations\n\ntests understanding of processes and actions that are either plausible or implausible\n\nMaking a salad out of a polyester shirt\n\n\nDynamic Task 2 : Atypical Consequences\n\nWhat happens when something unusual happens\n\nDynamic Task 3 : Entity Tracking\n\nA reader must keep track of entities in written text but this could also be applied in the computer vision side of the world\n\nDynamic Task 4 : QUantity Tracking\n\nPilot\n\nsetup:\n\n40 question answer pairs per task (after removing instances containing errors), via crowdsourcing;\n\nFuture Evaluations : Recurrent entity networks\n\nnot fully compatible as it’s geared around specific tasks\n\n\nDeep Understanding is hard\n\nWe shouldn’t confuse progress on superficial understanding for real progress\n\nWe can keep building things into the systems that give the innate properties (such as Convolution in CNN’s)\n\n\n\n\nSlides\n\nUsing brain imaging to study all kinds of cognitive processes in the brain\n\nreflection bring frustration in that more information has been discovered such as where in the brain or when in the brain is the neural activity\n\nless concentration on the “how” – this is a bit obvious as it’s easier to use these techniques to ask where and when\n\n\nOnce we understand the brain, what will be the form of the answer?\n\nDesign principles used wide in the brain\n\nwill vary in levels of detail – and we will surely have a description of how the brain computes\n\nWhether we might be on the verge of a time when we can take a new approach to studying the question of “how” in the brain\nRecent dramatic progress in the NN community where we’re gone from computers being blind, deaf, and dumb\n\nWe can now do many of these functions on machines\n\nAre we at a point where we can now take advantage of these NN models for things like vision and language and use them as hypoethesis about how to brain does these same things.\n\nPredicting fMRI output given people reading words\n\nhand designed vector embedding by co-occurence with 25 verbs\n\npredicting where in the brain we would find neural activity as a function of the input word stimulus\n\nthis studies where in the brain\n\nGustatory cortex activity is associated with activity related to co-occurences of words with words like “eat”\n\nsuggestions that the semantics of words are often grounded in parts of the brains who’s functions are affiliated with those words\nsaying “peach” caused activation in certain parts of human brains\nOur analysis is not asnwering the question “how”\n\nWe can look at “when”\n\nWhat information si encoded in space and time in the MEG video being shown\nIs the encoding of the semantics a function of time or is it a discrete process?\nTrained ~ 1,000,000 classifiers to predict activation in certain brain regions – the classifier was “decoding” the activity into words\n\nmost didn’t predict anything\nsome did\n\nduring the first 50 ms there was nothing to decode\nnext 50ms perceptual features could be decoded\n\nwe could get gross features of the line drawing as well\n\n200 ms, we have a semantic feature\n250 ms, is it hollow?\n400 ms even more\n\n\nThis is the kind of analysis that we can do on the When and Where details\n\nAt 50ms time intervals how accurately can we decode words from activity patterns captured through fMRI\n\ndecodability of feature “wordlength” (peak decodability 100-150ms)\nthis information doesn’t first appear “here” and then move\n\nit can be decoded simultaneously\nthere’s a synchrony that occurs between these disparate parts of the brain at the same time\n\nThis allows us to look at when\n\n\nMaybe these 6 regions work together to figure this out?\n\nmaybe not; maybe something else\n\n\nHow does the brain compute nueral representations?\n\nParadigm for studying “how”\n\nstimulus input to both models and the brain - compare the learned mappings - and measure the output of both systems\nour program is an example of “how”\n\nif we have 10 models, we can ask other questions like “does it allow us to explain, predict, the observed neural activity than the previous model?”\n\n\nAN example of what this paradigm implies\n\neach point on the slide is a hypothesis, the model obvs being the hypothesis\n\nthe more accurate the model is at recognizing the objects in this image\n\nthe better it is at predicting the neural activity in humans\ncorrelation between the two models’ performance\n\n\n\nCNN IT Alignment (Yamins et al 2014) Paper\n\nCNN v4 alignment\n\npenultimate layer – both predict more accurately than the output layer\n\n\n\nHow? : Language Processing\n\nSame concept as the CNN example above, this work was done using computation language models\n\nBERT\nELMo\netc.\n\nMEG scanner and showed the patients a new word every 500ms\n\nSentence mean MEG activity\n184 different sentences in passive and active voice\nUsed the above language models above and for every sentence they constructed each prefix of the sentence and fed it into the model\n\nlinear regression used to predict the neural activity\npredicted the 500ms neural activity\n\n\nWhich of these models works best/\n\nbrain actibity prediction accuracy*\n\nPaper?\n\nif we consider these models as hypothesis we can now rank them\n\n\nWill this paradigm really work? referencing the alignment approach\n\nthese studies have empirical evidence\nIs this helpful?\nLimits:\n\nmismatch between sequential computer processing vs. oscillatory, parallel neural activity\nthere’s a mismatch of constant activity in deep nets vs spiking in the brain\nmismatch in what we’re even measuring using these experiments\n\nbloog oxygen, fluctuations, etc. and actual neural activity\n\n\nImportant questions:\n\nDoes observed neural activity represent neural data representations, or processes that alter neural representations? (e.g. predictive coding : activity reflects energy being expended to update representations)\n\nword by word neural activity while reading - reading word number 4 doesn’t mean we can decode the first word\n\nwe can’t find it in the neural activity - we could when it appeared on the screen\n\nis this measuring a delta or the representation of the stimulus?\n\n\n\nWhat are brains truly doing/\nHow does context influence?\nShould we care if we model only part of it? (BERT doesn’t model word perception)\nIf we can’t interpret representations in deep nets, does it help explain brain activity in terms of these?\n\nAlisnging activity bween DNNs and neural activity\n\nwe can write down computation hypothesis and try to align these\n\n\n\n\n\n\nSlides\n\nSemantic defn – connected to language in that somehow through language we communicate a representation of the world through these high level variables\n\nthis is closely associated to the idea that we might be able to find these\n\nAnother connection one is trying to make is through that of agency – we are agents that act on the world and we cause changes in the world which induces distributional shifts\nWe need to figure out the OOD generalization problem\n\nis reality compositional in that i can build a symbolic approximation of certain properties of objects such as roundness for wheels and balls and redness for heat, etc.\n\nCompositionality is usually associated with linguistics\n\ndistributed represetations already exist in the idea of DL – and we can think of this compositionally\n\nthis has been intuiitively understood since the 80’s but now we can see why these forms of compositionality bring us up with exponential advantages\n\nCompositionality works because some assumption about the world can be exploited\n\nassumption is that I can learn about these features somewhat independently\n\nglasses independent of if that person is wearing shoes or not\n\nwe don’t need to see all combinations of these things to generalize\n\n\n\nSystematic generalization\nDynamically recombining representations of objects\n\neven when new combinations have 0 probability under training distribution\n\nscience fiction scenarios\nDriving in an unknown city\nattempting to exploit the regularity that is in the world\n\n\nClosure\n\nExpressions in novel contexts\nAssessing systematic generalization of CLEVR models (ARXIV) Paper\nMatching referring expressions (see slides)\n\nqualifier (brown cube)\n\n7 Tests\n\nsee slides\n\nCurrent SoTA\n\nStruggles on extension to CLEVR\n\n\nContrasting with the Symbolic AI Program\nChoices that are happening within the mind aren’t an active process\n\nThis system 1 computation in that it is intuitive and performs a selection of things that are relative to the situation\nThis is a reason why we need to put together the disiderata of the two fields of Symbolic and DNN approaches\n\nBuilding block for conscious processing is attention\n\nfocus on one or a few elements at a time\ncontent based soft-attention is concenient, can brkprop to learn where to atend\nattention is an internal action\n\nAttention is also a key to something also very important\n\nmemory\ncredit assignment\nvanishing gradients come up in training NN’s\nunreasonable to assume brains are doing anything like BBTT\nAlternative to this introduces at the last NeurIPS\n\nexploting memory – and we get this effect in things like transformers\n\nCredit assignment so we don’t make the same mistake multiple times\nSparse Attentive Backtracking\n\ndynamically building a graph that can relate the past to the present through many steps\nOn the fly we can create connections to the past and the present\n\neliminating the exponential loss of gradients problem\n\nAttention really forces NN’s to develop a form of representation for indirect references\n\nwhy is this coming up?\nattention mechanisms\n\nthere are many inputs “fighting” for attention and the module that receives the weighted sum and it sin’t able to understand which modules contributed to the weights\n\nthis allows us to think of NN”s as set transformation machines\n\nGlobal Workspace Theory\n\nBaars++ 1988, Dehaene 2003++\nbottleneck of sonscious processing\nselected item in broadcast stored in short-term memory\n\nLong term goal is to have ane nvironment where learning agents are faced with gradually more difficult tasks\n\nwhere humans are in the loop, helping the agents to figure out how it works and communicate with humans\n\nAffordances, options, exploration & controllable factors\n\naffordances : concepts / aspects of the environment which can be changed by the agent\ntemporal abstractions : options, super-actions, macros, or prcedures, which can be more complext procedures\ncontrollable factors\n\njointly learn a set of (policy, factor) such that the policy can control the factor and maximize the mutual information between policues (Bengio et al 2017) Paper\n\n\nConsciousness Prior :\n\nhigh level variables have a joint distribution, and are not independent which we can manipulate with language\ngraph that represents the joint is a sparse factor graph\n\neach of the factors in a factor graph corrrespond to a dependency between a group of variables\nit’s making a statement that links these 3 variables\n\neach statement as one possible sentence in natural language.\n\n\nWhat’s the connection?\n\ninference in a graphical model such as this allows us to exploit the sparsity of the graph and we sould visit the the nodes in this graph and it would require we only look at a few neighbors\n\nselecting these variables from a large set, at lteast the in\n\n\nThe brain is performing inference on this factor graph\n\nWhat causes changes in distributions\n\nthe changes in these distributions are about agents doing things and causing these shifts in the world\nactions are localized in space and time\n\nthese changes could be explained by just a few variables in the right model\n\nvideos – pixels at timesteps, we’re dead in the water\n\n\nconsequences of an intervention on few causes or mechanisms\n\nHow to factorize a joint distribution means uncovering this cause and effect structure\n\nBengio et al arxiv : 1901.10912\nWe can recover from a change in distribution faster\n\nDisentangling the causes:\n\nA meta-transwer objective for learning to disentangle causal mechanisms\n\nDoing inference on the Intervention\n\nLearning causal models from unknown interventions\n\nlearning small causal graphs, avoid exponential explosions of # of fraphs by parameterizing factorized distributions over graphs\n\nInference over intervention\n\n\n\n\n\nSlides\n\nThe human language network\n\nThis workshop, but more generally, what people call language seems way beyond what language actually is\n\nstable structure that spans across people and all brains\nA stronger response to sentences than lists of unconnected words\nWhy is the sentence the preferred stimulus\n\nstructure\n\nWhat features of linguistic simuli and what associated computations drive neural responses in the human language system?\nAbstract structure\n\ndomain general syntactic processnig\nsome argue about key hierarchical structure\nshare that all computations behind language processing are highly abstract\nOverlapping structures in numberical cognition and language and music and language\ndata suggest that this region of our brain is used as much when solving math as when looking at a blank screen\n\neffectively not at all\n\nSpacial and mentalization\nAll of these aspects don’t engage regions that handle language for us\nWhen processing computer langauges – we had people read snippets of code the critical condition is code comprehension\n\nunderstanding coding problems DID NOT elicit a response in the language network\n\nRules out this abstract syntactical structural processor\nPuts a damper on modeling language in a really abstract way\n\nMeaningful event / event comprehension\n\ncan this representation and activation be linked to sound or story, etc.\npresented participants with sentences and pictures of certain events and asked them to perform hard tasks\nsemantic conditions elicit a non-trivial amount of repsonse but still lower than the sentence condition\nLanguage regions may engage in processing non-verbal representations\n\nevidence of people with brain damage that language cortex isn’t required for abstract concept mangement, or some such\nwhat features are necessary and sufficient to elicit neural responses in face-selective cells / brain areas?\n\nMotivation - vision research\n\nSyntactic frame\n\nlanguage specific meaning-independent syntactic processing\nwhat is a syntactic frame?: word order, function words, functional morphology\nsentences elicit the strongest response, word list is 2 or more times lower\njabberwonky is even lower\na syntactic frame on it’s own elicits a low response in the language specific cortex\n\nSyntactic frame + meaning\n\nGrammatrical word order / word-order-based, prediction\ncombinatorially (semantic + syntactic) of words / composition\nReasons to facor composition as the core driver :\n\ncombinability of words in the nearby context seems to be a true universal property of our lingual systems\n\ncompositional\n\n\nDestroying word order while preserving local combinability\n\ncolors for no reason than to show manipulations\nmade local word swaps\n\n\nEstimating local combinabiliy:\n\nusing PMI - does a reasonable job of measuring how dependent two words are on eachother\nthis measures a little bit more bias toward semantics because it weights down certain words\nfMRI Results\n\none of many examples where I wasn’t predicting the results and the results blew my mind\nthe response doesn’t drop AT ALL with the swapping of the words\n\nhas been reproduced multiple times\n\nLocality\n\nIs it important?\nPrior reasons to suspect that it is:\n\nthe language network doesn’t care about structure above clause/sentence level;\nmost dependencies in natural language are local (Futrell et al 2015)\n\n\nDestroying word order and minimizing (local) combinabiity\n\nshuffling this causes a drop in the results that coincides with random word order list\n\n“Local coherence, global gibberish”\n\nhuman language system is all about local sentence coherence\nspan where humans can track is ~ 6 to 7 words\n6 words is the beginning of how much activation correlates given any length of input, it seems like it’s the lower bound\n\nTake home message\n\nLinguistic composition is the overall driver of the language system\n\n\n\n\n\n\n\n\n\n\nAutomatic adaptation in robotics –&gt; Learning\nPractical constraints –&gt; data efficiency\nModels are useful for data-efficient learning in robotics\n3 Models\n\nProbabilistic models\n\nfast RL\n\nHierarchical Models\nPhysically meaningful models\n\nencode real world constaints to help move learning along\n\n\nProbabilistic Models\n\nPILCO Framework :\n\nProbabilistic model for transitiion function\n\nsystem identification\n\nCompute a long term state evolution\nPolicy improvement\nApply controller\n\n\nWhy probabilistic models?\n\nSlide\n* we need to find functions that allow us to capture the uncertainty about the world\n    * How do we plan and make decisions using the output of a regression model?\n    * Instead of picking a single function, we can posit a distribution over all functions\n        * I'm much more robust to modeling any of those functions that are within the bounds of the function\n\nHierarchical Problems\n\nGeneralize knowledge from knwon tasks to new (related tasks)\nRe-use experience gathered so far to generalize learning to new dynamics\nSeparate global and task specific properties\nShared global parameters\nGP captures global properties of the dynamics\n\nlatent variables \\(h_{p}\\)\nVariational inference\n\nModified cart-pole\n\nmodify length and weight of the pendulum\n\n\nData efficiency and interpretability\n\ncan we use model sfrom physics to encode more structure into the problems\nStarting point is lagrangian mechanics\nLagrangian : encodes “type” of physics\n\nhelps us talk about symmetries and conservation laws\n\nHamilton’s principle\n\nLearn L instead of learning the dynamics directly\n\nEuler-Lagrange Equations\n\nHow do we discretize these things?\nNaively, the errors build up and it becomes completely unphysical\nVariational Integrators\n\npreserve the physics as they discretize the space\nmomentum preserving\nsymplectic\nbounded energy behavior\n\nDiscretize it in a way that preserves the physics\n\nVariational Integrator Networks Paper Slide\n\nWrite down the parameterized Lagrangian\nDerive explicit variational integrator\nDefine the network architecture\n\n“just a whole big network”"
  },
  {
    "objectID": "posts/neurips-2016/index.html",
    "href": "posts/neurips-2016/index.html",
    "title": "NeurIPS 2016",
    "section": "",
    "text": "This year’s NIPS conference had record attendance at over 6000 people! As opposed to when I attended last year in Montreal, this was an, almost, two fold increase. That said, hats off to the organizers and all of the staff for being able to double the size of a conference and still have it be a relatively smooth attendance experience.\nSpeaking with other attendees though, I think there was a general interest in structuring the conference a bit more differently than the way it had been. There was even mention of breaking the Deep Learning portion of the conference off into it’s own conference in and of itself. That might be a bit extreme, however there was, without a doubt, a healthy amount of sessions and talks that were based on deep learning.\nAll said and done, though, it was an awesome experience that has left me charged to keep learning and trying the amazing ideas that were presented and exchanged throughout the conference. With that, I thought I’d post a list of all of the sessions I’d attended and try to provide a quick summary of what intuitions I’d built about the presentations. Keep in mind these notes are made both from memory and from the scribbles I have in my notebook. Come to think of it, there is actually a second thing I would have preferred, if it were possible. Sessions tended to be in the range of 20-30 minutes, and that never seemed to be enough time for individuals to present on their problems and potential progresses they had made. A lot of the problems that are being framed up can be incredible technical and may require half to three quarters of the allotted presentation time. I don’t pretend to have a solution to this problem, but rather am just interested in providing feedback should anyone stumble on it.\nOne thing that I wish that I could figure out how to do, in a meaningful way, is to contribute to the greater research “good” that the Open Science ideology that the machine learning community follows, outside of either a large(r) ML shop that can “afford” to pay someone to be half-reseacher, and outside of direct academia. There is the new effort AI-ON.org – so maybe that’s the answer?"
  },
  {
    "objectID": "posts/neurips-2016/index.html#symposia",
    "href": "posts/neurips-2016/index.html#symposia",
    "title": "NeurIPS 2016",
    "section": "Symposia",
    "text": "Symposia\n\nRecurrent Neural Networks and Other Machines that Learn Algorithms\n\nNIPS Link\nSymposium Homepage\n\n\n\nDeep Learning Symposium\n\nNIPS Link\nSymposium Homepage\n\nLooking at this list after trying to recompile it from my notes, it’s both exhausting and intimidating to think about trying to rehash the workshops on top of everything else listed here. So I will keep this post to just the tutorials and oral talks for the time being. I will create another post that covers the workshops material, as well."
  },
  {
    "objectID": "posts/machine-learning-and-ai/index.html#definitions",
    "href": "posts/machine-learning-and-ai/index.html#definitions",
    "title": "Machine Learning and Artificial Intelligence",
    "section": "",
    "text": "I want to start this post with some definitions from some of the major literature in the field to normalize what is meant when using different words and phrases to describe the application of science to quantify and/or qualify some dataset.\nThe first term I will address is Machine Learning and I will decompose the phrase into its constituent components. Both machine, and learning.\n\n\nDepending on your choice of common English dictionaries the definition of the word machine can vary. We will utilize the Oxford Dictionary which defines the term machine as follows:\n\nAn apparatus using mechanical power and having several parts, each with a definite function and together performing a particular task.\n\nThe definition fits that of a modern computer. Most modern consumer computers follow the Von Neumann architecture in that there are disparate components working together to perform a particular task. Mainly a Central Processing Unit, and a Memory Unit that work in orchestra to take as input some value, and produce another value as output. This is the definition that I will apply to the word machine.\n\n\n\nFigure 1: Von Neumann Architecture Source\n\n\n\n\n\nWe will also appeal to the Oxford Dictionary for the definition of the term learning.\n\nThe acquisition of knowledge or skills through study, experience, or being taught.\n\nThis definition is in line with something that is intuitive but can sometimes become lost in the noise of the every day higher level interactions we’re having with the world around us. We are, in real time, learning on many levels of abstraction that are presented to us through out cognitive faculties. Consciously learning the fundamental theory of algebra is a seemingly active exercise in which we study material to learn the theorem and how to apply it. However separate from that conscious learning, we are also subconsciously learning from many inputs we are receiving. Suppose we decide to stop studying our material on algebra and instead decide that we want to go for a jog. As we begin our journey things are going smoothly and when we aren’t paying attention we stumble on an obstruction in the middle of the pathway. As we stumble our brain is subconsciously using many inputs from our sensors contained within our system called the human body to right our trajectory to ensure we don’t fall. Using a seemingly more concrete example would be a gymnast performing a routine that they have repeatedly practiced until they are satisfied. A similar analogy could be made for the increase and decrease in requirement of oxygen within the body as we are burning more energy throughout the process of exercise. As our heart rate increases our respiration rate increases to compensate for the increase in oxygen required for metabolism."
  },
  {
    "objectID": "posts/machine-learning-and-ai/index.html#machine-learning",
    "href": "posts/machine-learning-and-ai/index.html#machine-learning",
    "title": "Machine Learning and Artificial Intelligence",
    "section": "",
    "text": "I want to formalize the definition of machine learning in the way that the research and industry tend to apply them.\nI think about it in terms of Tom Mitchell’s definition, in his book Machine Learning, which offers a formal definition in the bounds by which science and engineering can work.\n\nA computer program is said to learn from experience E with respect to some class of task T and performance measure P, if its performance measure P at task T as measured by P, improves with experience.\n\nThis definition also captures the fundamental areas of research and industry application that exist within machine learning.\nAbove, when referencing emulation of tasks that are normally performed by other systems that exist in the world, I mean that these systems can be a product of biological or human engineering efforts. There are many tasks that biological systems, such as humans and animals, perform which can be emulated using this process called machine learning. Actions that we perform such as recognizing objects around us, to understanding the approximate trajectory of an object, such as a ball, and being able to intercept that ball to catch it. Along with other processes such as understanding what someone is saying through the combination of speech and gesture recognition while communicating.\nThere are also other, external to the human psyche, tasks that we can use machine learning for as well. We may lack the biological sensors to measure many of the systems around us but we can build sensors for these systems and then use machine learning to have computers “learn” about these systems. Systems like the weather, astronomical objects, search engines, etc."
  },
  {
    "objectID": "posts/machine-learning-and-ai/index.html#artificial-intelligence",
    "href": "posts/machine-learning-and-ai/index.html#artificial-intelligence",
    "title": "Machine Learning and Artificial Intelligence",
    "section": "",
    "text": "Now I want to address the term artificial intelligence in the same way that I had addressed machine learning. By decomposing it into its constituent words, defining them, and attempting to define what the combination of the two terms, means.\n\n\nThe term artificial is defined by the Oxford Dictionary as follows:\n\nMade or produced by human beings rather than occurring naturally, especially as a copy of something natural.\n\nThis term is a bit more broad in its definition in that many of the objects we see around us are an artificial representation of something naturally occurring. Things like artificial flowers, or artificial limbs that enable individuals perform tasks that might otherwise not be possible. The key interpretation here is the fact that it is the creation of something that seemingly isn’t naturally occurring. Notwithstanding the logical argument that could be made that there is some hierarchical interpretation to the idea of something being created by something that was created naturally, humans, ergo whatever was created could be interpreted as something naturally occurring.\n\n\n\nNow we come to a seemingly ill-defined term that exist in the world of science and engineering. That of intelligence. When we look at the Oxford definition of the word we can see just how general the interpretation is:\n\nThe ability to acquire and apply knowledge and skills.\n\nThe ability to apply knowledge and skills is interestingly broad in its definition. As with the logical argument that could be made above about what really is artificial and what isn’t, a similar logical argument could be made in that if we create a machine that embodies some sort of skills or abilities that we as humans have acquired, are we creating an intelligent system? Or is the system itself required to acquire the referenced knowledge and skills. We might have to go one rabbit hole deeper in order to make this definition a bit more concrete.\nKnowledge is defined as:\n\nFacts, information, and skills acquired through experience or education; the theoretical or practical understanding of a subject.\n\nYet again we’re up against the wall with some of the lingual abstractions present in this definition. Facts, information, skills, experience, education, etc. all contain some sort of implicit definitions in which we tend to take for granted. Rather than looking into the definitions and etymologies of words used to describe a phenomenon that we are observing, we can appeal to a more rigorous definition of what intelligence may be. Legg and Hutter (2007) provide a working definition of machine intelligence.\nLegg and Hutter start with an analysis of 70 or so definitions of intelligence from different areas of academia including researchers in artificial intelligence and psychologists. There are a few salient definitions from both backgrounds that I would like to reference.\n\n“Intelligence is a general factor that runs through all types of performance.” A. Jensen\n\nI quite like this definition because it affords us a general interpretation in that many of the systems that we build, deploy, and label as intelligent can logically satisfy this definition. All systems have performance measures used to justify whether or not that system is able to perform better than previously, due to some mechanism contributing to the idea. There are also others that are seemingly more philosophically intriguing as well:\n\n“The capacity for knowledge, and knowledge possessed.” V. A. C. Henmon\n\nThis eludes to the idea that there is some form of consciousness that needs to exist, a self awareness of ones own capacity for knowledge. This is less concrete in the way of mathematical definition, but I do enjoy at least entertaining the idea if for nothing more than thought exercise.\nLegg and Hutter (2007) distill these definitions down to something more general as their definition seems to capture many of the special case interpretations of the 70 odd quotes:\n\n“Intelligence measures an agent’s ability to achieve goals in a wide range of environments.” S. Legg and M. Hutter\n\nAs they note in their paper this definition has implicit in it many of the abilities an agent should have to define it as intelligent, abilities such as learning, adapting, and understanding."
  },
  {
    "objectID": "posts/machine-learning-and-ai/index.html#artificial-intelligence-1",
    "href": "posts/machine-learning-and-ai/index.html#artificial-intelligence-1",
    "title": "Machine Learning and Artificial Intelligence",
    "section": "",
    "text": "Now that we’ve defined artificial and intelligence, we can define what the two words mean together. It is an agent that doesn’t occur naturally that can some how achieve goals in a wide range of environments.\nThere is a more formal definition as is defined in Legg and Hutter (2007). That is left to the reader for investigation. For now we will leverage just the general linguistic definition of the term intelligence.\nThere are what seem to be direct lines that can be drawn between artificial intelligence and reinforcement learning in that both definitions and the latter’s frameworks encompass the process of training an agent on a given environment to improve its performance over time to achieve whatever goals have been defined, and depending on the area of research there is also the research into the transferability of these agents between many different environments. Whether it be an already trained agent being exposed to a new environment or whether a particular methodology is applicable to more than one environment."
  },
  {
    "objectID": "posts/machine-learning-and-ai/index.html#normalizing-nomenclature",
    "href": "posts/machine-learning-and-ai/index.html#normalizing-nomenclature",
    "title": "Machine Learning and Artificial Intelligence",
    "section": "",
    "text": "This leads me back to the reason for this writing. It is an attempt to normalize the nomenclature that we as an industry use when addressing the application of these technologies to problem spaces. There are many ways that these terms can become muddied and conflated and I want to ensure we’re all speaking the same language as we make the efforts to apply these technologies in new and interesting ways.\n\n\nI also want to provide a concrete example of where these definitions can be used in our specific problem space of technology systems. Depending on the scale at which we are analyzing a given system, one could be analyzing a single computational device that is part of a larger cluster of computational devices that are meant to distribute computational operations or storing state in a persistent manner. In respective parlance, distributed systems and databases.\nWhen reasoning about the application of machine learning to systems such as these, there are many aspects of the system that we can attempt to model using methodologies that fall firmly in the machine learning definition. A relatively simple example would be the application of some form of novelty detection with respect to the operation of the system. When collecting sensor data at times when the system is considered in steady-state operation, or nothing is currently wrong with the system, we can use novelty detection techniques to model either the data that has been collected itself, or the data generating distribution that we assume our data set has been produced from. Commonly referred to methodologies used to perform this are the application of autoencoders which can learn to reconstruct an input given some compressed representation of that input, or something like a variational autoencoder which attempts to model the parameters of the data generating distribution that produced our dataset that we’re analyzing.\nWhere we can cross the line into the area of artificial intelligence is when we start to use models to affect change on the system that we are reasoning about. When we think about this from a particular perspective of infrastructure operations, it would be the assumption that a system that is artificially intelligent would be able to modify the configuration of a given distributed system to improve the operation of that system. This definition is more in line with that of reinforcement learning, that I haven’t covered in this post. This will be covered in later posts.\nThis may become part of a multi-post series in an effort to combat the “buzzwordyness”, for lack of a better term, of the industry side of the applications of these methodologies, and I will update posts accordingly.\nMore to come…"
  },
  {
    "objectID": "posts/hashing-in-python/index.html#hashing",
    "href": "posts/hashing-in-python/index.html#hashing",
    "title": "Hashing in Python",
    "section": "",
    "text": "Hashing can be useful in speeding up the search process for a specific item that is part of a larger collection of items. Depending on the implementation of the hashing algorithm, this can turn the computational complexity of our search algorithm from \\(O(n)\\) to \\(O(1)\\). We do this by building a specific data structure, which we’ll dive into next.\n\n\nA hash table is a collection of items, stored in such a way as to make it easier to find them later. The table consists of slots that hold items and are named by a specific integer value, starting with 0.\nExample of a hash table (sorry for the poor formatting because markdown :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n\nEach entry in this hash table, is currently set to a value of None.\nA hash function is used when mapping values into the slots available within a Hash table. The hash function typically takes, as input, an item from a collection, and will return an integer in the range of slot names, between \\(0\\) and \\(m-1\\). There are many different hash functions, but the first we can discuss is the “remainder method” hash function.\n\n\n\nThe remainder hash function takes an item from a collection, divides it by the table size, returning the remainder of it’s hash value. Typically modulo arithmetic is present in some form for all hash functions, as the result must be in the range of the total number of slots within the table.\nAssuming we have a set of integer items \\(\\{25,54,34,67,75,21,77,31\\}\\), we can use our hash function to find slots for our values, accordingly.\nitems = [25,54,34,67,75,21,77,31]\n\ndef hash(item_list, table_size):\n    hash_table = dict([(i,None) for i,x in enumerate(range(table_size))])\n    for item in item_list:\n        i = item % table_size\n        print(\"The hash for %s is %s\" % (item, i))\n        hash_table[i] = item\n    \n    return hash_table\n\n# Execute the hash function\n# Create table with 11 entries to match example above\nhash_table = hash(items, 11)\n\n# Print the resulting hash table\nprint(hash_table)\nThe hash for 25 is 3\nThe hash for 54 is 10\nThe hash for 34 is 1\nThe hash for 67 is 1\nThe hash for 75 is 9\nThe hash for 21 is 10\nThe hash for 77 is 0\nThe hash for 31 is 9\n{0: 77, 1: 67, 2: None, 3: 25, 4: None, 5: None, 6: None, 7: None, 8: None, 9: 31, 10: 21}\nOnce the hash values have been computed, we inset each item into the hash table at the designated position(s). We can now see that there are entries with corresponding hash values stored in a python dictionary. This is obviously a very simple implementation of a hash table.\nThere is something interesting to note here, though, when working through using a simple hashing algorithm like the remainder method. We have items, in our case integers, which hash to the same value. Specifically, we can see that there are 2 items that hash to each of the 1, 9, and 10 slots. These are what are known as collisions.\nClearly these collisions can cause problems, as out of the 8 initial items that we’d started with, we only have 5 items actually stored in our hash table. This leads us into the next section we’ll discuss, and that is hash functions that can help alleviate this collision problem.\n\n\n\nHash functions that map, perfectly, every item into it’s own unique slot in a hash table is known as a perfect hash function. If we knew the collection of items and that it would never change, it’s possible to construct a perfect hash function specific to this collection, but we know that the dynamics of the real world tend to not allow something so simple.\nDynamically growing the hash table size so each possible item in the item range can be accomodated is one way to construct a perfect hash function. This guarantees each item will have it’s own slot. But this isn’t feasible, as something as simple as tracking social security numbers would require over one billion slots within the hash table. And if we’re only tracking a small subset of the full set of social security numbers, this would become horribly inefficient with respect to hardware resources available within the machine our code is running on.\nWith the goal of constructing a hash function that will minimize the number of collisions, has low computational complexity, and evenly distributes our items within the hash table, we can take a look at some common ways to extend this remainder method.\n\n\nThe folding method for hashing an item begins by diving the item into equal size pieces (though the last piece may not be of equal size to the rest). These pieces are then added together to create the resulting hash value. A good example of this is a phone number,such as 456-555-1234. We can break each pair of integers up into groups of 2, add them up, and use that resulting value as an input to our hashing function.\ndef stringify(item_list):\n    \"\"\"\n    Method to convert integer values into array of component integers\n    \"\"\"\n    string_items = []\n    while len(item_list) &gt; 0:\n        for item in item_list:\n            chars = [int(c) for c in str(item)]\n        item_list.remove(item)\n        string_items.append(chars)\n    return string_items\n\ndef folding_hash(item_list):\n    '''\n    Quick hack at a folding hash algorithm\n    '''\n    hashes = []\n    while len(item_list) &gt; 0:\n        hash_val = 0\n        for item in item_list:\n            while len(item) &gt; 1:\n                str_1 = str(item[0])\n                str_2 = str(item[1])\n                str_concat = str_1 + str_2\n                bifold = int(str_concat)\n                hash_val += bifold\n                item.pop(0)\n                item.pop(0)\n            else:\n                if len(item) &gt; 0:\n                    hash_val += item[0]\n                else:\n                    pass\n            hashes.append(hash_val)\n        return hashes\n\n# Example phone numbers\nphone_number = [4565551234, 4565557714, 9871542544, 4365554601]\n\n# String/Character-fy the phone numbers\nstr_pn = stringify(phone_number)\n\n# Hash the phone numbers\nfolded_hash = folding_hash(str_pn)\n\n# Input values into hash table\nfolding_hash_table = hash(folded_hash, 11)\n\n# Print the results\nprint(folding_hash_table)\nThe hash for 210 is 1\nThe hash for 502 is 7\nThe hash for 758 is 10\nThe hash for 969 is 1\n{0: None, 1: 969, 2: None, 3: None, 4: None, 5: None, 6: None, 7: 502, 8: None, 9: None, 10: 758}\n\n\n\nWhen dealing with strings, we can use the ordinal values of the constituent characters of a given word, to create a hash.\nIt’s important to notice, however, that anagrams can produce hash collisions, as shown below.\ndef ord_hash(string, table_size):\n    hash_val = 0\n    for position in range(len(string)):\n        hash_val = hash_val + ord(string[position])\n        \n    return hash_val % table_size\n\nprint(ord_hash(\"cat\", 11))\nprint(ord_hash(\"tac\", 11))\n4\n4\n\n\n\nIn the case above, just using ordinal values can cause hash collisions. We can actually use the positional structure of the word to as a set of weights for generating a given hash. As seen below.\nA simple multiplication by the positional value of each character will cause anagrams to evaluate to different hash values.\ndef weighted_ord_hash(string, table_size):\n    hash_val = 0\n    for position in range(len(string)):\n        hash_val = hash_val + (ord(string[position]) * position)\n    return hash_val % table_size\n\n# ord_hash\nprint(ord_hash(\"cat\", 11))\n\n# weighted_ord_hash\nprint(weighted_ord_hash(\"tac\", 11))\n4\n9"
  },
  {
    "objectID": "posts/neurips-2019/index.html#quantum-black-workshop",
    "href": "posts/neurips-2019/index.html#quantum-black-workshop",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "The presenter covers the analysis of different Q-learning approaches in the context of their presentation.\nThey also cover the importance of evaluating a learned policy and how that can be done.\n\nWe need to understand how well out optimcal policy is able to learn an optimal set of decisions\nWe need to propose a set of actions that is realistic for the organization to implement\nWe need to interpret out how our optimcal policy differs from the data generating behavior policy.\n\nThese questions that we need to ask, outlined above, help us to evaluate the effectiveness of our approaches in the real world.\n\n\nDefn : Evaluating a learned policy through the simulation of the environment\nConsiderations :\n\n\n\nAre we even able to simluate the dynamics of the environment?\n\n\n\n\n\n\nMore easily applied to historical data (we don’t have access to a simluation environment)\nImportant sampling can be used to estimate the expected reward under a new policy\nRequires estimates of the behavior policy which is usually estimated by separate supervised models\n\nThis might be the only way forward as we don’t have the capability to simulate the dynamics of the envoironment.\n\n\n\nGoal of PS : estimate the value of a new policy\nTake the ratio of the optimal bahavoior policy at a given time point and * reward at the time step / number of data points – this generates an estimator for the expected value of importance sampling\nSlide\n\n\n\nHas high unbounded variance\nweighted importance sampling has lower variance, but is a biased estimator (because of the weighting)\nVariance is much higher when our policy drastically differs from true behavior\nVariance increases with the length of the episode\nRequires accurate estimate of the behavior policy\nNewer evaluation methods (MAGIC) use both important sampling and model-based approaches depending on the sparsity if your episode\n\nIn summary, as time progresses the variance of the policy will increase, this seems intuitively correct\nPaper : Evaluating Reinforcement Learning Algorithms in an Oberservational Health Setting\n\n\n\n\n\nCertain domains may Require offine evaluation due to ethical or data issues\nOPE may seem relevant in scenarios where a simulator isn’t available, but high variance makes estimating the value of the optimal policy extremely difficult\nInvestimg time to build a model of the environment is important"
  },
  {
    "objectID": "posts/neurips-2019/index.html#private-federated-learning",
    "href": "posts/neurips-2019/index.html#private-federated-learning",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Privacy Principles at Apple for ML\n\nData Minimization\n\nCollect only what we need\n\nOn Device Intelligence\n\nProcess data local to devices - this prevents the uneeded transport that can allow for eyes that shouldn’t see it\n\nTransparency and Control\n\nAllow for ‘opt-in’\n\nSecurity\n\nThe foundation for all of this\n\nThreat Model\n\nMalicious Adversarys\n\nPerform arbitrary inferences on the learned model to extract data points\n\nCurious onlooker\n\nPassively looking at updates\n\n\nCentral Differential Privacy with small privacy parameters (Epsilon &lt;= 2)\n\nmoment accounting\n\nLocal Differential Privacy\n\nlocal pertubation on the device modifies the vector into a new vector \\[ z = M(w) \\]\n\nControl - allows users to opt into this feature within device settings\n\nExpose the information that is being sent to apple\n\nActual parameters, and many other data points\n\n\n\nRetention * Keep as little data as possible for as short a time as possible * If user deletes a data source, immediately remove the corresponding training data, as well."
  },
  {
    "objectID": "posts/neurips-2019/index.html#crypten-privacy-preserving-machine-learning",
    "href": "posts/neurips-2019/index.html#crypten-privacy-preserving-machine-learning",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "The question being asked “Can data be encypted and still be used to train models”\n\n\n\nHomomorphic encryption\n\nData encrypted localled on some device and transmitted back to some central repo\nPerform some function wrt to data and the function itself can or cannot be encrypted as well\n\nSecure Multiparty Computation\n\nFederal Ownership of data\n\nMultiple parties involved in the encryption scheme\nWe can then evalute functions\n\n\ntrusted execution environments\n\nMuch like Enclaves from Intel – thought these were proven to be insecure with the meltdown and spectre attacks\nattestation - we can attest to the fact that only the desired function was executred\n\nDifferential Privacy\n\nExecute some function on some data that has had noise added to it\n\nWe have to formally prove that the noise we’ve added still provides some guarantees through the distributional definition?\n\nNeed to understand this more.\n\n\n\nFederated Learning\n\nAdditive noise that has interesting properties described in Secure Aggregation (see Secure Aggregation Protocol Paper)\n\nHow and why does this matter to PyTorch\n\nEnd state of this would be to have a flag in an API where (privacy=True)\nFar from that.\nBuilt a framework called CrypTen URL\n\nCrypTen Design Principles\n\nEager Execution\n\nEasier troubleshooting and learning curve\n\nRealistic\n\nsome other OSS projects assumed 2 parties, they wanted to head toward N parties\n\n\nEncryption by sharing\n\nMultiparty computation\n\nMultiple parties process “part” of the data\n\nimages divded between parties would be done pixel by pixel and this might be uninteresting to any single participant\nNo parties can reconstruct the actual data without collaboration from all parties\n\n\nAssume we have an integer representing some information : \\[x \\in \\{0,..,N-1\\}\\]\nChoose some random mask in the same interval : r ~\nEncrypt by subtracting the mask that we’ve samples above\n(x-r) becomes independent btween participant clients\nDecryption in this domain is easy, just add all of the shares\n\njust need agreement from all of the participant parties\nWe can design this “agreement” in many different ways\n\n\nEncryption is homomorphic\n\nadding a public value : \\[ [x] + y = (x^(0) + y) \\]\nMultiplication needs triples of encrypted random numbers with the property that \\[ [a][b]=[c] \\]\n\nonce we have these tiples we can then generated a share for \\[a\\], \\[b\\], \\[c\\]\n\nthese tiples are sent the participant parties who then calculate epsilon and deltas\ncontain no information because they’re all encrypted\n“open epsilon and delta” - leak no information because they’re substracttions from a random number\n\n\n\nOperations for Neural Nets\n\nConbvolutions are additions and multiplies\ndivisions are approximated\nnon-linearities that involve exponentation are\nrelu break down into sharing ‘bits’\n\nHigh level architecture slide\nFeature List slide\nHas ONNX integrations"
  },
  {
    "objectID": "posts/neurips-2019/index.html#bayesian-deep-learning",
    "href": "posts/neurips-2019/index.html#bayesian-deep-learning",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "URL Recording\n\nGradient Descent with Bayes\n\nClaim is that we can derive this by choosing Gaussian with fixed covariance\nGlobal to local approximation\n\nNewton’s Method from Bayes\n\nIn this case we choose a multivariate Gaussian SlidePaper\nWe can express in terms of gradient of Hessian of the loss\n\nWe can use this expectation parameter and ask questions about higher order information about the loss surface\nIf we’re not sure about a second order method, then principles say that we shouldn’t maybe use these methods\n\n2 parts\n\nchoose the approximation\nsecond is choosing which order of parameters we might want to use\n\n\nRMS/Adam from Bayes Slide Paper\n\nThis has taken the community a long time to figure out but we can see that we can draw lines between the bayeslian learning rule for multivariate Gaussian and RMSprop\n\nSummary\n\nIf we add momentum to the bayesian objective a lot of these things can be explained using similar prinsiples\n\nBayes as optimization Slide Papers\n\nWhat we can do to derive the bayes rule in this special case\n\nWe define the loss to be the negative log joint (estimate)\nWe can plug this into an objective function over all distributions\nWith no restriction we should arrive at the posterior distribution\nEntropy is the negative expected value\nThe expectation of the log ratio of q over e^-l becomes 0 (as log(1) = 0)\n\n\nBayes with Approximate Posterior\n\nTrying to make a point that using this learning rule, how do we optimize it?\n\nOptimizing it in the right way allows us to do much more than variational inference, including exact bayesian inference\n\nBayesian Principle, rather than variational principle\n\nConjugate Bayesian Inference from Bayesian Principles Slide Paper\n\nComputing these messages in forward backward algorithm, we’re finding the expectation of the gradient\nWe can write this loss as two parts\n\nloss of the joint\ndepends on the data (conjugate really means depends on the data)\n\nChoose a q to match the sufficient statistics\nWe can write this as a combination of a learning term and a quadratic term\nCompute the expectation fo the loss, we can see that it’s linear in the expectation parameter\nThe expectation of the loss is linear in the expectation parameter\n\nNeed this to compute the squares (see slide)\n\nThis is a generalization that applies to things like forward backward, SVI, Variational message passing, etc.\n\nThis is all proved in the paper link above\n\n\nLaplace Appriximation Slide\n\nRun the newton method and eventually it will converge to the laplacian\n\n\n\n\n\n\n\nUncertainty Estimation for Image Segmentation Slide\n\nWe can see missing pieces of sidewalk, etc. this shows wher\n\nSome Bayesian Deep Learning Methods Slide\n\nOne line of work proved popular and just keep running standard DL, and then use some ideas to dropout some weights, and doing this it somehow corresponds to solving this bayesian problem (see paper)\n\nPros : Scales well to large problems\nCons : Not flexible\n\nPoint is to get the average with the goal of “how do we choose this average”\n\nGet this model, and perturb and this allows to add some noise and allow us to explore a bit\n\nThe principle of SGD corresponds to a Gaussian with parameters that we cannot really control\nWe can use any parameterization we want\n\nScaling up VI to ImageNet Slide\n\nTaking a sample of the gradients and this helps us to scale it to ImageNet\n\nVariation Online Gauss-Newton Slide\n\nImprove RMSprop with Bayesian Touch\n\nRemove the “local” approximation of the Expectation\nAdd some second order approximation\nNo square root of the scale\n\nTakes some more computation but it’s worth it in that we’re estimation varaince of the diagonal gaussian then we might want to make that tradeoff\nEstimating a diagonal gaussian with some variance around it, and the variance scaled\nWe can borrow a lot of tricks from the DL side of the world through the framing of the problem we covered previously\n\nBDL Methods do not really know that they are performing badly under dataset shift\n\nThis is telling us about uncertainty and about performance\n\nWe are shifting the data slowly and we can see that accuracy goes down\nIf we’re estimating uncertainty it should be reflected in our calibration\n\n\nResources for Uncertainty in DL Slide\nChallenges in Uncertainty Estimation\n\nWe can’t just take bayesian principles and apply them to non-convex problems\ndifferent local minima correcpond to various solutions\n\nlocal approximations only capture “local uncertainty” – in the same way that DL only captures a local solution to the functional defn\nThese methods miss a whole lot of the data space\nThis is a very hard problem\n\nMore flexible approximations really tells us that we need to go beyond second order optimization\n\nFundamentally there are tools that are missing for us to do this\n\n\n\n\n\n\n\nWhich examples are more important for a classifier given Slide\n\nDoes our model really “know’ this?\nDoes the model understand why it is the way that it is?\n\nModel View vs Data View Slide\n\nBayes automatically defines data-importance\nPoints closer to the boundary are more “important”\nThe data view tells us what makes the model certain\n\nDNN to Gaussian Processes Slide\n\nTrained with just a deterministic method (Adam, etc)\nCan we warp that line to get a distribution?\n\nGet a gaussian approximation of this red line and it turns out that the GP are posteriors of linear models\n\nposterior is equal to the posterior approximation\n\nFind a basis function where this linear approximation and we can convert it to a Gaussian Process\n\nThese things seem to a dual of eachother\n\n“Global” to “Local” Slide\n\nThis posterior approximations connect “global” parameters (model weights) to “local” parameters (data examples)\nWhen we use gaussian approximation, we approximate this loss function\nLocal parameters can be seen as “dual” variables that define the “importance” of the data.\n\nContinual Learning Slide\n\nWe’re not seeing part of the data and when we do this with NN’s we show that if we do this in the naive way then we start forgetting the past\nThere is no mechanism to remember the past, this global thing that I want to remember what I did classify and what mistakes I’d made in the past\n\nContinrual Learning with Bayes Slide\n\nRemembers almost everything that happened\nComputing this posterior is challenging, so we can use posterior approximations\n\nSome regularization-based continual learning methods Slide\nFunctional Regularization of Memorable Past (FROMP)\n\nIdentify, memorize, and regularize the past using Laplace approximation (similar to EWC)\n\n\nChallenges in Continual Learning Slide\nTowards Life Long Learning Slide"
  },
  {
    "objectID": "posts/neurips-2019/index.html#interpretable-comparison-of-distributions-and-models",
    "href": "posts/neurips-2019/index.html#interpretable-comparison-of-distributions-and-models",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Divergence Measures\n\nAre P and Q the same?\n\nWe can measure the difference or the ratio of probabilities P - Q or P/Q\nDivergence measure measuing difference will\n\nIntegral Probability Mertrics\n\nIPM are looking for a function that is well behaved , meaning smooth\n\ndiffernce in distributional expectations\n\n\nThe MMD: integral probability trick Slide\n\nMaximimze the mean discrepancy of the distributions\n\nsmooth function for P vs Q\n\nAssuming we can axpress our algorothm using dot products, we can take advantage of the analytical form for solving\nInfinitely many features that allow us to tell what is different when we use MMD\n\nFeature dictionary allows me to distinguish P and Q, no matter the difference between them\n\nExpectations of functions are linear combinations of eprected features\n\nTurns out the expectation of F is a fot product of F and X\n\n\n\n\n\n\nHow does the Wassterstein 1 behave?\n\nUsing a wasserstein-1 function that is lipschitz defined\n\nPhi divergences Slides\n\nTaking the ratio of the expectations of the densities\nTaking the reverse KL\n\n\n\n\nSlides\n\nCIFAR 10\n\nWeird conclusions in the reults of this paper\nIn the testing of CIFAR 10 - given these distributions how can we measure if they’re the same?\nRemember that MMD(P,Q) =\n\nEstimating the MMD Slide 1 Slide 2\n\nDiffernce between the mean embeddings (difference between these latent representations)\nExpected features of the same size\nDifferences in the mean of the distributions and across the distributions\nTake an empirical average and we can measure this\nWith this discrepancy, is this MMD true? Small numner “0.09” is small, but not 0.\n\nBehavior of the MMD Slide\n\nP,Q laplace with difference variances in y\nsamples frawn iid from P and Q\nIf we keep drawing on P and Q, we can see that this looks a lot like a normal distribution\nAsymptotics of the MMD are a normal distribution\nCentral limit theorem results hold\nAsymptotically normal with a mean at tthe TRUE MMD, and variance sigma^2 of the MMD\n\nvariance decays asymptotically\n\nWhat about when P and Q are the same? Slides\n\nturns out it’s an infinite sum of chi^2 distributions\n\nthis distribution depends on choice of kernel and what thr distribution of the data is\nWe do know that it converges to distribution of something\n\n\nA summary of asymptotics\n\n\nDistributions are close, they’re normal\n\n\nThe same and its this weird mixture of chi^2\n\n\nClassical statistical tests\n\nDistance is big, then we can say they’re not the same\nIf the estimate is less than a threshold then maybe they’re the same or we didn’t have enough data to capture the variance\nWe can take the MMD estimator and ask whether our estimator is bigger than a threshold CL Slide\n\nunder the known process, when P=Q, we want to reject the null at the rate most 0.05\nProbability of doing that, to be less than L\n\nWe can shuffle all examples together, which is a random mixture of dogs and fish Slide\n\nwe can estimate the distance between these new tilde’s and we can estimate what this actually means when P=Q\nWhat is the 1 - quantile, and that should be a good estimator\n\nGiven a kernel, we can now run a test\n\nChoosing a kernel, we can start with exponentiated quadratic\n\nKernel is characteristic no matter what bandwidth we pick\nAs we see infinitely many examples, all of the maxx escapes to the right\n\nProblem is, we never have infinite samples\nIn our example, bandwidth choice mastters A LOT\n\nIf we choose too smooth of a kernel then we get a witness function that can barely distinguish between the two distributions\n\nPower will be really low because of rejection bandwidth will be really high\n\nIn high dimensions, it doesn’t matter what bandwidth we pick because the bandwidth is based on pixel distance between images which breaks down in the curse of dimensionality\n\nOften helpful to use a relevant representation, by creating a new representation\n\nTake some hidden layer near the end of a classifier (reneralizes a little bit better)\n\nMeasure MMD between 2000 hidden dimensional representation from a classifer\nTurns out KID and FID use MMD and give way better properties\n\nInteresting that they use the semgentation mask as the pixel count (linear kernel of counts of pixels)\n\nThis seems super informative\n\n\n\nWhat about tests for other distances\n\nSometimes nice closed forms are useful\n\nChoosing the best test\n\nPicking a kernel that’s good for a particular model\nPower depends on the distributions P and Q (and n)\nCan maybe pick a good kernel manually for a given problem\n\n\nOptmizing MMD for test power Slide\n\nAs we see more and more data this will converge to Gaussian\n\nfor large n, the second term is negligible\n\nOur estimator is differentiable in kernel parameters\n\nData Splitting\n\nImportant we don’t trick ourselves and keep everything statistically bound\nWe need to be looking at the test error here\nWe split part of the data to learn the kernel, and the other part to test that kernel\n\nThis second part is exactly the standard testing framework covered above\nThis is a methodology notion\n\nLearning a kernel is very helpful Slide\n\nAlternative Approach\n\nWe can train a classifier to do something like this above\nWe split the data, train a classifier to distinguish X from Y and evaluate it on the other half of the data\n\nAccuracy 50% = can’t tell, accuracy = 100%, clearly different\n\n60%, the fact that we can classify at all tells us the distributions are different\n\n\n\nClassifier as two-sample test Slide\n\nalmost exactly equivalent\n0-1 kernel inflates invariance, decreases the test powr\n\nInterpretability Slides\n\nCan we distinguish two distributions\nBreak up each image into its component pixels and learn a kernel for each pixel\n\nUsing an ARD kernel\n\nWe can look at where the witness function “cares about the most”\n\nhistogram of witness function might overlap, as the means are close to eachother\nthe points that have the witness function interprets that it looks the most like a dataset\n\n\nMain references and further learning Slide"
  },
  {
    "objectID": "posts/neurips-2019/index.html#representation-learning-and-fairness",
    "href": "posts/neurips-2019/index.html#representation-learning-and-fairness",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides\n\nThis tutorial will outline how representation learning can be used to address fairness problems\nA Framework for Fair Representation Learning\n\nRepresentation as a fairness problem Slide\n\nCreating a data regulator Slide\n\nDetermines what the fairness criteria are\ndetermines data sources\naudits results\nWhen Training\n\nInteracting with all of the stakeholders to understand the fairness criteria\n\noutput is the fairness criteria\n\nDeterminining fairness critera\n\nAlgorithmic fairness\nDataset fairness\n\n\nExamples for how to do this\n\nPartition the dataset into space of disjoint cells such that similar individuals are in the same cell.\nIndividuals in the same cell should be treated similarly\n\nLipschitz continuity implies individual fairness\n\nGood news : One can achieve fairness through Lipshitz regularization.\nBad news : Data is non-Euclidean (eg. images, graphs, etc).\n\nStandard Euclidean distance metrics aren’t a good measure for this\n\nChallenge : Can we learn representations of the data such that the l_2 norm is a good metric to compare instances?\n\nGroup Fairness : Similar Classifier Statistics across groups Slides\n(im-)possibility result for group-fair classification\n\nClassifier statistics are not artbitrarily flexible\neg. Binary classification statistics have two degrees of freesom thus can match two independent statistics across groups\nBeyond binary classification, the degrees of freedom grows quadratically with number of classes\n\nGroup Fairness : Advantages and Challenges\n\nAdvantages\n\nFairly easy to compute and roughly scales with the number of samples\nOften easier to explain to policy-makeers (as in terms of population behavior)\nMore existing work, strategies already exist for representation learning Challenges:\ndata regulator must determine which classifier statistics to equalize.\nfairness of the representation depends on the quality of the fairness metric chosen by the regulator\nGroup fairness can lead to (more) violated individual fairness, e.g intersectionality can lead to fairness gerrymandering\n\n\n\n\n\nMetric Elicitation Slide\n\nDetermine the ideal evaluation metric by interacting with users, and experts\nQuery an oracle for this fairness metrics\n\n\nWhich statistics should be equalized across groups?\nCommonly used measures are straightforward functions of classifier performance statistics.\n\nData Producer\n\nComputes the fair representation given the data regulator criteria\n\nData User\n\nComputes ML model given sanitized data"
  },
  {
    "objectID": "posts/neurips-2019/index.html#keynote-1-how-to-know",
    "href": "posts/neurips-2019/index.html#keynote-1-how-to-know",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides\n\nHow can two people living in the same world come to two different conclusions?\n5 Things Everyone in ML Should know about\n\nHumans continuously form beliefs\n\nWe don’t set them and we’re done\nWe continuously update our beliefs\nEvery time we encounter an instance of a bowl, we update our beliefs about bowls\n\nCertainty diminishes interest\n\nWhat you think you know is what determines your curiosity\nPeople do not have an accurate model of their own uncertainty\nIf you think you know the answer, you won’t check, and if we present the right answer to the person they still reject it\n\nMight be why there is confirmation bias\n\n\nCertainty is feedback driven\n\nhigh level beliefs about concepts\n\nmost useful for the decision making points in our lives\n\nwe are sometimes certain when we shouldn’t be\n\n\nPeople learned a novel rule based concept\n\nboolean logic to determine daxxy-ness\n\nIn the beginning there is no concept, it could be a property of the system, or not\nEntropy of an idealized model has little to do with interest and learning\n\ninstead this certainty comes from feedback\n\nReasoning about the world\n\nFlat earthers – if they watch online videos that confirm this it might increase the chance of early adoption of this idea as truth\n\n\nLess feedback may encourage overconfidence\nHumans form beliefs quickly\n\nEarly evidence counts more than later evidence\n\nLeads to becoming certain and plays down our ability to update our beliefs\n\n\n\nThere is no such thing as a neutral tech platform\n\nThe order in which information is presented makes a huge difference in our understanding of the world\nThis reinforces some of the studies done around the 2016 elections\nChildren are consuming more online data and this is affecting them"
  },
  {
    "objectID": "posts/neurips-2019/index.html#veridical-data-science",
    "href": "posts/neurips-2019/index.html#veridical-data-science",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides\n\nVeridical - coinciding with reality\nPCS Framework for Data Science Paper\n\nPredictability (P) (From ML)\nComputability (C) (From ML)\nStability (S) (from statistics)\nBridges two of Breimann’s cultures\n\nPCS connects science and engineering\n\nPredictability and stability embed two scientific principles\n\nStability unifies and extends myriad works on perturbation analysis\n\nIt’s a minimum requirement for reproducibility, interpretability, etc.\nTests DSLC by shaking every part (I describe this as wiggling all parts of the system to see how it changes the output)\nThere is always something to follow up when building models\n\nNew users\nNew patients\nNew collaborators\n\n\nData Perturbations\n\nData modality choices\nSynthetic data\nData under different environments (invariance)\nDifferential privacy (DP)\nAdversarial attacks to deep learning algorithms\nData cleaning also falls into this data perturbation bucket\n\nModel/algorithm perturbations\n\nRobust statistics\nSemi-parametric\nLasso and Ridge\nModes of non-convex empirical minimization\n\nHuman decision is prevalent in DSCL\n\nWhich problem to work on\nWhich data sets to use\nHow to clean\nWhats plots\nWhat data perturbations, etc.\nWRITE THIS ALL DOWN (MODEL CARDS FOR MODEL REPORTING)\n\nReality correspondences &lt;- great description of what we do when we “model” something\nHow do we choose these perturbations?\n\nOne can never consider all perturbations\nA pledge to the stability principle in PCS would lead to null results if too many perturbations were considered\nPCS requires documentations on the appropriateness of all perturbations\n\nExpanding statistical inference under PCS\n\nModern goal of statistics is to provide one source of truth\n\nCritical examination of probabilistic statements in statistical inference\n\nViewing data as a realization of a random process is an ASSUMPTION unless randomization is explicit\n\nTHIS DATA COULD HAVE BEEN GENERATED NON-RANDOMLY\n\nWhen not, using an r.v. actually implicitly assumes “stability”\nUse “approximate” and “postulated” models\n\nInference beyond probabilistic models\n\nWe need to have a way to bring PDE models in with things like synthetic data\nProposed PCS framework\n\nProblem formulation - translate the domain question to be answered by a model/algorithm (or multiple of them and seek stability)\nPrediction Screening for reality check : filter models/algorithms based on prediction accuracy on held out test data\nTarget value perturbation distribution - Evaluate the target of interest across “appropriate” data and model pertubations\n\n\n\nMaking Random Forests more interpretable using stability"
  },
  {
    "objectID": "posts/neurips-2019/index.html#uniform-convergence-may-be-unabe-to-explain-generaliation-in-deep-learning",
    "href": "posts/neurips-2019/index.html#uniform-convergence-may-be-unabe-to-explain-generaliation-in-deep-learning",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides Poster\n\nTightest uniform convergence bound that eventually shows it is vacuous\nGiven a training set \\(S\\), algorithm \\(h\\) in \\(\\mathbb{H}\\), then [Sl]\nIn what setting do we show this bounds failure/\n\nSeparating an nested hyperspheres, with no hidden noise, and completely separable\nObserve that as we increase number of training data point, the loss follows as expected\nAs we change the label of the datapoints between the hyperspheres, we take the set of all data points and show that s’ is completely mis-classified even though it is a completely valid member of the training dataset.\n\nintuitively this can happen only if the boundary we have learned has “Skews” at each training point\n\nWhat this means is the learn decision boundary is quite complex\nThis complexity that even the most refined hypotehsis class is quite complex\nThis proves the bounds are vacuous\n\nThis overparameterzed deep network can\n\nLooking aheead it’s important to understand the complexities contained within the decision boundaries and derive new tools as a test case"
  },
  {
    "objectID": "posts/neurips-2019/index.html#on-exact-computation-with-an-infinitely-wide-neural-network-slides-poster",
    "href": "posts/neurips-2019/index.html#on-exact-computation-with-an-infinitely-wide-neural-network-slides-poster",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Neural Tangent Kernel’s (NTK’s)\nTheoretical contribution\n\nWhen width is sufficiently large (polynomial in number of data, depth and inverse of target accuracy) the predictor learned by applying gradient descent\n\nEmpirical contribution\n\nDynamic programming techniques for calculating NTK’s for CNN’s + efficient GPU implementations\nThere is still a gap between the performance of CNN’s and that of the NTK’s\n\nThis means that the success of deep learning cannot be fully explained by NTK’s\n\nFuture directions\n\nUnderstand neural net architectures and common techniquest from the lends of NTK’s\nCombine NTK with other techniques in kernel methods"
  },
  {
    "objectID": "posts/neurips-2019/index.html#generalization-bounds-of-stochastic-gradient-descent",
    "href": "posts/neurips-2019/index.html#generalization-bounds-of-stochastic-gradient-descent",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides Poster\n\nLearning large overparameterized DNN’s, the empirical observation don extremely wide networks shows that generalization error tends to vary\nDeep RELU networks are almost linear in terms of their parameters on small neighborhoods around random initialization\nApplicable to general loss functions\nGeneralization bounds for wide and DNN’s that do not increase in network width\nRandom feature model (NTRF) that naturally connects over-parameterized DNNs with NTK"
  },
  {
    "objectID": "posts/neurips-2019/index.html#efficient-and-accuract-estimation-of-lipschitz-constands-for-dnns",
    "href": "posts/neurips-2019/index.html#efficient-and-accuract-estimation-of-lipschitz-constands-for-dnns",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides Poster\n\nLipschitz constant means with 2 points X and Y, they’ll be close before and after being passed through the neural network\n\ngeneralization bounds and robust classification lean on this\nThis problem of computing Lipschitz constants is NP hard so we try to find tight bounds around this\nSay we have an accurate upper bound of a model\n\nWe can take a point f(x), and we can measure of mis-classification and input that back into the network\n\nWe can certify that if we perturb X in a small ball drawn around this delta, it odesn’t change the classification\nIf we can find this small lipschitz constanc we might be able to prove that this network has a form of robustness\n\nHOw do we do this?\n\nProduct of the norm of the matrices\n\nSimple methods like this give upper nounds to the lipschitz constant that are conservative. Can we do anything more accurate?\n\nWe cna frame up finding this Lipschitz constant as a non-convex optimization problem\n\nOver approximate the hidden layers via incremental qudratic constaints\n\nGive rise to a semi-definite program giving us this tight upper bound that we’re looking for\n\nWe can trade off scalability with accuracy of the upper bound\n\n\nHow does this bound we get compare to others?\n\nWe show that in general our bound is much tighter than other bounds\n\n\nAdversarial robustness\n\nHypothesis trained using adversarial optimizers\n\nEmperitically when we evaluate this lipschitz constant these networks have much lower\n\n\n\nAccurate and scalable way of calculating Lipschitz constants in Neural Networks"
  },
  {
    "objectID": "posts/neurips-2019/index.html#regularization-effect-of-a-large-initial-learning-rate",
    "href": "posts/neurips-2019/index.html#regularization-effect-of-a-large-initial-learning-rate",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides Poster\n\nLarge initial learning rates are crucial for generalization\nScaling back by a certain factor at certain epochs\n\nsmall learning rates early on lead to better train and test performance?\n\nLearning rate schedule changes the order of patterns in the data whcih influence the network\n\nclass signitures in the data that admit what the class is, but it will ignore other patterns in the data\n\nLarge learning rates initially learn easy patterns but hard-to-fit patterns after annealing\nNon-convexity is crucial because different learning rate schedules will find different solutions\nArtificially modify CIFAR 10 to exhibit specific pattern types\n\n20% are hard to generalize - because of variations in the image\nEasier to fit in the second set 20%, easy to generalize but hard to fit – this is by construction\nPath that imitates what the class is\n60% of examples overlay a patch on the image and the memorization of the patch early on shows this method fits early and doesn’t generalize well"
  },
  {
    "objectID": "posts/neurips-2019/index.html#data-dependent-sample-complexities",
    "href": "posts/neurips-2019/index.html#data-dependent-sample-complexities",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides Poster\n\nHow do we design principle regularizers for DNN’s\n\nCurrent technqiues are designed ad-hoc\n\nBatch-norm and dropout - we know they work, but noy why\n\nCan we prove a theoretically upper bound on the generalization and hope it improves performance\n\nBottle-neck prior\n\nmost priors only ocnsider the norms of weight matrices and because of this they get pessimistic bounds that are exponential in depth\n\nBounds that depends more on data dependent properties\n\nupperbounded by the weights and training data\ninformal theory is that this can be upper bounded by (see slides)\nJacobian norm isthe max norm of the jacobian of the model on the hidden layers\nmargin is the largest logit of the output, minus the second largest\n\nINterpretation of this bound is it measures the ’Lipschitzness” of the network around training examples\nNoise stability is small in practice with looser bounds (see slides)\nRegularize the bound\n\npenalize the square jacobian norm in the loss\nNormalzation layers such as batch norm and layer norm\n\nHelps in a lot of settings\n\n\nCheck the bounds correlate with the test error and we found that our bound correlates well with test error\nCOnclusions\n\nTighter data dependent properties\nBound will avoid the exponential dependency on th depth of the network and optimizing this bound helps to improve performance\nFollow up work : tigher bounds and empirical improvement over strong baselines"
  },
  {
    "objectID": "posts/neurips-2019/index.html#machine-learning-meets-single-cell-biology-insights-and-challenges",
    "href": "posts/neurips-2019/index.html#machine-learning-meets-single-cell-biology-insights-and-challenges",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides\n\nAddress something asked by DaVinci - How are we made?\n\nWe’re created from a single cell and it eentually creates every cell in our body.\nHow does this process happen?\n\nCells are like tiny computers, taking input and output through things like proliferation, differentiation, and activation\nALl cells have the same genetic code – 3 billion letters\n\nHow our genome is the instruction set for assembling the different cells\nTelling eachother how to behave\nWe have 100’s of cells\n\nSingle cell RNA-sequencing in droplet micro-fluids\n\nMeasures for every single gene and cell for what gene it is and what cell it came from\n\nThe data matric for one sample ~ 250 million\n\nGene by cell matrix that is rife with errors and artifacts from measurement\nWe only capture about 5% of the transcripts, or what the humans are expressing\n\nIn the field, zero-inflation has taken root\n\nIt’s wrong\n“Drop out” – this is uniform sampling and it sometimes leaves us to capture no gene or transcription gene\n\nevery sample is affected by this\nNo value is at it’s actual value\nThis should be modeled properly and not with 0 inflation\n\nHow do we handle all of this data\n\nWe like to visualize into 2 and 3 dimensions\nPCA failed this data type and we couldn’t visualize it very well\n\nFollowing a Keynote at NeurIPS, someone presented T-SNE and it seems to fit the data well and we get good cell level separation\nWhile we might have a matrix of 20000 genes x 100000 cells – T-SNE and UMAP seem to capture this non-linearity well\nWe have this manifold because cell phenotypes are highly regulated\n\nWe can see in 3D the nicely shaped non-convex shape\nSimilar shapes in families because few regulators drive cell shape\nLots of feedback loops and interactions between these genes, which limits and constricts the phenotypes a cell can be in\nStill have challenges in visualization\n\nBuild better vis for this data as it’s non-uniform and 5 orders of different density\n\nChallenge is to handle this data with such different densities, it trips up many of the approaches we have today\n\n\nCommon way to viz beyond 2 or 3 dimensions using nearest neighbor graphs\n\nWe connect a cell to cells nearest that cell\n\nThis is dependent on probability distributions which help to define this similarity metric\n\n\nThe idea is one we have this graph we can really retain the manifold and use things like geodesic differences\n\nEach tiny differnce in this graph can represent small differences between cells\n\nWe can do distances and walks in these grpahs that allow us to measure the distance between cells\n\n\nData is extreme structures and there are communities\n\nSocial media community detection approaches find cell states and cell densities that are captures as cliques in the graph\n\nNice thing is that these graphs are connected\n\nThey share connectivity which allows us to cpature cell type transitions\n\nThese transitions are very sparse relative to the cell type\n\n\nUsing 100 ro 200 examples over 10000 entities\n\nThis isn’t regular science and this works because biology has lots of structure and isn’t adversarial in that respect\n100000 or 1000000 cellshave awesome things – treating each cell as a computer we can assume that the mollecrular influences create statistical dependencies in the data\n\nOut of the box algorithm gets us a correct reconstruction with no prior information of TCell networks\nThisallows us to do disease regulatory networks and helps us to understand what is wrong in this specific cancer patient\n\nAsynchronous nature of the data\n\nAll of our immune cells are in our bone marrow and these cells are able to generate all varities of immune cells within our bodies\nAsynchrony enables the inference of temporal pgoression\nFrom a single time point we can capture all of the dynamics of the process\n\nPesudo-time\n\nReconstructing developement which allows us to reconstruct order grom a single time point\nTHis process is highly non-linear we can order cells by chronology and the assumption is cell phenotypes change gradually ` * Cannot be treated as absolute values as we know this data is incredinly noisey\n\nWanderlust\n\nWe are able to reconstruct accurate progressions and discover order and timing of key events along differentiation\nThis checkpoint of DNS recombinations inside of a cell, we wanted to understand if it were OK\n\npediatric cancer is caused by understanding this checkpoint\nThis wasn’t known until we could find this tiny new cell population and it’s novel regulation\n\n\nData is structured\n\nbifurcations through use of walks and waypooints\n\nthe direct route between two cells along the same path should be more direct than non-immediate connections\n\nThese waypoints help to resolve structure\n\nWe find these using spectral clustering\n\n\n\nMapping development\n\nWe want to order these cells on their manifold and understand how they bifurcate\nWhat decision making is going on and what is their possible future cell types and propensity to turn into these cell types\nPalantir : Building a Markiv Chain out of this graph allows us to find time ordering in our neighbor grpahs\n\nstrong assumption that development goes forward and not back\nbroken in processes such as cancer\n\nbuild a directed graph from this\n\nwe can look at the extrema states and we find ourselves with an absorbing markov chain\nThis allows us to compute the end states of all of our cells and we can roll out the fate for each cell\n\nentropy of these probabiities for all cells\n\nThe proof is in the pudding – applied to early mouse developemnt (endoderm (all internal organs are made of this))\nData organized nicely along these dimensions\n\ncells aligned along temporal orders\napproximal distral organizations\nThese organizations happen head to tail\nA smooth gut tube, even though we can’t see anything that accounts for this organization, we can see the primodal organs jutting out from this tube\n\nWe can transcriptionally see where cells are headed a full day before they progress in that way\n\nWe go into the early days of the first decisions of the cells\n\ncell can become one of many classes\n\n\nWe can take spatio-temporal maps of the mamaallian endoderm\n\nWe see when FGR1 and FGR2 are both high, they’ll be primitive endoderm\n\nVery high entropy Right before this deicsion is made and entropy drops immedaitely after\n\nanalysis shows that biologist saw that these cells are plastiq – they can change by jumping out of that area and into the emryonic layer and assume the nature of the other cells\n\nPlasticity was predicted computationally, and we were then able to verify empirically\n\n\nThe Human Cell Atlas\n\nCells in our body, relationships between then, and transitions that happen within the human cellular system\nMost of the data is still single cell genomics\n\nThis atlas will have single cell genomics and spacial information of these cells\n\nThis will require tons of computation\n\nglobal and open community that anyone can join\npublic data of 10 billion cell playground\n\nHuman cell atlas will serve as a healthy reference and ground truth for disease\n\nThe methods we have now don’t scale\nData harmonization\n\ndata from multiple samples that might be diseased\n\nour methods mistake disease for biological differnce\n\n\nFactor analysis for good gene programs\n\nHow this data factors betwen cells and genes\nSimply comparing disease to normal\n\n\n\nLatent sapces : Deep Learning in scRNA-seq\n\ncount basis projected into latent space\n\nData denoising and integration\nlow dimensional embeddings\n\nInterpretation of latent factors is still lacking\n\nOur goal is not to predict, but to understand\n\nOften the outlier is the most important\nmachine learning is all about the common mechanism and not the outlier, whereas biology wants to know those outliers\nKeep our eye on the goal in biology and understanding that something rate\nDendritic cells are rare\n\nThese cells split into different\n\nCell types aren’t necessarily clusters\n\nThough clusters still have their own version of structure to them\n\nThe more we zoom in, the more we find structure in this data and we see that meta-cells have real peaks in their density\n\nThese meta-cells are defined by different programs and different covariances\n\n\nAcute myioloid lukemia is accute cancer gone awry\n\nNormal immune cells seem to overlap\nThese are early projectors of cancer cells – before they go awry and crazy\nWe want to know what happens that normal &lt;&gt; breaks, and cancer forms\nWhen we look at classicial methods, these diseases don’t connect\n\nWe believe in covariation and find a manifold that is driven by covariation and not just normal distributions\nCovariation in much lower dimensional space is much more computationally efficient\nThe regulatory systems that go awry\n\nWhen we run these methods, we can see exactly where the cancer breaks off and becomes cancer\n\n\n\nResponse to therapy\n\nBone marrow transplant patients who relapse and understanding how that relapse happens\nUnderstand the immune populations that differ between them, and using these dynamics we can see cell populations that really follow and raise up as the tumor burden rises and falls\nThese are very tiny populations, so one has to be very careful in computation\n\nEpigenetic data\n\nWhat potential regulators that can be regulating these systems\nWe can build generative processes, and using these latent variables we can understand different properties of these biological systems\n\nWhat is the covariate nature\nWe can understand inter-variable influence\nWhat factors combine to what targets through their regulators\n\n\nMost cells in a tumor in solid tumors are not cancer\n\nimmune cells and supportive tissue make up 90%\nusing factor analysis we can see cancer highjacks early development of these processes\n\nusing a program that the embryo knows to metasticize a new organ in another part of the body\n\nunderstanding how they survive in the brain\nIdentify that all cancers, both breast and lung, have the same gene that created their ability to survive there\n\n\nCancer uses regenerative mechanisms for it’s evil deeds\n\n1 change in 6 billion base pairs can make it go different under injury\nThe reason that this is is because there is enormous cross talk and remodeling between the eputhelial systems and the cancer\n\nSpacial techniques are critically important\n\nRapid autopsy programs allow us to collect samples"
  },
  {
    "objectID": "posts/neurips-2019/index.html#test-of-time-award-dual-averaging-method-for-regularized-stochastic-and-online-optimization",
    "href": "posts/neurips-2019/index.html#test-of-time-award-dual-averaging-method-for-regularized-stochastic-and-online-optimization",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides\n\nStochastic optimization and empirical risk minimization\nWe want to minimize the empirical risk of a very large sample\n\nConvergence theory\n\nDepending on loss function’s convexity or strong convexity\n\nOnline Convex optimization\n\nHere we consider an online game where each player predicts t+1\n\nsuffers a loss \\(f_t(w_t)\\)\nloss measures total loss of a fixed \\(w\\) from hindsight\nvery similar to SGD\n\n\nCompressed sensing / sparse optimization\n\nLASSO\n\nminimize quadratic function with a constraint in the \\(l_1\\) norm\n\n\nProximal gradient methods\n\nAdding up of convex"
  },
  {
    "objectID": "posts/neurips-2019/index.html#causal-confusion-in-imitation-learning",
    "href": "posts/neurips-2019/index.html#causal-confusion-in-imitation-learning",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides\n\nImitation learning is a powerful method for learning complex behaviors\n\ndriving cars, flying drones, and grasp and pitch\nBehavioral cloning\nSupervised learning through observations of experts\nNot perfect\n\nExpert state and we roll out these states we get errors of the imitator acculator that show up in other parts of the state space\ndistributional shift that arises due to this causality\n\nDoes more information lead to better performance?\n\nWhat happens under distribution shift?\nTurns out that a given model learns to pay attention to the road and brakes when someone is in the road\n\nThese fail because the model cannot infer causality\nCan we predict the expert’s next action\n\nThis is the only cause\nThe expert ignore it and its nuisance variable\nEnd state variables\n\nIf we learn 1 imitator that watches the road\nWe can learn another imitator that wrongly treats both variables as a cause\n\nIN general if we have 2^N possible causal graphs\n\nExistence of causal confusion\n\nConsider 2 examples\n\nlearns actions through history and one that doesn’t\nvalidation score on held out data history plays a role in working well with history but in test it causes confusion\n\nHow do we demonstrate this\n\nWe add to the original state and use this information to create confounded states\nThis corresponds to having just the causal\nUse a VAE and treat the dimensions of the latent variables as potential causes\n\nUsing behavioral cloning on the original state we get expert like rewards\nOn confouded states, we do much worse indicating causal confusion\n\nWhat we need is to have a causal graph that indicates the random variables that the expertt pays attention to\nIn the first phease we learn from all possible causal graphs\n\nBinary vectors 1 - cause, 2 - nuisance\nrandomly sample a causal graph and mask out the nuisance part of the state\nwe concatenate the graph and feed it to into a NN and it predicts an action\n\nbehavioral cloning loss\n\n\nIn the second phase we infer the correct causal graph\n\nintervention changes th distribution of the state\nwe score all possible graphs on additional information\n\nMode 1 : query reward\nMode 2 :\n\n\nCollect trjectories as policies\nQuery expert on states\nPick graph most in agreement with experts\n\nDAGGer baseline performs significantly worse\n\nLearned graph visualization\n\nlearned causal graph can ignore nuisance variables\nMore information can hurt performance without this effort\nHow to scale this up to more complicated tasks"
  },
  {
    "objectID": "posts/neurips-2019/index.html#imitation-learning-from-observations-by-minimizing-inverse-dynamics-disagreement",
    "href": "posts/neurips-2019/index.html#imitation-learning-from-observations-by-minimizing-inverse-dynamics-disagreement",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides\n\nMDP formulation\nDivergence minimization perspective on inverse learning\n\nGAIL or AIRL\n\nKL and JS divergences could be used\nAdversarial training for divergence minimization"
  },
  {
    "objectID": "posts/neurips-2019/index.html#structure-prediction-approach-for-generalization-in-cooperative-multi-agent-rl",
    "href": "posts/neurips-2019/index.html#structure-prediction-approach-for-generalization-in-cooperative-multi-agent-rl",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "posts/neurips-2019/index.html#guided-metapolicy-policy-search",
    "href": "posts/neurips-2019/index.html#guided-metapolicy-policy-search",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides Paper"
  },
  {
    "objectID": "posts/neurips-2019/index.html#using-logarithmic-mapping-to-enable-lower-discount-factors-in-reinforcement-learning",
    "href": "posts/neurips-2019/index.html#using-logarithmic-mapping-to-enable-lower-discount-factors-in-reinforcement-learning",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Standard RL Setting\n\nDiscount factor in play here\n\nThis can be modeled as an MDP\nGoal : find an optimal policy to maximize reward which is some long term objective"
  },
  {
    "objectID": "posts/neurips-2019/index.html#weight-agnostic-neural-networks",
    "href": "posts/neurips-2019/index.html#weight-agnostic-neural-networks",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides Poster\n\nInnate abilities in animals\n\nwe’re beginning to understand that ML architectures seem to have innateness\nCNN’s are so well suited to image processing that they can do many tasks in that area\n\nHow far can we push this innateness idea\n\nTo what extent can NN architectures along encode solutions to tasks?\nDifferent kind of NAS\n\nWE’re looking for NAS’ that perform without any training at all\nJudged on 0-shot performance\nBecause of the large weight space\n\nUse a single value for our weight space\nJudge how well the network works by doing several roll out with different values\n\nCreate a population of minimal networks\n\nThese have inputs with no hidden nodes, connected to some outputs\n\nPerformance of the network is averaged over rollouts and then ranked\n\nvary the networks to create new populations and continue the process\n\nWe can vary in one of 3 ways\n\nInset node\nAdd hidden connection\nChange the activation function\n\nGauss\nReLU\nSigmoid\n\n\n\nTested on 3 RL tasks\n\ncart-pole\nbi-ped\ncar racing\n\nCompare these WAN found topologies\n\nwhen trained can reach SoTA\n\nRandomize the weights they don’t perform well\nShared weights produce pretty good behaviors\nIf we tune the weights, they perform the same kind of SoTA performance as general purpose networks\n\nAble to do this with much smaller networks, sometimes orders of magnitudes\n\nFor fun we tested this on MNIST\n\nHere we use a random weight – we get an expected accuracy of 82%, best single weight is 92% – little better than linear regression\n\nSearching for building blocks toward a differnt kind of neural architecture search\n\narchitectures that have innate biases and priors"
  },
  {
    "objectID": "posts/neurips-2019/index.html#social-intelligence",
    "href": "posts/neurips-2019/index.html#social-intelligence",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides\n\nWhat has changed since Rosenblat started playing with NN’s?\n\nCompute power\nData\nThis gave Dean the idea to found Google Brain\n\nEarly 2010’s\n\nEdge TPU - 2 watts and 4 TOPS\n\nMobile-net-v2 @ 400FPS\n\nCoral.ai prototyping boards\nRunning workloads locally is important, BUT NOT SUFFICIENT, tool for implementing and reasoning about privacy\nHave to be “smart” about what data is thrown off of a device using ML\nSanity wrt energy consumption and other natural resources\n\nmoving data is what costs energy\nonce the data is in a register and we want to operate over it, it’s relatively free\n\nmoving it is what costs energy\n\n\n\nHow do we make these giant distributed frameworks run efficiently, effectively, and privately?\n\nFederated learning allows centralized inference but localized training\n\nScale?\n\n100’s of millions of android phones have machine learning running on them\nScale story is more complex\n\ndata are both abundant and rare / precious\n\nwe don’t always have access to the data\n\ncompute is both massive and limited/precious\n\ndon’t effect UX\n\nPremium on quick covergence, ie/ &lt; 1 pass over the data\nFL both enabled ML fairness and can be in tension with it\n\nlong tail and rare event learning can endanger some of the privacy aspects\n\n\nGenerative models are really important in the federalted setting – and it’s not just about making pretty pictures…\n\nCapturing this underlying data generating distribution is key\n\n\nOpen problems\n\ntightening bounds, extending domains, handling infrastructure\n\nWhere is all of this AI stuff going anyway?\n\nML / Data Science\n\nConflicting narratives\ngenerally discussing regression problems\n\nAI\n\nPassing a test or winning a game\nSuper human performance given\n\nA well defined problem\nA loss function\nenough data\n\nJust how remarkable territory this actually covers\nWhat’s the loss function for more profound things like\n\ncriminal justice\ncouples therapy\nArt\noptimal hiring\n\n\nThis is not just a human issue\n\nNeurophilospher\n\nSuccess of ANN’s, notwithstanding, is a far cry from what intelligent bodies on this planet can do\n\nMotivational tribes are messy - paper in 2016 Paper\n\n\nEcoli\n\nbacteria have a 1 bit output\n\ncan go forward / back / turn like an RC car\ntrjectory os ecoli looks (see slides)\n\nEnergy is a function of their consumption and subsequent output\nIs this consumption methodology optimal?\n\nWhat actually has been optimized by evolution in this process?\nInverse Reinforcement Learning\n\nwell studied but ill-conditioned\n\nModeling a signalling and sensoring function of the bacteria\n\nGenome is the reward map here\n\nthrough this evolutionary approach we can see chemotaxis(SP?)\n\nThe error bars on this example are relaly big\n\ncolonies have lots of reward maps\nhuge variety of reward maps that do work (see slides)\n\nWhat persists, exists\nevolution decides on what is good or bad\nthis is not exactly optimization**..\nSimple GAN Paper\n\nAll points are stable in wasserstein gan’s\nThe combined GAN is not doing gradient descent, locally each actor here is going gradient descent of its own well-defined cost function.\n\nPut together, the combined system\n\n\nLoss functions and gradient wrt special and general relativity\n\nEvery actor is curving the space and this leads to general relativity\n\nMany “solutions” in this bacteria case\n\nsignally begets collectivity\n\nOptimization is not really how life works\n\nit’s also not how brains work\n\n\nBackprop\n\nLooking at real neurons are a hell of alot more complicated\nBrains don’t just evaluate a function\n\nThey develop\nimprint\npre-programmed tasks\nself-modifying\n\nLooking at learning in ML, we’re trying to minimize a loss by picking a particular set of weights\n\nChain rule for all of this stuff\n\nbackprop through a linear layer, we can see that these backprop equations look very similar to forward prop\n\nthere’s a symmetry here\nAlso this weight update equation looks kind of Hebbian\n\nANN’s generally only implement the top part of the equation\nIf we didn’t do the bottom parts of the equation, it wouldn’t be doing much\n\n\nThe learning part of ML is a lot more complex than the feedforward linear layers in RELU\n\nThere’s always feedback\nThere’s always temporal dynamics\nAlso\n\nmomentum\nmini-batch\nadam\nstructured ranom init\n\n\nNeurons have all the building blocks\n\nPer-cell state\nPer-synapse state\nTemporal averaging\nRandom number sources\nmultiple timescales\n\nCan we learn to learn with these building blocks?\nA more general, biological symapse update rule that doesn’t require gradient descent\n\nLSTM at every synapse\nShared weights, but individual state\nNoise gate g\n(Anti-)Hebbian\nNeurons can behave the same way\n\nPer cell state and learned behaviour for how to propagate\n\nEquations are factorial and not scalary\n\nChemical activity\nallows multiple timescales\nmu parameters allow mixing at different time scales\nSlow timescales needed for learning, but also useful for time- qquestions\n\nWeights (or connectum)\n\nlearning\nDevelopment\n\nLSTM parameters\n\nIn supervised learning paradigm:\n\nshort brain lifetimes\n\n\nSelf-organizing neural cellular automata\n\nSelf-training NN’s that are training each cell\n\nreproduces a pattern\nlearns how to do this via purely local interactions\n\n\nThese kinds of fundamentally social concepts and ensembles of “things” come together and create the systems we have today\n\nhow we find this dance\n\nGrand Challenges\n\nBrains with fully evolved architectures\nUnderstanding and characterizing evolved systems\n\nrealm of anthropology and sociology\n\nProblem solving by artificial societies\nLarge-scale meta learning in the Federated setting\nPurposive “artificial ecology” engineering\nDynamical systems theory for neural ensembles\nCan we define quantitative “SOTAs” for sociality?\nCan we think about what it would mean to approach this kind of “curved space” AI ethics\n\nArtificial Life approaches?"
  },
  {
    "objectID": "posts/neurips-2019/index.html#dualdice",
    "href": "posts/neurips-2019/index.html#dualdice",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides Poster"
  },
  {
    "objectID": "posts/neurips-2019/index.html#from-system-1-deep-learning-to-system-2-deep-learning",
    "href": "posts/neurips-2019/index.html#from-system-1-deep-learning-to-system-2-deep-learning",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "These things are linked together in really interesting ways and he’s going to convince us of this\nConnected to the notion of agency\nSome people think that it might be enough to take what we have and grow our datasets and computer speed and all of a sudden we have intelligence\n\nNarrow AI - machines need much more data to learn a new task\nSample efficiency\nHuman provided labels\n\nThese dont catch changes in distribution, etc.\n\nNext step completely different from deep learning?\n\nDo we need to take a step back to classical eras?\n\n\nThinking fast and slow\n\n2 tasks\n\nkinds of things we do inuitively and consciously and we can’t explain verbally\n\nThis is currently what DL is good at\n\nSlow and logical, sequential, conscious, linguistic, planning, reasoning\n\nFuture DL\n\n\nWe’re generalizing in a more powerful and conscious way in a way that we can explain\n\nThe kinds of things we do with system programming\n\n\nMissing to extend DL to reach human level AI\n\nout of distribution generalization and transfer\nHigher level cognitive system\nHigh level semantic representations\n\ncorresponding to the kinds of concepts we link back to language\n\nCausality\n\nMany of these things tend ot be causal in effect\n\nAgent perspective\n\nBetter world models\nCausality\nKnowledge-seeking\n\nThere are questions between all of these 3 things listed above\n\nIf we make progress in one, we can make progress in another\n\n\nConsciousness\n\nRoadmap for priors for empowering system 2\nML Goals : handle changes in dsistribution\nSystem 2 basics : attention & consciousness\n\nCognitive neurscience to understand the human side of the consciousness\n\nConsciousness Prioer : sparse factor graphs\n\nWe can think of these things as assumptions of the world\n\nThe joint distribution betwen these high level concepts can be thought of as a factor graph\n\n\nTheoretical framework\n\nmeta-learning\nlocalized changes hypothesis -&gt; causal discovery\n\nLocalized in some abstract space\n\n\nCompositional DL architectures\n\nArchitectures we should explore to introduce the compositionality that we’ll need to explore\n\nNN’s that operate on sets of objects, and not just vectors\nDynamical recombination\n\n\n\nChanges in distribution (from IID to OOD)\n\nArtifically shuffle the data the achieve that?\n\nNatures does not shuffle the data, we shouldn’t\n\nIRM paper from Bottou\n\n\nOut of distribution generalization and transfer\n\nNo free lunch : need new assumptions to change this IID assumption\nIf we discard this IID assumption, we need to replace it by something else\nBengio posits priors might be the way to do this.\n\n\nOOD Generalization\n\nThe phenomenon of learner being able to genearlaize in some way to a different distribution\n\nIf we are a learning agent (agent = actions)\n\nwe almost always face non-stationarities\n\nDue to actions of self (agent)\nactions of other agents\nmovement through time and space\n\n\nOnce we start looking at multi-agent systems, it gets even more complicated\nTHERE IS NO STATIONARITY IN OUR ABILITY TO SAMPLE REALITY IN THE WAY THAT WE DO\n\n\nCompositionality helps IID and OOD to generalize\n\nIntroducting more forms of compositionality allows us to learn from some finite set of combinations about a much larger set of combinations that are NOT in the set of the data that we have today\nDistributed representations\n\nHelps us see why we get an exponential advantage\n\nIf we make the right assumptions, these things can be explained by variables and factors, and once we train a bunch of eatures we can generalize to new combinations of these features\n\nEach layer is composed for the next one, and this gives us another exponential advantage\n\nThe one we know best we find in language\n\nWe call this systemasticity\n\n\n\nThis opens the door to better powers of analogies and abstract reasoning\n\nSystematic generalization\n\nDynamically recombining existing concepts into new concepts\nEven when new combinations have 0 probability under training distributions\n\neg. science fiction scenarios\neg. Driving in an unknown city\n\nNot very successful with the use of DL systems\n\nCurrent methods when asking models to answer questions not in the distribution they do not know how to answer them\n\n\nConstrast with Symbolic AI Programs\n\nAvoiding the pitfalls of classical AI rule-based symbol-manipulation\nNeed efficient large scale learning\nneed semantic grounding\nneed distributed representations for generalization\nefficient = trained search (also system 1)\nNeed uncertainty handling\nBut want\n\nsystematic generalization\nfactorizing knowledge into small exchangable pieces\nmanipulating variables, instances, references & indirection\n\n\nSystem 2\n\nConsciousness and attention\n\nFocus on one or a few elements at a time\n\nwhen translating we focus on a specific word to do the translation\n\nContent-based soft attnetion is convenient, can backprop to learn where to attend\n\nSoft-max that conditions on each of the elements and we can see how well we match on context\nAttention is parallel in that we compute a score for each and decide which ones where we want to put attnetion\n\nAttention should be thought of as the internal action\n\nneeds a learned attention policy\n\n\nSoTA language models all rely on attention\n\nHow attention, connected to memory, can also unlock the problem of vanishing gradients\nOperating on unbounded sets of key value pairs\n\nWe can think of attention as creating a dynamic connection between layers\n\nas opposed to being hard coded today\nThis is great, but from the point of view of the receiving model, it receives a value but it has no idea of where it’s coming from\n\nWe condition to the value, we have the concept of a key, an identifier for where this value came from\n\nWe use this as a routing mechanism\n\n\nDownstream computation can know what the value it’s receiving and where it’s coming from\n\nCreating a name for these objects through a form of indirection\nwe have systems of operating on sets\n\n\n\nFrom attention to consciouness\n\nC-word is a bit taboo – but maybe not anymore\nnumber of theories are related to the global workspace theory\n\nThis theory says that what is going on with consciousness, there is a bottleneck of information in that some elements of what is computed in your brain is selected and then broadcast to the rest of the brain and influencing it\nConditions heavily on perception and action\n\nAlso gives rise naturally to the system 2 abilities above\n\n\n\nRelation to ML?\n\nML can be used to help brain scientist understand consciouness\nWork in neuroscience is based on fairly qualitative defns\n\nML can help us to quantify what this actually means\n\nFeedback loops help provide specific tests that we can use to measure these concepts\nOne of these motivations it to get rid of fuzziness and magic that surrounds consciousness\nProvide advantages to these particular form of agents\n\nThoughts, Consciousness, Language\n\nThere is as trong ling between thoughts and language, in that one can be translated between mediums farily easily though a lot of information si dropped on the floor during decoding\nWe want to explore things like Grounded Language Learning, by learning through environment interaction and perception\n\nAllows a learning to get to patterns through to it’s understanding of how the world works\n\n\nThe consciousness prior : sparse factor graphs\n\nWe can use these systems to encourage our learning systems to do a good job at out of distribution reasoning\nSparse factor graph\n\nattention : to form conscious state, thought\nA thought is low dimensional object\n\nWe sample these from a larger higher dimensional conscious state\nThe conscious states that we sample\nThe thoughts we consciously have are pushed through this consciousness bottleneck\n\n\nWhat do these computations mean\n\nSome kind of inference is required\nWhat kind of joint distribution of high level concepts are we reasoning about\n\nThink about the kind of statements we make with natural language\n\n“If I drop the ball, it will fall on the ground”\n\ntrue but involved very few variables in that the statement only contains a finite number of words\n\nThe relationship that I need to descibe can tightly capture the elements of this joint probability through very few variables\n\nDisentangled factors != marginally independent, eg. hand & ball\n\nWe think of these as having this very structured joint distribution\nThey come with very strong and powerful relationships\nInstead of imposing a very strong prior of complete margin independence we can find some prior that finds a joint distribution between these high level variables\n\n\nMeta-Learning : End to end\n\nMeta-learning is learning to learn\n\nBackprop through inner loop\nHaving multiple timescales of learning\n\niterative optimization like computation\nout of loop evolution\n\nIn this way we can talk about evolution algorithms, etc. and when we talk about this in the life of an individual\n\nlifetime learning is the outer loop and local interaction through time is the inner loop\n\n\nWe can train it’s slow timescale meta parameters to generalize to new environments\nWhat kind of hypothesis can we make?\n\nbecaue these actions are localized in space and time, because these things are locallly temporal then we can try to understand\nIndependent of cause and mechainsim – from an information perspective\n\nlearning from one mechanism tells you nothing of the others\nif something like this changes due to an intervention then ew only need to adapt the portion of the model that has to deal with that part of the distribution\nIt can be explained by a tiny change\n\n\nGood representations of variables and mechanisms + localized change hypothesis\n\nfew bits needed to adapt to what has happened\n\n\nHow to factorize a joint distribution in this way?\n\nLearning whether A causes B\n\nLearner doesn’t know but we might observe just X and Y\nTurns out, if we have the right composition then we can use this to learn about how to map X to Y, such as things like pixels that don’t have causal structure in that image itself\n\nThe assumption that these high level variables are causal doesn’t work on pixels\n\nWe cna’t find a pixel that causes another pixel\n\nLearning neural causal models\n\nwe can avoid the exponential number of grpahs that need to be considered through this\nONe of the things found was that in order ot facilitate the causal structure the learner should try to infer the intervention on which variable it was performed\nmost of the time our brain is trying to figure out “What caused the changes that I am seeing?”\n\nAble to find these commonly used causal induction methods\n\nAttacking this problem in a deep learning friendly way\n\ndefined obejective with some regularization\n\n\n\nOperating on sets of objects\n\nUsing dynamically recombinations of objects\nRecurrent Independent Mechanisms\n\noperating on sets of these objects\napplied to recurrentness\n\nstate is broken into pieces\nconstaining the way these networks are talking to eachother in that they done in sparse and dynamic ways\nvectors aren’t the standard vectors but rather sets of pairs\n\nnetworks are exchanging variables along with their type (key, value) pairs\n\nleads to better out of distribution generalization than those that don’t use these structures\n\n\n\n\nTested in reinforcement learning\n\nfound it helped in atari games\n\n\nRecap\n\nConscious processing by agents, systematic generalization\n\nSparse factor graphs in space of high-level semantic variables\nSemantic variables are causal : agents, intentios, controllable objects\n\nShares “rules” or modules that are reusable across tuples\n\nA particular subnetwork recieves input that is different dependent on context\n\nThis can be applied to different instances in that it’s much more like a bayes net but the same parameters can be used in many spaces\n\n\nanother really important hypothesis is that the changes in dsitribution are mostly localized if we’re presented information\nThings preserved across changes in distribution have to be grounded in that they’re stable and robust to stationarity"
  },
  {
    "objectID": "posts/neurips-2019/index.html#mapping-emotions-discovering-structure-in-mesoscale-electrical-brain-recordings",
    "href": "posts/neurips-2019/index.html#mapping-emotions-discovering-structure-in-mesoscale-electrical-brain-recordings",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides\n\nBrain is an organize that integrated bio-information with electricity\n\nworking on integrating across discplines to get better models of brain functions\n\nCausality???\n\nIf we understand how the brain does what it does, we can reverse engineer it and use that to understand it better\nCan we turn on and off other areas of the brain\nWe come to this conclusion around causality because human beings have observed the earht other and over again in many contexts and test these assumptions using models and test these assumptions and map them back to our models\n\nThis is important for neuroscience\n\nMany of the manipulations of the brain cause it to not function how it does naturally\n\nIt might make more sense to observe the system over and over again\nWe can test these models\n\nIf that’s the case, the human body is 98 degrees farenheit\n\n\n\n\nDepression (DSM)\n\ndepressed mood\ndiminished interest\nincrease or decrease in appetite\nhypersomnia or insomnia\n\nIs MDD prevention a viable therapeutic strategy?\n\nImagine a disease case where someone has a heart attack of failure\n\nmake that heart pump more normally\nIdea is to make a diseased heart and make it function more normally\nOne of the things that has the greatest impact on the system of the heart was aging\n\nmeasured variables in lifestyle that might help predict heart attack later on in life\n\n\n\nEmotions at latent networks\n\nThe idea is that we use fMRI to look at changes in bloodflow in the brain\n\nUsing these changes as a proxy for brain activation\nTaking healthy controls, or students\nPut them in a scanner and let them watch movies while inducing emotions\n\nUse ML to see what emotional patterns were induced\n\nAfter about 20 mins they tap the students and ask how they were feeling\nThis suggests we’re able to liberate the emotion from the patients’ brain without self-reporting\n\n\n\nAssumptions\n\nAssumption 1: Emotional encoding at the second timescale\n\nThis is important because people have classically thought about controlling variables and repeatedly observe a system\nUseful for studying vision, motor function, sensory systems, etc.\n\nThis system might be built to do something very different than emotional systems\n\nWhen we have a system to process emotions, we want something that has information resonance at a slower timescale than moving arms, and legs, etc.\nEmotions are encoded at the timescale of seconds\n\nAssmption 2 : Emergent Properties\n\nWe have a cell property and it traverses a neuron and checmical information is sent down the axxon\n\nhard to think about emergent properties of these systems\na seizure is an emergent property of the brain\n\nwouldn’t generate this phenomenon without more than 4 cells\n\n\nThe system is working together in an integration fashion to create these properties\nTHINK SLEEP\nLOcal field potentials can be used to measure certain properties of the system\nLFP coherence (functional connectivity)\nsycnhrony or coherence\n\nwe can infer directionality in a circuit\n\nCoupling between cell firing and LFP activity\n\nAssumption 3 : Local Field Potentials reflects the activity of populations of neurons (emergent features)\n\nTrying to find things that generalize across brains, species, and inviduals\n\nLatent Network Model\n\nEach layer is useful for one of things we wanted to do\n\nEach of us had to believe one layer of this model\n\nInformation across frequency and information that is leading or lagging\n6000 things that we could measure and quantify in an animals brains\nPhase offset\n\nAcquiring brain network behavior"
  },
  {
    "objectID": "posts/neurips-2019/index.html#agency-and-automation",
    "href": "posts/neurips-2019/index.html#agency-and-automation",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides\n\nHype and sensationalism drive some of the interest but there is a substory there around automation\n\nFei Fei Li - write in NYT that enthusiasm for AI is preventing us from reckoning our immersion into it…\nMichael Jordan\n\nNeed well thought out interactions with humans and computers\n\n\nThese are not new\nWe have a 60 year old design challenge to find an optmiality between divisions of labor and automation\n\nAutomation and user control\n\nWaht si the appropriate balance here?\nChallenges of automation\n\nAutomated methods may be biased or innacurate\nThese concequences can be quite damanging in the real world\nLoss of critical engagement and domain expertise\n\nWe lack a global view as humans and over-weight local information\n\nBalancing automation and control can be done through building models of capabilites, actions, and controls around the tasks that we perform\n\n3 Examples\n\nExploratory Data Visualization\n\nincorporate tasks that the user is trying to achieve into the design of the visualization of the data\nSee slides for multi-verse analysis of the topics that might be present in these documents\nWhat makes a visualization good?\n\nTask specific and subjective references\nFoundational issues in perception that we can build upon\n\nShows the long standing results in psychophysics in how our perceptual system can quickly decode visual types of information\nCommon exploration pitfalls\n\nOverlooking data quality issues\nFixating on specific relationships\nMany other biases… *Data Voyager\nexamples in slides\nWe want to suppoer systematic considerations of the data\nModel user’s search frontier, optimize for related chart specification, seeded by the user’s current docus\nCandidate charts pruned and ranked using a formal model of design constraints"
  },
  {
    "objectID": "posts/neurips-2019/index.html#bayesian-deep-learning-workshop",
    "href": "posts/neurips-2019/index.html#bayesian-deep-learning-workshop",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Slides\n\nPaper has been around for almost 2 years now\nLots of forward work – along with an extended paper on arxiv today along with code\nData efficiency is a serious problem for deep RL\nPrior and weights are typically very difficult to interpret\n\nWhy do we expect good performance?\nPossible what we are doing inference with a terrible prior\n\nSeminal results of the paper\n\n1994 - showed that a single MLP with K hidden units, with a carefully scaled prior\n\nscaling outgoing weights by 1/K – as you take the limit as k -&gt; inf\n\nthe vector converged to distribution multivariate with mean 0 and unit variance\nsome form of gaussian quadrature\nproves this with standard multivariate central limit theorem\n\n\n\nCentral Limit Theorem\n\n1 dimenstional CLT there are some interesting things\nRandom variables converges to CDF at all points where the CDF is continuous\nCLT says that if we consider IID rv with mean 0 and unit variance\nSome sublties\n\nConsider an IID sequence of 2 possibilities [-1, 1] with P(0.5) has mean 0 variance 1\nWe can define a set A\n\nThen for all n where A has probability zero under N(0,1)\nThis set has 0 probability under this distribution\n\nbe careful with what convergence of distribution actually means\n\n\n\n\nWhat does this mean for a stochiastic process to converge in distribution\n\ncarefully scaling the prior\nweights coming out of these layers will have 1/k_1 and 1/k_2 respectively\nc_2 is a normal quadrature is defined in terms of the covariance of the previous layer making it a recursive kernel definition\n\nDeep Neural Networks as Gaussian Process\n\nReleased same day and accepted as same confernece\nCheck this paper out as well Paper\n\nRigorous proof provided\nWhy would we expect a CLT here at all with multiple hidden layers\n\nRadford Neil\n\nFeeding a single data point through and we can look at the f_1 units - each will converge independent of other variables\n\nMultiple input data points\n\nthere is a correlated normal vector at each f^(1)\n\nat some point, increasingly independent vectors converge to a correlated normal vector\n\n\nProblem with the argument\n\nPreliminaries\n\nNeed a convergent non-linearity\n\nDraw a bounding envelope on any point around the non-linearity\n\nwe might get something that might not be defined if we don’t, it effectively stabilizes everything\n\n\nThink about the network as an infinite sequence of network\n\nThe hidden layers may grow at different rates as long as they all tend toward infinity\n\nFormal statement of the theorem\n\nProof sketch\n\nproceed through the network and by induction starting to closest data\nat each layer, reduce the problem to the convergence of any finite linear project of data and units\n\nExchangability\n\nAn infinite sequence is exchangable if any finite permutation leaves its distribution invariant\nde Finetti’s theorem\n\nan infinite sequence of random variables is exchangable iff ti’s IID conditional on some random variable\n\n\nExchangable CLT slide\n\napplies to triangular arrays\n\nExperiments in the paper are relatively small data with low dimensionality Slide\n\nIn the majority of cases considered, the agreement is very close\none can’t tell the difference between the GP and 3 layer NN\nSlide shows, empirically, there seems to be little difference between a standard GP and a DNN with 3 hidden layers\n\nLimitations of Kernel Methods Slide\n\nThis property might not be a good thing\nKernel methods are affine transformations of the training outputs\nThis limits the rperesentation that we can learn\n\nDeep GP’s\n\nNot marginal GP’s because they have finite restrictions in the norm\nThis prevents the onset of the CLT\n\nSubsequent work Slide\n\nCNN’s also converge to GP’s\nNeural Tangent Kernel considers not what just happens for the initial distribution of the NN, but also what happens when we apply gradient descent\n\n\n\n\n\nSlides Paper\n\nBackground\n\nVectrorize the output of the NN’s into an n x k vector\nwe know that the application that we have done will handle 1D output\nAll theoretical results apply to multi-outputs\nWe know that NN outputs are a function of the parameters which in turn are a function of time (think evolution of gradient descent)\n\nNatural GD\n\nappealing because it has convergence, covariance, and invariance under reparameterization\nFisher Information Matrix allows GD to take the curvature of the distribution space into account\n\nSmall changes in parameters can effect the training dynamics\nInverse Fisher allows us to take into account this space’s information geometry\n\n\nConcatenated Fisher Information\n\nwe can condition the FIM on a single data point x\n\nTraining dynamics under natural gradient descent\n\nNatural Nerual Tangent Kernel includes the fisher information matrix which includes the distribition geometry into account\n\nAssumptions\n\nNetwork overparameterization\nPositive definiteness\n\n\nImplications\n\nComputing the NTK yields an interesting result\nBound on prediction discrepancy Slide\nEmpirical results\n\nSymthetic data Slide\nTheoreitcal bound is meant to be tight\nThe values increase further away from data – see tails of the plot\nComparing the predictive distribution – see slides\n\nFuture Direction\n\nApproximate inference\nscaling to larger datasets\nClassification tasks\nGeneralization analysis\n\n\n\n\n\n\nThere is huge interest in the intersection between Neural Networks and Bayesian Believers\nepistemic estimation is REALLY important for areas with high risk\n\nbayesian or not there is a really great potential to healthcare and autonomous driving\n\nType of Uncertainty Slides\n\nEpistemic uncertainty “how much do I believe this coin is fair?”\n\nmodels’ belief after seeing the population\nreduces when we have more data\n\nAleatoric Uncertainty - “What’s the nest coin flip outcome?”\n\nIndividual experiment outcome\nnon-reducible\n\nDistribution Shift - “Am I still flipping the same coin?”\n\nIndicating a change of the underlying quantity of interesting\n\n\nQuick intro to BNN’s Slide\n\ninstead of learning point updates, let’s put a distribution in place here over the parameters\nin practice \\(p(w|D)\\) is intractable\n\nFind an approximation \\(q(W) \\approx P(W|D)\\)\n\n\nWeight space uncertainty is less interesting\n\nin many cases NN’s weights are NOT scientific parameters we’re interested in\nsymmetries/invariance in parameterization\n\nexmaples like swapping nodes and scaling of weights, we’re still approximating the same function\n\n\nThis introduces vagueness Slide\n\nsample weights from the Q distribution\nfolklore belief for function-space (or output-space) uncertainty:\n“Epistemic uncertainty should be high when new input is less similar to observed inputs”\nWhat do “high uncertainty” and “less similar” mean qualitatively?\n\nThis is typically “eye-balled”, leaving it to be subjective by definition\nThere is really no agreeable diescription of where and by how much it should be higher\n\n\nEvaluating by comparing to references Slide\n\nBNN’s performance relies on a approximate posterior\nEvaluating inference:\n\ncomputes some distance metric between q(W) and p(W|D)\n\nFunction space “reference posterior” for BNN regression:\n\nsome hope in function space\nwide BNN has GP limit (under certain conditions)\nfor regression problems \\(p_{GP}(f|D)\\) is tractable\n\n\n\n\n\n\nSlides\n\nHow do we accomplish learning from scratch or from very small amount of data\n\nModeling image formation\n\ngeometry of the image\nSIFT features, HOG features, etc.\nFine tuning from ImageNet features\nDomain adaptation from other painters\n\nFewer human priors as we move down the list above\n\nCan we explicitly learn priors from previous experience?\nBrief Overview\n\nGiven 1 example of 5 classes :\n\nclassify new examples\n\n\nHow does meta-learning work?\n\nOne approach : parameterize learner by a neural network\nAnother approach : embed optimization into the learning process\nThe Bayesian perspective : learn priors of a Bayesian model that we can use for posterior inference\n\nThe problem\n\nHow we construct tasks\nWhat if label order is consistent?\n\nA single functional can solve all the tasks\nThe network can simply learn to classify inputs, irrespective of the data distribution\n\n\nMeta-training to “close the box”\n\nIf you tell the robot the task goal, the robot can ignore the trials\nanother example : pose estimation and object position\n\nmemorize the post and orientation of the meta-training set\nat meta-test time, without knowing the canonical orientation, we don’t be able to accurate predict the orientation\n\n\nWhat can we do about this?\n\nIf we had a proper bayesiaan meta-learning algorithm that was learning a proper posterior, we might not have this problem\nHowever, I’m not sure if we have the tools to create a proper meta-learning algorithm\nIf the tasks are mutually excluse, a single function cannot solve all the tasks (due to label shufflinf, etc.)\nIf tasks are non-mutually exclusive, a single function can solve all tasks\n\nmultiple solutions to the meta-learning problem\n\n\nMeta-regularization\n\nControl the information flow such that we can do zero-shot learning from the data\n\nminimize the meta-training loss and the information contained within the parameters of the model\nregularizing the weights forces the model to use information from the data as opposed\n\nCan combine this with a favorite meta-learning algorithm\n\nDoes meta-regularization lead to better generalization?\n\narbitrary distribution over \\(\\theta\\) that doesn’t depend on the meta-training data\n\nMeta-world benchmark\n\n\n\n\nSlides\n\nMotivation\n\nExperimental design problems that can be cast a a global optimization over some parameter space\noptimizing on non-linear projections\n\n\n\n\n\n\nWhy do we care about function space priors?\nLots of testing methods for bayesian approaches\n\nsee slides\n\nBut these all have non-Bayesian approaches that are competitive\nThree X’s\n\nExploration\nExplanation\nExtrapolation\nThese cases all depend crucially on having good priors that reflect thr structure of the underlying distribution\n\nCompositional GP Kernels\n\nGP’s are distributions over functions parameterized by kernels.\nPrimitive Kernels\nComposite kernels\n\ntaking products of kernels\n\nThis can express things like periodic structure that gradually changes over time\n\nNo need to specify structure in advanced and can be inferred online during training\n\n\nStructured Priors and Deep Learning\n\nDemonstrates the power and flexibility of function space priors\nProblems\n\nRequires a discrete search over the space of kernels for each candidate structure\nNeed to re-fit the kernel hyperparameters\n\n\nDifferentiable Compositional Kernel Learning for Gaussian Processes\n\nNeural Kernel Network\n\nrepresents a kernel\ninputs are 2 input locations\noutput is the value between them\n\n\n\n\n\n\nSlides\n\nChallenge the assumption\n\nASsumptions that the approximate posterior that we use to model our BNN, ought to have correlations between the weights\nMean-field assumption that our weight distributions are independent of eachother because we’re avoiding intractability\nThis is less true as our neural network gets much deeper\n\nWhy might our approximate posterior need to have correlation between weights?\n\nMaybe the true posterior does?\nA lot of intuitions we have come from this small interpretable single layer 4 neuron model\n\nWhat we think is that alot of these effects disappear as we get deeper and deeper networks\n\n\nWith depth, we can induce rich correlation over our output distribution with mean-field weights\n\none way to do this is to have covariance between \\(\\theta_{1}\\) and \\(\\theta_{2}\\)\nAs we get a deep network, we can get richer covariance structures\n2 inputs and 2 outputs with a simple weight layer w\n\nassuming linearity\nmean-field approximation\n\nLesson from the linear case\n\n3+ mean-field layers can approximate one full-covariance layer\nMore layers allow a richer approximation\n\nMeasuring the price of this mean-field approximation in NN’s that do have non-linearities\n\nHMC true posterior\nfit a full-covariance gaussian\nfit a diagonal covariance gaussian\nmeasure the difference between them, and it should give us an understanding of how costly the extra assumption of diagonality is\n\nMeasuring the ‘price’ of the mean-field approximation Slide\n\nhold parameters model throughout this testing\n\n\nWhat are the implications here?\n\nRely less on UCI evaluation with a single hidden layer\nMore research into other problems with Mean-Field Variational Inference\n\nE.g. sampling properties of high-dimensional gaussian (“Radial BNN’s”)\n\nLess research into structured covariance variational inference\n\n\n\n\n\nSlides\n\nIn bayesian statistics, priors are meant to represent our knowledge about the domain\nMapping domain knowledge to neural networks is hard\nControlled Directed Effect\n\nMeasure sensitivity of an outcome vaiarble to changes in a set of variables while all other factors are held fixed\n\nTypes of CDE Priors : Monotonicity and Invariance\n\nNeed to translate domainknowledge into expectations about CDEs for transition x -&gt; x’\n\nGuiding Functions\n\nOne way to think about input transoformations at every point x R, pick a direction R^{D} to push x\nthink of this guiding functions\n\nTranslating the CE into a prior\n\nCDE is expensive to compute, but can apprixmate it using gradients, and ignore scale\nDefine an error function for local invariance and monotonicity\nIpose a gaussian prior over this Error function above for a BNN\n\ncan be used for mini-batch variational inference\n\n\nContribution is proposing a framework for applying these priors above\n\ntoy examples\n\nin 1D - sampling from a NN - assuming independent gaussian priors for each weight – the functions depend a lot on\nimportant to understand this is a local constraint\n\nWe can impose a prior that it increases with\n\nConsider invariance case instead of monotonic\n\n2D manifold where the values of f and g are equal\n\nnot clear if we should be following f or g\n\npredictions are independent of changes in g\n\nsignificantly reduces error\n\n\n\n\nInvariance priors on COMPAS\n\nFirst trained model g(x) to predict defendant’s race, then trained a second model f(x) to predic recidivism w/ local invariance to g(x)\n\nwithout loss of accuracy, we can close the gap between false negative and false positive rates\n\nThresholding schemes\n\nThis si a building block for translating domain knowledge into a prior\n\n\n\n\nSlides\n\nPredicting the effect of human genetic variation from sequences alone\nProblems we saw in a number of areas dealing with the language of biological data\ngenotype to phenotype\n\nif we want to change the phenotype, we want to understand the interaction of the environment\nwe also want to design biological sequences\n\n10 billion people\n\n\n1000 billion genomes\n\nImportant to understand models and analysis we have are based in the 80’s and 90’s\nPotential sequences that are functional\n\npotential sequences that are functional are much much larger than that\n\nwe might want to predict this in expectation\n\nWhy do we need better prediction and design in biology\n\nuncertainty for medical decision making\nmolecular biology that impacts human health\npredicting how pathogens will mutate\nsynthetic biology for designing theripeutic impact\n\nWhat does this sequence look like under constraint?\n\ndon’t even have benchmark datasets ready for people to play with these environments\nmain thing to get across that the estimation of uncertainty really matters\n\nCan’t we just measure the effects of all genetic variation\n\nmutation effect prediction is hard\n\nmutation effect prediction lacks\n\nsparsity sampling\nnoisy\nchanging 1 position in the DNA, it’s not just thinking about that position but all of it’s impacts and confounders\n\npropr art:\n\ncompute what’s conserved across evolution\n\nsequence alignment and preservation\nthe way way we regard these sequences are the result of billions of experiments run on the human species\nnot accurate to look at one column because of dependency on positions\ncapturing the dependence between sequences\n\nusing pariwise factors are powerful\n\npsuedo likelihood because we can’t calculate the partition function\n\n\n\ncapture these complex dependencies\n\nwe can’t keep adding terms to likelihood models\nmutation prediction with a variational autoencoder\n\ninfer a generative model of the family\n\nestimate how probably sequences are\n\nA doubly variational autoencoder on decoder weights prevents overfitting\n\nbiological constraints included in the model\n\nlatent variables are generated for each sequence in alignment\nshowing that the latent space seems to be capturing some structure slide\n\nwe want to predict genetic variation\n\nhow are we going to know if we’re right?\nDeepSequence captures mutation effects better than state of the art\n\n\nBut P(X) has been dependent on alignments…\n\n“alignments” used by every branch of biology, genetics, clinical decisions – these are all methods from ~ 20 years ago\nall heuristics\nchallenge is to build models that don’t depend on these alignments\n\nalignment uncertainty\ninsertions and deletions\n\nReinterpret our methods in terms of structured noise distributions\nNew Seq2Seq regression model : intuition\n\nconditioned on the initial sequence X, predict sequence Y\n\ncan change letters in X and have the capacity to delete them\ndeveloped and explored simple seq2seq model\n\nused categorical distribution over nucleotides, etc.\n\nsample W from the prior over variables-size\n\n\n\nThis generalizes past algorithms and models\n\nHierachical latent alignment HMM\n\nSample latent x from a population\n\nproperties that make it really easy to use in BNN methods\n\nThis is an unsolved problem for HMM’s which have been used in biological sequencing\n\nmarginal is a smooth function of x and which allows for automatic differentiation\ninference method is SGD\n\nInference methods\n\nalignment HMM on the encoder side\n\nResults Slide\n\nSummary Slide\n\nhave done this on several families of protiens\nALignment uncertainty Example\nLatent representation from PCA models reflect the underlying biolog of VDJ recombination\nFlu Virus evolution Slide\n\nWe can see the evolution across the latent space\nwe might be able to predict sequences\n\n\n\n\n\n\nSlides Poster\n\nInteraction with liquids happens every day\n\nSpecific containers and specialised tools to manipulate these liquids\nWe can approximate the way these things will behave\nThe shape of the continaer has causal influence over the way that liquids interact with it\nViscosity of the liquid has intersting causal properties\n\nThinking about this from a robotics perspective\n\nSome of the things very natural to us are hard for robots\n\nthe complex properties of liquids makes this hard for robots\n\n\nWhat is it that we, as humans, do to help this manipulation\n\nCogSci theories\n\nWe have some approx simulation in our heads that enable these predictions\nPeople have shown that we can invert this simulator in our head and make predictions about properties in our heads\nDifferent types of interactions can give us different cues about viscosity\n\nWe need some sort fo fast approxiate like thsi for embedding in robots\nWe don’t need exteme accuracy but rather representing these objects in a more approximate and efficient way\n\nIntuition as approcimate simluation\n\nNVIDIA Flex\n\nPosition based dynamics\nAs with any simulation we use, we have a reality gap\n\nThis is discrepancy between observation in the real world and simulation environments\n\nSources of error\n\nmodel approximation – not much we can do here\nparameterization – we have to set the parameters of the model and without correct settings we’ll get variance in our predictions\n\nSim2real discrepancy is what we’re trying to track in this\n\n\nTwo stage process\n\nEstimate parameters & learn to pour\n\nwe want generative models to enable adaptatoin to dynamics of the environment\neven though it’s the same experiment, we want to minimize spillage, but at the same time we want it to spill a bit because it’s informative\n\n\nLearning how to pour\n\nHow to use the sim here\n\naction and observation spaces are as follows\ncount the number of particles that fall outside of the container\nmeasure the spillage with a scale, and noramlize such that we have a % spillage to compare between the two domains\nFind the relative distance between source and target container while measuring how fast it’s being filled\n\nThe way we do this is to model this as a Gaussian Process\n\npour N times (37 in paper though 15 should be enough)\nlearning combinations of velocity and relative spillage\nafter learning this we transfer it right to the robot\n\nApproximate fluid simulation is useful\n\ngeometry of the container causes high spillage!\ninitialization of policy with simulation works best\n\n\nTo stir or not to stir\n\ncalibration to properties of the liquid through perception\nThe way we calibrate is to perform, in a synchronous way\nCohesion models the best the change in viscosity in the real world\n\nthe condition is the thing that is being modeled\nsimulator cannot model adhesion\n\nthese characteristics cannot be simulated\nis there a way we can model this friction coefficient that might be present in specific liquids?\n\n\n\n\n\n\n\nSlides\n\nGoal of this research is to study generative models of physics from a cognitive science perpsective\n\nHemholtzian idea of perception as inverse optics\nSome uderlying true state of the world\n\nbut we don’t have access ot that\nwe only have access to retinal images\nthere is some lawful set there\n\ncan we invert this image, knowing what we know from optics, to derive information about the world\n\n\nThe world changes over time and gives rise to a sequence of images that we see\n\nthese changes happen in lawful ways such as dynamics\n\nWe don’t want to treat these observations as IID\nWe can use this to constrain our inferences\nPerception is constrained by dynamics\n\nPeople’s judgements about the slant of a ramp given the visual state of the ramp\n\nas our perception of the ramp changes, if affects how we perceive the world\n\n\n\nWhat are these dynamics in the world and how do we capture that?\n\nIntuitive Physics ENgine\n\nThe generative models we have in our heads are based on object baed representation\n\nSome probability distribution presents a range of world state\nThis gives us a range of possible ways the world might unfold\n\nWe run out model forward and count up the number of blocks\n\n\nImportant features\n\nobject based\n\nshows object based importance in early human development\n\nprobabilistic model\n\nNot just one possible future, but range that we can make predictions over\n\nWe don’t need a veridical model of physics but one good enough to action plan\nThis physics engine should favor speed and efficiency over precision\nMOdel is generalizable in that we don’t need to learn separate physics models for all situations\n\nHow do we do this?\n\nPredict - have a generative model of the world, ask what happens next, run out the model and observe\nProbabilistic framework unlocks a lot of additional capabilities for perception\nPerceive causality – remove A from simulation\nMake plans and choose actions based on these model run outs\n\n\n\nPhysics in the loop of perception\n\nPerceiving what is in the world\n\nseeing occluded objects Papers\nseeing surprising events Papers\n\nUnderstanding actions in the world\n\nSeeing Occluded objects with generative models in the loop\n\nIf we have a set of objects that have cloth draped over them, we can infer what object might be under that cover\nWe need some sort of generative model that allows us to internally ask “what would this look like with a cloth over it?”\nSee slides for how to model this occlusion phenomenon\n\nuse dynamics and physics of cloth to find a draped cloth geometry\nInference with Bayesian Optimization is key here\nUnderstanding How that cloth might drape is important for understanding what something occluded with cloth might look like\n\n\nSeeing Surpriving Events like an infant\n\nDetecting violations of expected dynamics\nPermenance\n\nobjects can’t teleport\n\nSolidity\nCOntinuity\n\nwhen objects violate these properties of how objects work, then they can update their model of the world\n\n\nWhat do we need to build into an agent such that it can percieve the world but then update their understanding of the world according to some surprise factor\n\nPerceive violations of these principle drives learning\n\nADEPT Model Slide\n\nGiven an image - we first extract object information\n\napproximate de-renderer\nthis object has attributes has understanding of position, velocity, etc.\n\nshape information is thrown away\n\n\npropose object masks\n\nfeed through renderer gets object properties\n\nInternal scene representation -&gt; physics observations\n\nobjects are moving at certain velocity and objects interact, they don’t move through eachother\n\nWe want to match the above observations against a “ground truth”\n\nthis isn’t matching in pixel space, but rather matching wrt objects\nwe also have to gracefully deal with unexpected events\n\ndisappears and we want to handle it by saying this is something weird that happened, but this is my new normal and I no longer need to track it\n\n\nMeasuring Surprise\n\nviolation of expectations (from psychology)\ncreation of a bunch of physics based violation types\n\nthese match to infant understanding principles\n\n\n\nInfants don’t see non-physical events\n\nthis allows us to constrain our space of potential evaluation\nobjects in shapenet\n\nAlternate Theory\n\nBootstrap these princples above\nCan we learn this from enough data?\n\nRapid trial and error (repuposing of objects)\n\nexample of a stake and a tent\n\nrule out branch, pinecone\npick rock\n\nfinding representations of the properties of these objects is inherent to planning in these situations\nthis seems to be a core feature for people\nPHYRE benchmark\n\nFocused on model-free RL from balanced datasets\nlearn generative model of the dynamics of the envornment\n\nVisual foresight for learning to push objects with tools\n\nfrom vision required many samples + demonstrations\n\n\nSSUP Framework\n\nsample, simulate, update\n\nPrior\nInternal simulator\nLearning mechanism\n\n\nConclusions\n\nCausal models of dynamics are important for perception and action\nTypes of representations & dynamics are crucial\n\nobject-based, approximate world models\n\nJust generative models is not enough – requires additional information\n\n\n\n\n\n\nNeural Netwoks and CONVNETS are super dense\n\nwe’re grabbing much of background context, etc that don’t necessarily matter\n\nHierarchical compositionality\n\nWay fewer parameters\n\nthe whole model was quite interpretable ane debuggable\n\neach unit was a node in a graph – allowing representations of images in graphs\ninference was done in a very hacky way\n\nAI Today\n\nAI for simulation\nSimulation needs a lot more learning involved\nOpen Problems\n\n3D Envornments / Scenes\n3D Objects\nActivities\nBehavior\n\nScalability, realism, diversity : Learn how to simulate!\nScene composition\n\nmaking this a little more scalable\nIn gaming, worlds are build using sort of probabilistic\n\n\nMeta-SIM\n\n\n\n\n\nCross-bite challenge\n\nBuilding Machine That Learn and Think Like People\n\nUnsupervised Object Tracking\n\nTraining (no annotations!)\n\nfind donstruction of videos in terms of moving objects\n\nTesting\n\nnew set of images from the same distribution and eval performance same as supervised learning\n\nSequential Attend, Infer, Repeat (SQAIR) Paper\n\nUnsupervised object tracking\n\nVariational autoencoder\ntrained by maximizing the ELBO\n\nhopes to learn the dynamics of the objects\n\n\n\nSpatially Invariant, Label-Free Object Tracking (SILOT)\n\nnew architecture\nincludes features to help it scale up\nallows objects to condition and coordinate on eachother\n\nwe can sidestep the require sequential structure"
  },
  {
    "objectID": "posts/neurips-2019/index.html#artificial-and-biological-reinforcement-learning",
    "href": "posts/neurips-2019/index.html#artificial-and-biological-reinforcement-learning",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Let’s let machine learning figure out the catastrphic forgetting problem\n\nFraming the problem up as a meta-learning problem\nThis is called “meta-training”\nOnce this training is done, we take the meta-vector and evaluate on all T tasks that have been learned\n\nOnline aware meta-learning Paper\n\ndoesn’t suffer from catastrphic forgetting\nlearns to induce a sparsity in it’s representation\n\nactivates fewer neurons – ie. most of them\n\nGets a lot right, but it’s still ultimately subjective to SGD\n\nhard problem of finding representation once SGD gets applied to it\n\nWe allow control over SGD “neuromodulation”\n\ndirectly modulated activations\n\n\nANML (Neuralmodulated Meta-Learning Algorithm)\n\nneuromodulatory network can gate the DNN that will also gate the backward pass\n\nselective activation\nselective plasticity\n\n\nOmniglot, following OML\n\neach character type is a class/task\ndifferentiate through 600 tasks\n\nevaluate on all 600 tasks – this is WAY too unstable for today’s SGD methods (like 9000 steps)\n\nLearn sequentially on one class in the inner loop\n\nContinual Learning is Hard\n\nNormal deep learning\n\nIID sampling (no catastrophic forgetting)\nmultiple passes through data\n\nSequential Learning *ANML might be leading to an overall solution to catastrophic forgetting\n\n\n\n\n\n\nRepeated Decisions with Imperfectly Known Consequences\n\nBrain science – they frame this as a multi-bandit problem\n\nExperimental Setup\n\nfour arm bandit tasks\n4 conditions"
  },
  {
    "objectID": "posts/neurips-2019/index.html#context-and-compositionality-in-biological-and-artifical-systems",
    "href": "posts/neurips-2019/index.html#context-and-compositionality-in-biological-and-artifical-systems",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Deep Understanding vs Shallow Understanding\n\nResponding (frequently) in behaviorally appropriate ways without really getting the overal picture\n\nlook up Eliza\n\nreveals the gulliability gap\n\n\nKeyword matches are used all the way through to 2014\n\nGoostman (won some version of the Turing test)\nDoesn’t represent real progress\n\nGPT-2 seems fluid\n\nlong way from early generations\nnot actually coherent\nOften plausible for a few sentences of text of surrealist fiction, where there are no facts of the matter\n\nPrediction at the world level != prediction at the world level\n“Local coherency; global gibberish” - Dan Brickley\nAdversarial NLI : A new benchmark for Natural Language Understanding\n\nState of the art models learn to exploit spurious correlations in the data – we see this in the visual perception field as well\n\n\nWhat is deep understanding?\n\nDeep Understanding is being able to\n\nconstruct an internal model of what is said/depicted in a story/article/movie/etc\nperform every day inferences about what is left unsaid\n\n“What do I think of Western civilization? I think it would be a very good idea.”\n\nArguably the closes to deep understanding of the oft-misaligned CYC\n\nCan make nuanced inferences about character motivations\nBut : system doesn’t have a natural langauge front end (you cna’t just feed Romeo & Juliet in)\nRelies on human experts to encode each problem\nthere are also serious issues of coverage, sealing with uncertainty, etc.\nMOve this out of it’s domain, it would fail completely\n\nin some interesting ways it’s the closest thing we have\n\n\nShallow prediction vs deep parser\n\nthere are lots of tools out there that are useful\n\nlots of coverage issues\nparse sentence into units and do symbolic computation\n\n\nHow might we get to deeper understanding : Two ways of thinking about that moving forward\n\n\nBenchmarks don’t encourage out-of-the-box thinking\nBenchmarks can and are often easily gamed\nThe Kaggle Effect\n\noptimizing for a single metric leads to tradeoffs and shortcuts which make you over specialized\n\nBenchmarks take a lot of time to develop\nBenchmarks are prepackaged; humans experience rarely is\n\nkids don’t get to download datasets and test\n\nWe shouldn’t, in princple, expect any single benchmark to suffice\n\nno one thing should be measured because the mind is not one thing\nintelligence is clearly multidimensional\ninvolves manY vectors\n\n\n\nAdvice to young scholars\n\ndon’t just look to what the ML community is publishing\nlots of extent data in other fields involving the brain that might be useful\nplenty of work suggesting other challenges as well\n\nChildren’s over-regularization errors\n\n1992\nwidely modeled throughout the 90s\nlots of people modeled this data, didn’t need to be on kaggle, but people tried to figure it out\nnot packaged nearly and nicely – we need to go find this data for our use\nAll models out there cheated relative to what a child does\n\nlist of stems in past tense forms as data but kids don’t have this available to them directly\nThey’re able to map grammar structures without the spoonfeeding of the field\n\n\nMarcus et al 1990\n\ncouldn’t use transitional probabilities because of the way they structured the grammar\ndata is still there, not in benchmark form\nkids only had 2 minutes of data\n\nInfant Learning Rule\n\nmany models proposed in 1999\n\nAdult generalization of inflection to foreign phenomenon\n\nHebrew speakers could generalize to sounds in English though they’d never seen it before\n\nUniversally quantified 1:1 mapping\n\nAll these are examples of free generaliation of universally\n\nEven today there are challenges in learning UQOTMS in systems that lack operations ovre variables\nOnly now is the importance of this issue started to become recognized\n\nOOD generalization\n\nComprehension challenge\n\nToward a benchmark for Dynamic Understanding\n\ndevelop internal models about what is happening\nDistinguished from static understanding\n\nconventional knowledge about what happens in general/generic/ordinary undertanding\ndynamic understanding is keeping track over time in some situation\n\n\nCaveats\n\nNot claiming sufficiency capturing all aspects of NLU\nNot claiming this is the only way to improve NLU benchmarks\n\nWe do think that too few existing tasks look directly at dynamic understanding\nTwo for static understanding\nFour for dynamic understanding\nThe dix tasks are illustrative not exhaustive\nStatic task 1 : Conventional knowledge\n\ntests understanding of every dat factual knowledge\n\neg. the part of a fish that gives it’s body rigidity is…\n\n\nStatic task 2 : transformations\n\ntests understanding of processes and actions that are either plausible or implausible\n\nMaking a salad out of a polyester shirt\n\n\nDynamic Task 2 : Atypical Consequences\n\nWhat happens when something unusual happens\n\nDynamic Task 3 : Entity Tracking\n\nA reader must keep track of entities in written text but this could also be applied in the computer vision side of the world\n\nDynamic Task 4 : QUantity Tracking\n\nPilot\n\nsetup:\n\n40 question answer pairs per task (after removing instances containing errors), via crowdsourcing;\n\nFuture Evaluations : Recurrent entity networks\n\nnot fully compatible as it’s geared around specific tasks\n\n\nDeep Understanding is hard\n\nWe shouldn’t confuse progress on superficial understanding for real progress\n\nWe can keep building things into the systems that give the innate properties (such as Convolution in CNN’s)\n\n\n\n\nSlides\n\nUsing brain imaging to study all kinds of cognitive processes in the brain\n\nreflection bring frustration in that more information has been discovered such as where in the brain or when in the brain is the neural activity\n\nless concentration on the “how” – this is a bit obvious as it’s easier to use these techniques to ask where and when\n\n\nOnce we understand the brain, what will be the form of the answer?\n\nDesign principles used wide in the brain\n\nwill vary in levels of detail – and we will surely have a description of how the brain computes\n\nWhether we might be on the verge of a time when we can take a new approach to studying the question of “how” in the brain\nRecent dramatic progress in the NN community where we’re gone from computers being blind, deaf, and dumb\n\nWe can now do many of these functions on machines\n\nAre we at a point where we can now take advantage of these NN models for things like vision and language and use them as hypoethesis about how to brain does these same things.\n\nPredicting fMRI output given people reading words\n\nhand designed vector embedding by co-occurence with 25 verbs\n\npredicting where in the brain we would find neural activity as a function of the input word stimulus\n\nthis studies where in the brain\n\nGustatory cortex activity is associated with activity related to co-occurences of words with words like “eat”\n\nsuggestions that the semantics of words are often grounded in parts of the brains who’s functions are affiliated with those words\nsaying “peach” caused activation in certain parts of human brains\nOur analysis is not asnwering the question “how”\n\nWe can look at “when”\n\nWhat information si encoded in space and time in the MEG video being shown\nIs the encoding of the semantics a function of time or is it a discrete process?\nTrained ~ 1,000,000 classifiers to predict activation in certain brain regions – the classifier was “decoding” the activity into words\n\nmost didn’t predict anything\nsome did\n\nduring the first 50 ms there was nothing to decode\nnext 50ms perceptual features could be decoded\n\nwe could get gross features of the line drawing as well\n\n200 ms, we have a semantic feature\n250 ms, is it hollow?\n400 ms even more\n\n\nThis is the kind of analysis that we can do on the When and Where details\n\nAt 50ms time intervals how accurately can we decode words from activity patterns captured through fMRI\n\ndecodability of feature “wordlength” (peak decodability 100-150ms)\nthis information doesn’t first appear “here” and then move\n\nit can be decoded simultaneously\nthere’s a synchrony that occurs between these disparate parts of the brain at the same time\n\nThis allows us to look at when\n\n\nMaybe these 6 regions work together to figure this out?\n\nmaybe not; maybe something else\n\n\nHow does the brain compute nueral representations?\n\nParadigm for studying “how”\n\nstimulus input to both models and the brain - compare the learned mappings - and measure the output of both systems\nour program is an example of “how”\n\nif we have 10 models, we can ask other questions like “does it allow us to explain, predict, the observed neural activity than the previous model?”\n\n\nAN example of what this paradigm implies\n\neach point on the slide is a hypothesis, the model obvs being the hypothesis\n\nthe more accurate the model is at recognizing the objects in this image\n\nthe better it is at predicting the neural activity in humans\ncorrelation between the two models’ performance\n\n\n\nCNN IT Alignment (Yamins et al 2014) Paper\n\nCNN v4 alignment\n\npenultimate layer – both predict more accurately than the output layer\n\n\n\nHow? : Language Processing\n\nSame concept as the CNN example above, this work was done using computation language models\n\nBERT\nELMo\netc.\n\nMEG scanner and showed the patients a new word every 500ms\n\nSentence mean MEG activity\n184 different sentences in passive and active voice\nUsed the above language models above and for every sentence they constructed each prefix of the sentence and fed it into the model\n\nlinear regression used to predict the neural activity\npredicted the 500ms neural activity\n\n\nWhich of these models works best/\n\nbrain actibity prediction accuracy*\n\nPaper?\n\nif we consider these models as hypothesis we can now rank them\n\n\nWill this paradigm really work? referencing the alignment approach\n\nthese studies have empirical evidence\nIs this helpful?\nLimits:\n\nmismatch between sequential computer processing vs. oscillatory, parallel neural activity\nthere’s a mismatch of constant activity in deep nets vs spiking in the brain\nmismatch in what we’re even measuring using these experiments\n\nbloog oxygen, fluctuations, etc. and actual neural activity\n\n\nImportant questions:\n\nDoes observed neural activity represent neural data representations, or processes that alter neural representations? (e.g. predictive coding : activity reflects energy being expended to update representations)\n\nword by word neural activity while reading - reading word number 4 doesn’t mean we can decode the first word\n\nwe can’t find it in the neural activity - we could when it appeared on the screen\n\nis this measuring a delta or the representation of the stimulus?\n\n\n\nWhat are brains truly doing/\nHow does context influence?\nShould we care if we model only part of it? (BERT doesn’t model word perception)\nIf we can’t interpret representations in deep nets, does it help explain brain activity in terms of these?\n\nAlisnging activity bween DNNs and neural activity\n\nwe can write down computation hypothesis and try to align these\n\n\n\n\n\n\nSlides\n\nSemantic defn – connected to language in that somehow through language we communicate a representation of the world through these high level variables\n\nthis is closely associated to the idea that we might be able to find these\n\nAnother connection one is trying to make is through that of agency – we are agents that act on the world and we cause changes in the world which induces distributional shifts\nWe need to figure out the OOD generalization problem\n\nis reality compositional in that i can build a symbolic approximation of certain properties of objects such as roundness for wheels and balls and redness for heat, etc.\n\nCompositionality is usually associated with linguistics\n\ndistributed represetations already exist in the idea of DL – and we can think of this compositionally\n\nthis has been intuiitively understood since the 80’s but now we can see why these forms of compositionality bring us up with exponential advantages\n\nCompositionality works because some assumption about the world can be exploited\n\nassumption is that I can learn about these features somewhat independently\n\nglasses independent of if that person is wearing shoes or not\n\nwe don’t need to see all combinations of these things to generalize\n\n\n\nSystematic generalization\nDynamically recombining representations of objects\n\neven when new combinations have 0 probability under training distribution\n\nscience fiction scenarios\nDriving in an unknown city\nattempting to exploit the regularity that is in the world\n\n\nClosure\n\nExpressions in novel contexts\nAssessing systematic generalization of CLEVR models (ARXIV) Paper\nMatching referring expressions (see slides)\n\nqualifier (brown cube)\n\n7 Tests\n\nsee slides\n\nCurrent SoTA\n\nStruggles on extension to CLEVR\n\n\nContrasting with the Symbolic AI Program\nChoices that are happening within the mind aren’t an active process\n\nThis system 1 computation in that it is intuitive and performs a selection of things that are relative to the situation\nThis is a reason why we need to put together the disiderata of the two fields of Symbolic and DNN approaches\n\nBuilding block for conscious processing is attention\n\nfocus on one or a few elements at a time\ncontent based soft-attention is concenient, can brkprop to learn where to atend\nattention is an internal action\n\nAttention is also a key to something also very important\n\nmemory\ncredit assignment\nvanishing gradients come up in training NN’s\nunreasonable to assume brains are doing anything like BBTT\nAlternative to this introduces at the last NeurIPS\n\nexploting memory – and we get this effect in things like transformers\n\nCredit assignment so we don’t make the same mistake multiple times\nSparse Attentive Backtracking\n\ndynamically building a graph that can relate the past to the present through many steps\nOn the fly we can create connections to the past and the present\n\neliminating the exponential loss of gradients problem\n\nAttention really forces NN’s to develop a form of representation for indirect references\n\nwhy is this coming up?\nattention mechanisms\n\nthere are many inputs “fighting” for attention and the module that receives the weighted sum and it sin’t able to understand which modules contributed to the weights\n\nthis allows us to think of NN”s as set transformation machines\n\nGlobal Workspace Theory\n\nBaars++ 1988, Dehaene 2003++\nbottleneck of sonscious processing\nselected item in broadcast stored in short-term memory\n\nLong term goal is to have ane nvironment where learning agents are faced with gradually more difficult tasks\n\nwhere humans are in the loop, helping the agents to figure out how it works and communicate with humans\n\nAffordances, options, exploration & controllable factors\n\naffordances : concepts / aspects of the environment which can be changed by the agent\ntemporal abstractions : options, super-actions, macros, or prcedures, which can be more complext procedures\ncontrollable factors\n\njointly learn a set of (policy, factor) such that the policy can control the factor and maximize the mutual information between policues (Bengio et al 2017) Paper\n\n\nConsciousness Prior :\n\nhigh level variables have a joint distribution, and are not independent which we can manipulate with language\ngraph that represents the joint is a sparse factor graph\n\neach of the factors in a factor graph corrrespond to a dependency between a group of variables\nit’s making a statement that links these 3 variables\n\neach statement as one possible sentence in natural language.\n\n\nWhat’s the connection?\n\ninference in a graphical model such as this allows us to exploit the sparsity of the graph and we sould visit the the nodes in this graph and it would require we only look at a few neighbors\n\nselecting these variables from a large set, at lteast the in\n\n\nThe brain is performing inference on this factor graph\n\nWhat causes changes in distributions\n\nthe changes in these distributions are about agents doing things and causing these shifts in the world\nactions are localized in space and time\n\nthese changes could be explained by just a few variables in the right model\n\nvideos – pixels at timesteps, we’re dead in the water\n\n\nconsequences of an intervention on few causes or mechanisms\n\nHow to factorize a joint distribution means uncovering this cause and effect structure\n\nBengio et al arxiv : 1901.10912\nWe can recover from a change in distribution faster\n\nDisentangling the causes:\n\nA meta-transwer objective for learning to disentangle causal mechanisms\n\nDoing inference on the Intervention\n\nLearning causal models from unknown interventions\n\nlearning small causal graphs, avoid exponential explosions of # of fraphs by parameterizing factorized distributions over graphs\n\nInference over intervention\n\n\n\n\n\nSlides\n\nThe human language network\n\nThis workshop, but more generally, what people call language seems way beyond what language actually is\n\nstable structure that spans across people and all brains\nA stronger response to sentences than lists of unconnected words\nWhy is the sentence the preferred stimulus\n\nstructure\n\nWhat features of linguistic simuli and what associated computations drive neural responses in the human language system?\nAbstract structure\n\ndomain general syntactic processnig\nsome argue about key hierarchical structure\nshare that all computations behind language processing are highly abstract\nOverlapping structures in numberical cognition and language and music and language\ndata suggest that this region of our brain is used as much when solving math as when looking at a blank screen\n\neffectively not at all\n\nSpacial and mentalization\nAll of these aspects don’t engage regions that handle language for us\nWhen processing computer langauges – we had people read snippets of code the critical condition is code comprehension\n\nunderstanding coding problems DID NOT elicit a response in the language network\n\nRules out this abstract syntactical structural processor\nPuts a damper on modeling language in a really abstract way\n\nMeaningful event / event comprehension\n\ncan this representation and activation be linked to sound or story, etc.\npresented participants with sentences and pictures of certain events and asked them to perform hard tasks\nsemantic conditions elicit a non-trivial amount of repsonse but still lower than the sentence condition\nLanguage regions may engage in processing non-verbal representations\n\nevidence of people with brain damage that language cortex isn’t required for abstract concept mangement, or some such\nwhat features are necessary and sufficient to elicit neural responses in face-selective cells / brain areas?\n\nMotivation - vision research\n\nSyntactic frame\n\nlanguage specific meaning-independent syntactic processing\nwhat is a syntactic frame?: word order, function words, functional morphology\nsentences elicit the strongest response, word list is 2 or more times lower\njabberwonky is even lower\na syntactic frame on it’s own elicits a low response in the language specific cortex\n\nSyntactic frame + meaning\n\nGrammatrical word order / word-order-based, prediction\ncombinatorially (semantic + syntactic) of words / composition\nReasons to facor composition as the core driver :\n\ncombinability of words in the nearby context seems to be a true universal property of our lingual systems\n\ncompositional\n\n\nDestroying word order while preserving local combinability\n\ncolors for no reason than to show manipulations\nmade local word swaps\n\n\nEstimating local combinabiliy:\n\nusing PMI - does a reasonable job of measuring how dependent two words are on eachother\nthis measures a little bit more bias toward semantics because it weights down certain words\nfMRI Results\n\none of many examples where I wasn’t predicting the results and the results blew my mind\nthe response doesn’t drop AT ALL with the swapping of the words\n\nhas been reproduced multiple times\n\nLocality\n\nIs it important?\nPrior reasons to suspect that it is:\n\nthe language network doesn’t care about structure above clause/sentence level;\nmost dependencies in natural language are local (Futrell et al 2015)\n\n\nDestroying word order and minimizing (local) combinabiity\n\nshuffling this causes a drop in the results that coincides with random word order list\n\n“Local coherence, global gibberish”\n\nhuman language system is all about local sentence coherence\nspan where humans can track is ~ 6 to 7 words\n6 words is the beginning of how much activation correlates given any length of input, it seems like it’s the lower bound\n\nTake home message\n\nLinguistic composition is the overall driver of the language system"
  },
  {
    "objectID": "posts/neurips-2019/index.html#robotics-workshop",
    "href": "posts/neurips-2019/index.html#robotics-workshop",
    "title": "Notes from NeurIPS 2019",
    "section": "",
    "text": "Automatic adaptation in robotics –&gt; Learning\nPractical constraints –&gt; data efficiency\nModels are useful for data-efficient learning in robotics\n3 Models\n\nProbabilistic models\n\nfast RL\n\nHierarchical Models\nPhysically meaningful models\n\nencode real world constaints to help move learning along\n\n\nProbabilistic Models\n\nPILCO Framework :\n\nProbabilistic model for transitiion function\n\nsystem identification\n\nCompute a long term state evolution\nPolicy improvement\nApply controller\n\n\nWhy probabilistic models?\n\nSlide\n* we need to find functions that allow us to capture the uncertainty about the world\n    * How do we plan and make decisions using the output of a regression model?\n    * Instead of picking a single function, we can posit a distribution over all functions\n        * I'm much more robust to modeling any of those functions that are within the bounds of the function\n\nHierarchical Problems\n\nGeneralize knowledge from knwon tasks to new (related tasks)\nRe-use experience gathered so far to generalize learning to new dynamics\nSeparate global and task specific properties\nShared global parameters\nGP captures global properties of the dynamics\n\nlatent variables \\(h_{p}\\)\nVariational inference\n\nModified cart-pole\n\nmodify length and weight of the pendulum\n\n\nData efficiency and interpretability\n\ncan we use model sfrom physics to encode more structure into the problems\nStarting point is lagrangian mechanics\nLagrangian : encodes “type” of physics\n\nhelps us talk about symmetries and conservation laws\n\nHamilton’s principle\n\nLearn L instead of learning the dynamics directly\n\nEuler-Lagrange Equations\n\nHow do we discretize these things?\nNaively, the errors build up and it becomes completely unphysical\nVariational Integrators\n\npreserve the physics as they discretize the space\nmomentum preserving\nsymplectic\nbounded energy behavior\n\nDiscretize it in a way that preserves the physics\n\nVariational Integrator Networks Paper Slide\n\nWrite down the parameterized Lagrangian\nDerive explicit variational integrator\nDefine the network architecture\n\n“just a whole big network”"
  },
  {
    "objectID": "posts/first-class-functions-in-python/index.html#first-class-functions",
    "href": "posts/first-class-functions-in-python/index.html#first-class-functions",
    "title": "First Class Functions in Python",
    "section": "",
    "text": "Typically first class functions are defined as a programming entity that can be :\n\nCreated at runtime\nAssigned to a variable or element in a data structure\nPassed as an argument\nReturned as the result of a function\n\nBy this definition, looking at how Python treats all functions, all functions are first class within Python.\nBelow we’ll see examples of exactly how this looks.\n\n\ndef factorial(n):\n    \"\"\"\n    Returns n! or n(factorial)\n    \n    e.g 5! = 5 * 4 * 3 * 2\n    \"\"\"\n    return 1 if n &lt; 2 else n * factorial(n-1)\n\nfactorial(5)\n120\n\n\n\nWe can show the first class nature of this function object using a few examples.\nWe can assign the function to a variable, which will invoke the function when calling that variable.\nfact = factorial\nfact(5)\n120\nWe can also use the map function, and pass our function as the first argument, allowing that function to be evaluated against the second argument, which is an iterable. Allowing this function to be applied in a successive fashion to all elements of this iterable.\nmap(factorial, range(10))\n[1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880]\n\n\n\nA higher order function is a bit….meta. It can take, as an argument, a function and then returns a function as a result.\nThe map() example used above is a great example of this.\nThe built-in sorted() function is another great example of this, within Python. We can pass it an iterable, along with a key that can then be applied in succession to the items in the list.\nfood = ['eggplant', 'carrots', 'celery', \n        'potatoes', 'tomatoes', 'rhubarb',\n        'strawberry', 'blueberry', 'raspberry',\n        'banana', 'cherry']\n\nprint(sorted(food, key=len))\n['celery', 'banana', 'cherry', 'carrots', 'rhubarb', 'eggplant', 'potatoes', 'tomatoes', 'blueberry', 'raspberry', 'strawberry']\nAny single argument function can be used in the key argument of the sorted() method.\nas a trivial example, we may want to use the reversed order of the characters to sort of words, as this will cause certain clustering of character strings together, such as -berry, and -toes.\ndef reverse(word):\n    '''\n    Reverse the order of the letters in a given string\n    '''\n    return word[::-1]\n\nprint(sorted(food, key=reverse))\n['banana', 'rhubarb', 'tomatoes', 'potatoes', 'carrots', 'eggplant', 'celery', 'blueberry', 'raspberry', 'strawberry', 'cherry']\n\n\n\nMap, filter, and reduce are typically offered in functional languages as higher order functions. However, the introduction of list comprehensions and generator expressions have downplayed the value of the map and filter functions, as listcomp’s and genexp’s combine the job of map and filter.\n# Build a list of factorials from 0! to 5!\nlist(map(fact, range(6)))\n[1, 1, 2, 6, 24, 120]\n# Build a list of factorials from 0! to 5!\n# but using list comprehension\n[fact(n) for n in range(6)]\n[1, 1, 2, 6, 24, 120]\n# Build a list of factorials of odd numbers up to 5!, using `map` and `filter`\nlist(map(factorial, filter(lambda n: n % 2, range(6))))\n[1, 6, 120]\nWe can see above that with the map and filter functions, we needed to use a lambda function.\nUsing a list comprehension can remove this requirement, and concatenate the operations.\n# Build a list of factorials of odd numbers up to 5!, using list comprehension\n[factorial(n) for n in range(6) if n % 2]\n[1, 6, 120]\n\n\n\nThe example above, where we’ve utilized map and filter combined with a lambda function leads us into our next example.\nThe lambda keyword created an anonymous function within a Python expression. However the syntax limits the lambda to be pure expressions. This means that the body of a lambda function can’t use other Python statements such as while or try, etc.\nThese are typically limited in their use because of the lack of the ability to use more complex control structures within the lambda functions. This can lead to unreadable or unworkable results.\nHowever, they can still prove useful in certain contexts, such as list arguments.\nfood = ['eggplant', 'carrots', 'celery', \n        'potatoes', 'tomatoes', 'rhubarb',\n        'strawberry', 'blueberry', 'raspberry',\n        'banana', 'cherry']\n\nprint(sorted(food, key=lambda word: word[::-1]))\n['banana', 'rhubarb', 'tomatoes', 'potatoes', 'carrots', 'eggplant', 'celery', 'blueberry', 'raspberry', 'strawberry', 'cherry']"
  },
  {
    "objectID": "notes/notes.html",
    "href": "notes/notes.html",
    "title": "Notes",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "talks/talks.html",
    "href": "talks/talks.html",
    "title": "Talks",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Ed Henry",
    "section": "Research Interests",
    "text": "Research Interests\n\nNatural Language Processing\nNovelty Detection\nCausality\nDeep Learning\nDistributed Systems\nNetworks"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Ed Henry",
    "section": "Experience",
    "text": "Experience"
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Ed Henry",
    "section": "Teaching",
    "text": "Teaching"
  },
  {
    "objectID": "index.html#conference-talks",
    "href": "index.html#conference-talks",
    "title": "Ed Henry",
    "section": "Conference Talks",
    "text": "Conference Talks"
  },
  {
    "objectID": "index.html#patents",
    "href": "index.html#patents",
    "title": "Ed Henry",
    "section": "Patents",
    "text": "Patents"
  },
  {
    "objectID": "index.html#quotes",
    "href": "index.html#quotes",
    "title": "Ed Henry",
    "section": "Quotes",
    "text": "Quotes\n\nAny sufficiently advanced technology is indistinguishable from magic. - Arthur C. Clarke\n\n\nVery few things are true without any assumptions. - Yoshua Bengio"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Some notes on building Retrieval Augmented Generation Systems\n\n\n\n\n\n\n\ndeep learning\n\n\ntransformers\n\n\nRAG\n\n\nretrieval\n\n\ngenerative modeling\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nEd Henry\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/retrieval-augmented-generation/index.html",
    "href": "notes/retrieval-augmented-generation/index.html",
    "title": "Some notes on building Retrieval Augmented Generation Systems",
    "section": "",
    "text": "I’m not sure where or how to start the process of brain dumping everything I’ve learned over the last couple of months as I’ve designed, built, and deployed various Retrieval Augmented Generation systems."
  },
  {
    "objectID": "notes/retrieval-augmented-generation/index.html#what-is-a-rag",
    "href": "notes/retrieval-augmented-generation/index.html#what-is-a-rag",
    "title": "Some notes on building Retrieval Augmented Generation Systems",
    "section": "What is a RAG",
    "text": "What is a RAG"
  },
  {
    "objectID": "notes/retrieval-augmented-generation/index.html#what-is-retrieval-augmented-generation",
    "href": "notes/retrieval-augmented-generation/index.html#what-is-retrieval-augmented-generation",
    "title": "Some notes on building Retrieval Augmented Generation Systems",
    "section": "What is Retrieval Augmented Generation?",
    "text": "What is Retrieval Augmented Generation?"
  }
]