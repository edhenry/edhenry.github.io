<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable MathJax -->
  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$', '$'], ["\(", "\)"] ],
          displayMath: [ ['$$', '$$'], ["\[", "\]"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
        //,
        //displayAlign: "left",
        //displayIndent: "2em"
      });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      First Post &middot; Ed Henry
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Ed Henry
        </a>
      </h1>
      <p class="lead">A place for me to take notes and share what I know</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
      
        
          
        
      
    </nav>

    <p>&copy; 2016. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">First Post</h1>
  <span class="post-date">29 Nov 2016</span>
  <h1 id="first-post">First post!</h1>

<p>I’ve finally started a new blog! One that abandons the frustrating Wordpress platform, for something that I hope is easier and more amenable to my workflow.</p>

<p>That said - I wanted to dedicate my first post to an algorithm called backpropagation – or reverse mode automatic differentiation. This is the algorithm that gives a neural network the power that it wields. And I’ll show you how in this post. But first, we’ll need to baseline on what a neural network is.</p>

<p>Disclaimer : I don’t consider myself a mathematician…yet, so if I make any mistakes, please point them out! Thanks!</p>

<h2 id="what-is-a-neural-network">What is a neural network?</h2>

<p>There is a whole biological inspiration story behind where the idea of a neural network was derived, however I’m going to ignore this explanation of inspiration for now. I’d rather view it from a raw mathematical perspective. I’m no neuroscientist, or mathematician for that matter, but I’ll do my best in my explanation.</p>

<h3 id="neuron">Neuron</h3>

<p>A neuron, or node, in the network, receives a signal as input. These inputs are depicted as the $x_{0},…,x_{2}$ in the image below. These inputs are multiplied by a set of weights depicted as $w_{0}…w_{2}$ in the image below.</p>

<p><img src="/img/nn.png" alt="" />
<em>Neuron<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup></em></p>

<p>These linear transformations of the inputs $x_{i}$ by the weights $w_{i}$ are then summed up and a translated by the vector $b$, in the image. This vector is the <em>bias</em> vector, of which I’ll explain shortly. This process of linear transformation followed by a translation, is also called an affine transformation<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>. These affine transformations are then summed and a pointwise application of an activation function<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup> is applied. Using a sigmoid activation function as an example, we can write the affine transformation within the sigmoid function as follows :</p>

<script type="math/tex; mode=display">1/1+exp(-\sum_{j}w_{j}x_{j}-b)</script>

<h3 id="references">References</h3>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><a href="http://cs231n.github.io/">http://cs231n.github.io/</a> <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><a href="https://www.quora.com/Whats-the-difference-between-affine-and-linear-functions">affine transformation</a> <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p><a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a> <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/12/21/Hashing-in-Python/">
            Hashing In Python
            <small>21 Dec 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/12/21/First-Class-Functions/">
            First Class Functions
            <small>21 Dec 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/12/17/Sequential-and-Binary-Search-in-Python/">
            Sequential and Binary Search in Python
            <small>17 Dec 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
